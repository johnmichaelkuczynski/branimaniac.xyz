53 digits that are 1, and about three-quarters from 46 to 54. As the number of

digits is increased, the preponderance of cases in which 1’s and 2’s are nearly

evenly balanced increases.

  Why this purely logical fact should be regarded as giving us good ground

for expecting that, when we toss a penny a great many times, we shall in fact

attain  an  approximately  equal  number  of  heads  and  tails,  is  a  different

question, involving laws of nature in addition to logical laws. I mention it

now only to emphasize the fact that I am not at present discussing it.

  I want to lay stress on the fact that, in the above interpretation, there is

nothing about possibility, and nothing which essentially involves ignorance.

There is merely a counting of members of a class B and determining what

proportion of them also belong to a class A.

  It is sometimes contended that we need an axiom of equi-probability—e.g.

to the effect that heads and tails are equally probable. If this means that in fact

they  occur  with  approximately  equal  frequency,  the  assumption  is  not

necessary to the mathematical theory, which, as such, is not concerned with

actual occurrences.

  Let us now consider possible applications of the finite-frequency definition

to cases of probability which might seem to fall outside it.

  First:  in  what  circumstances  can  the  definition  be  extended  to  infinite

collections?  Since  we  have  defined  a  probability  as  a  fraction,  and  since

fractions are meaningless when numerator and denominator are infinite, it

will only be possible to extend the definition when there is some means of

proceeding to a limit. This requires that the a’s, of which we are to estimate

the probability of their being b’s, should form a series, in fact a progression, so

that they are given as a, a, a, … a, … where for every finite integer n there
                          123n

is  a  corresponding a  and  vice  versa.  We  can  then  denote  by  “p”  the
                        nn

proportion of a’s up to a that belong to b. If, as n increases, p approaches a
                            nn
                                                                                    1
limit, we can define this limit as the probability that an a will be a b. We

must, however, distinguish the case in which the value of p oscillates about
                                                                      n

the limit from that in which it approaches the limit from one side only. If we

repeatedly toss a coin, the number of heads will be sometimes more than half

the  total,  sometimes  less;  thus p  oscillates  about  the  limit  1/2.  But  if  we
                                      nconsider the proportion of primes up to n, this approaches the limit zero from

one side only: for any finite n, p is a definite positive fraction, which, for
                                      n

large values of n, is approximately 1/log n. Now 1/log n approaches zero as n

increases indefinitely. Thus the proportion of primes approaches zero, but we

cannot say “no integers are primes”; we may say that the chance of an integer

being a prime is infinitesimal, but not zero. Obviously the chance of an integer

being  a  prime  is  greater  than  that  of  its  being  (say)  both  odd  and  even,

although the chance is less than any finite fraction, however small. I should

say that, when the chance that an a is a b is strictly zero, we can infer “no a is

a b”, but when the chance is infinitesimal we cannot make this inference.

  It  is  to  be  observed  that,  unless  we  make  some  assumption  about  the

course of nature, we cannot use the method of proceeding to the limit when

we are dealing with a series which is defined empirically. For example, if we

toss a given coin repeatedly, and find that the number of heads, as we go on,

approaches  continually  nearer  to  the  limit  1/2,  that  does  not  entitle  us  to

assume that this really would be the limit if we could make our series infinite.

It may be, for example, that, if n is the number of tosses, the proportion of

heads does not approximate strictly to 1/2 but to






•  where N is a number much larger than any that we can reach in actual

  experiments.  In  that  case,  our  inductions  would  begin  to  be  empirically

falsified just as we were thinking they were firmly established. Or again, it

might happen, with any empirical series, that after a time it became utterly

lawless, and ceased in any sense to approach a limit. If, then, the above

extension to infinite series is to be used in empirical series, we shall have to

invoke some kind of inductive axiom. Without this, there is no reason for

expecting the later parts of such a series to continue to exemplify some law

which the earlier parts obey.

  In ordinary empirical judgments of probability, such, for example, as are

contained in the weather forecast, there is a mixture of different elementswhich it is important to separate. The simplest hypothesis—unduly simplified

for purposes of illustration—is that some symptom is observed which, in (say)

90 per cent of the cases in which it has been previously observed, has been

followed by rain. In that case, if inductive arguments were as indubitable as

deductive ones, we should say “there is a 90 per cent probability of rain”. That

is to say, the present moment belongs to a certain class (that of moments

when the symptom in question is present) of which 90 per cent are moments

preceding rain. This is probability in the mathematical sense which we have

been considering. But it is not this alone that makes us uncertain whether it

will rain. We are also uncertain as to the validity of the inference; we do not

feel sure that the symptom in question will, in the future, be followed by rain

nine times out often. And this doubt may be of two kinds, one scientific, the

other  philosophical.  We  may,  while  retaining  full  confidence  in  scientific

procedure in general, feel that, in this case, the data are too few to warrant an

induction,  or  that  not  sufficient  care  has  been  taken  to  eliminate  other

circumstances which may have also been present and may be more invariable

precursors of rain. Or, again, the records may be doubtful: they may have

been rendered nearly indecipherable by rain, or have been made by a man

who  was  shortly  afterwards  certified  as  insane.  Such  doubts  are  within

scientific procedure, but there are also the doubts raised by Hume: is inductive

procedure valid, or is it merely a habit which makes us comfortable? Any or

all of those reasons may make us hesitant about the 90 per cent chance of rain

which our evidence inclines us to believe in.

  We have, in cases of this sort, a hierarchy of probabilities. The primary

level is: Probably it will rain. The secondary level is: Probably the symptoms I

noticed are a sign of probable rain. The tertiary level is: Probably certain kinds

of events make certain future events probable. Of these three levels, the first is

that  of  common  sense,  the  second  that  of  science,  and  the  third  that  of

philosophy.

  In the first stage, we have observed that, hitherto, A has been followed,

nine times out of ten, by B; in the past, therefore A has made B probable in the

sense of finite frequency. We suppose without reflection, at this stage, that we

may expect the same thing in the future.  In  the  second  stage,  without  questioning  the  general  possibility  of

inferring the future from the past, we realize that such inferences should be

submitted to certain safeguards, such, for example, as those of Mill’s four

methods. We realize also that inductions, even when conducted according to

the best rules, are not always verified. But I think our procedure can still be

brought within the scope of the finite frequency theory. We have made in the

past a number of inductions, some more careful, some less so. Of those made

by a certain procedure, a proportion p have, so far, been verified; therefore

this procedure, hitherto, has conferred a probability p upon the inductions

that  it  sanctioned.  Scientific  method  consists  largely  of  rules  by  means  of

which p (as tested by the past results of past inductions) can be made to

approach nearer to 1. All this is still within the finite frequency theory, but it

is now inductions that are the single terms in our estimate of frequency.

  That  is  to  say,  we  have  two  classes  A  and  B,  of  which  A  consists  of

inductions that have been performed in accordance with certain rules, and B

consists of inductions which experience hitherto has confirmed. If n is the

number of members of A, and m is the number of members common to A and

B, then m/n is the chance that an induction conducted according to the above

rules will have, up to the present, led to results which, when they could be

tested, were found to be true.

  In saying this, we are not using induction; we are merely describing a

feature  of  the  course  of  nature  so  far  as  it  has  been  observed.  We  have,

however, found a criterion of the excellence (hitherto) of any suggested rules

of  scientific  procedure,  and  we  have  found  it  within  the  finite  frequency

theory.  The  only  novelty  is  that  our  units  are  now  inductions,  not  single

events. The inductions are treated as occurrences, and it is only those that

have actually occurred that are to be regarded as members of our class A.

  But as soon as we argue either that an individual induction which has

hitherto been confirmed will, or will probably, be confirmed in the future, or

that  rules  of  procedure,  which  have  given,  so  far,  a  large  proportion  of

inductions  that  have  been  confirmed  so  far,  are  likely  to  give  a  large

proportion of confirmed inductions in the future, we have passed outside the

finite  frequency  theory,  since  we  are  dealing  with  classes  of  which  thenumbers are not known. The mathematical theory of probability, like all pure

mathematics, though it gives knowledge, does not (at least in one important

sense)  give  anything  new;  induction,  on  the  other  hand,  certainly  gives

something new, and the only doubt is whether what it gives is knowledge.

  I do not want, as yet, to examine induction critically; I wish only to make

clear that it cannot be brought within the scope of the finite frequency theory,

even by the device of considering a particular induction as one of a class of

inductions,  since  tested  inductions  can  only  supply inductive  evidence  in

favour of a hitherto untested induction. If, then, we say that the principle

which  validates  induction  is  “probable”,  we  must  be  using  the  word

“probable” in a different sense from that of the finite frequency theory; the

sense in question must, I should say, be what we called “degree of credibility”.

  I incline to think that, if induction, or whatever postulate we may decide

upon as a substitute, is assumed, all precise and measurable probabilities can

be interpreted as finite frequencies. Suppose I say, for example: “There is a

high probability that Zoroaster existed”. To substantiate this statement, I shall

have to consider, first, what is the alleged evidence in his case, and then to

look  out  for  similar  evidence  which  is  known  to  be  either  veridical  or

misleading. The class upon which the probability depends is not the class of

prophets,  existent  and  non-existent, for  by  including  the  non-existent  we

make the class somewhat vague; nor can it be the class of existent prophets

only, since the question at issue is whether Zoroaster belongs to this class. We

shall have to proceed as follows: There is, in the case of Zoroaster, evidence

belonging to a certain class A; of all the evidences that belong to this class and

can be tested, we find that a proportion p are veridical; we therefore infer, by

induction, that there is a probability p in favour of the similar evidence in the

case  of  Zoroaster.  Thus  frequency  plus  induction  covers  this  use  of

probability.

  Or suppose we say, like Bishop Butler: “It is probable that the universe is

the  result  of  design  on  the  part  of  a  Creator”.  Here  we  start  with  such

subsidiary arguments as that a watch implies a watchmaker. There are very

many instances of watches known to be made by watchmakers, and none of

watches known to be not made by watchmakers. There is in China a kind ofmarble which sometimes, by accident, produces what appear to be pictures

made by artists; I have seen the most astonishing examples. But this is so rare

that, when we see a picture, we are justified (assuming induction) in inferring

an  artist  with  a  very  high  degree  of  probability.  What  remains  for  the

episcopal logician, as he emphasizes by the title of his book, is to prove the

analogy; this may be held doubtful, but cannot well be brought under the

head of mathematical probability.

  So  far,  therefore,  it  would  seem  that  doubtfulness  and  mathematical

probability—the latter in the sense of finite frequency— are the only concepts

required in addition to laws of nature and rules of logic. This conclusion,

however, is only provisional. Nothing definitive can be said until we have

examined certain other suggested definitions of “probability”.1 This limit depends upon the order of the a‘s, and therefore belongs to them as a series, not as a

  class.                                          IV 


            The Mises-Reichenbach Theory


THE frequency interpretation of probability, in a form different from that of

the  previous  chapter,  has  been  set  forth  in  two  important  books,  both  by

                                                              1
German professors who were then in Constantinople.

    Reichenbach ‘s work is a development of that of v. Mises, and is in various

ways a better statement of the same kind of theory. I shall therefore confine

myself to Reichenbach.

  After giving the axioms of the probability calculus, Reichenbach proceeds

to offer an interpretation which seems to be suggested by the case of statistical

correlations. He supposes two series (x, x, … x, …), (y, y, … y, …), and two
                                            12n12n

classes O and P. Some or all of the x’s belong to the class O; what interests

him is the question: how often do the corresponding y’s belong to the class P?

    Suppose, for example, you were investigating the question whether a man

is predisposed to suicide by having a nagging wife. In this case, the x’s are

wives, the y’s are husbands, the class O consists of naggers, and the class P of

suicides. Then given that a wife belongs to the class O, our question is: how

often does her husband belong to the class P?

  Consider the sections of the two series consisting of the first n terms of

each. Suppose that, among the first n x’s, there are a terms belonging to the

class  O,  and  suppose  that,  of  these,  there  are b  terms  such  that  the

corresponding y belongs to the class P. (The corresponding y is the one with

the same suffix.) Then we say that, throughout the section from x to x, the
                                                                              1n

“relative frequency” of O and P is b/a. (If all the x’s belong to the class O, a =

n, and the relative frequency is b/n.) We denote this relative frequency by

“Hn(O, P)”.

  We  now  proceed  to  define  “the  probability  of  P  O”,  given  which  we

denote by “W(O, P)”. The definition is:

  W(O, P) is the limit of Hn(O, P) as n is indefinitely increased.

  This  definition  can  be  considerably  simplified  by  the  use  of  a  littlemathematical logic. In the first place, it is unnecessary to have two series. For

both are assumed to be progressions, and there is therefore some one-one

correlator of their terms. If this is S, to say that a certain y belongs to a class P

is equivalent to saying that the corresponding x belongs to the class of terms

having the relation S to some one or other of the members of P. E.g. let S be

the relation of wife to husband; then if y is a married man and x is his wife, to

say that y is a government official is true if, and only if, x is the wife of a

government official.

  In the second place, there is no advantage in admitting the case in which

not all the x’s belong to the class O. The definition is only appropriate if an

infinite number of the x’s belong to the class O; in that case, those that belong

to O form a progression, and the rest can be forgotten. Thus we retain what is

essential in Reichenbach ‘s definition if we substitute the following:

  Let Q be a progression, and α some class, of which, in the important cases,

there are members, in the series of Q, later than any given member. Let m be

the number of members of α among the first n members of Q. Then W(Q, α) is

defined as the limit of m/n when n is indefinitely increased.

    Perhaps through inadvertence, Reichenbach speaks as if the concept of

probability were only applicable to progressions, and had no application to

finite classes. I cannot think he intends this. The human race, for example, is a

finite class, and we wish to apply probability to vital statistics, which would

be  impossible  according  to  the  letter  of  the  definition.  As  a  matter  of

psychological fact, when Reichenbach speaks of the limit for n = infinity, he is

thinking  of  the  limit  as  some  number  which  is  very  nearly  approached

whenever n is large from an empirical point of view, i.e. when it is not far

short of the maximum that our means of observation enable us to reach. He

has an axiom or postulate to the effect that, when there is such a number for

every large observable n, it is approximately equal to the limit for n = infinity.

This is an awkward axiom, not only because it is arbitrary, but because most

of the series with which we are concerned outside pure mathematics are not

infinite; indeed it may be doubted whether any of them are. We are in the

habit of assuming that space-time is continuous, which implies the existence

of  infinite  series;  but  this  assumption  has  no  basis  except  mathematicalconvenience.

  I shall assume, in order to make Reichenbach ‘s theory as adequate as

possible, that, where finite classes are concerned, the definition of the last

chapter is to be retained, and that the new definition is only intended as an

extension enabling us to apply probability to infinite classes. Thus his Hn (O,

P) will be a probability, but one applying only to the first n terms of the series.

  What Reichenbach postulates, as his form of induction, is something like

this: Suppose we have made N observations as to the correlation of O and P,

so that we are in a position to calculate Hn (O, P) for all values of n up to n =

N, and suppose that, throughout the last half of the values of n, Hn (O, P)

always differs from a certain fraction p by less than ε, where ε is small. Then

it shall be posited that, however much we were to increase n, Hn (O, P) would

still lie within these narrow boundaries, and therefore W (O, P), which is the

limit  for n  =  infinity,  will  also  lie  within  these  boundaries.  Without  this

assumption, we can have no empirical evidence as to the limit for n = infinity,

and  the  probabilities  for  which  the  definition  is  specially  designed  must

remain totally unknown.

  In defence of Reichenbach’s theory, in face of the above difficulties, two

things may be said. In the first place, he may contend that it is not necessary

to suppose n to approach infinity indefinitely; for all practical purposes, it

suffices if n is allowed to become very large. Suppose, for instance, that we are

dealing with vital statistics. It does not matter to an insurance company what

will happen to the statistics if they are prolonged for another 10,000 years; at

most, the next 100 years concern it. If, when we have accumulated statistics,

we assume that frequencies will remain roughly the same until we have ten

times as many data as we have now, that is enough for almost all practical

purposes. Reichenbach may say that, when he speaks of infinity, he is using a

convenient mathematical shorthand, meaning only “a good deal more of the

series than we have investigated hitherto”. The case is exactly analogous, he

might say, to that of the empirical determination of a velocity. In theory, a

velocity  can  only  be  determined  if  there  is  no  limit  to  the  smallness  of

measureable  spaces  and  times;  in  practice,  since  there  is  such  a  limit,  the

velocity at an instant can never be known even approximately. We can, it istrue, know with a fair measure of accuracy the average velocity throughout a

short  time.  But  even  if  we  assume  a  postulate  of  continuity,  the  average

velocity throughout (say) a second gives absolutely no indication as to the

velocity at a given instant during that second. All motion might consist of

periods of rest separated by instants of infinite velocity. Short of this extreme

hypothesis, and even if we assume continuity in the mathematical sense, no

finite velocity at an instant is incompatible with any finite average velocity

throughout  a  finite  time,  however  short,  which  contains  that  instant.  For

practical  purposes,  however,  this  is  of  no  consequence.  Except  in  a  few

phenomena such as explosions, if we take the velocity at any instant through

a very short measurable time to be approximately the average velocity during

that time, the laws of physics are found to be verified. “Velocity at an instant”,

therefore, may be regarded as nothing but a convenient mathematical fiction.

  In like manner, Reichenbach may say, when he speaks of the limit of a

frequency when n is infinite, he means only the actual frequency for very

large numbers, or rather this frequency with a small margin of error. The

infinite and the infinitesimal are equally unobservable, and therefore (he may

say) equally irrelevant to empirical science.

  I am inclined to admit the validity of this answer. I only regret that I do

not find it explicitly in Reichenbach’s books; I think, nevertheless, that he

must have had it in mind.

  The second point in favour of his theory is that it applies to just the sort of

cases in which we wish to use probability arguments. We wish to use these

arguments when we have some data as to a certain future event, but not

enough to determine its character in some respect that interests us. My death,

for example, is a future event, and if I am insuring my life I may wish to know

what evidence exists as to the likelihood of death occurring in some given

year. In such a case we always have a number of individual facts recorded in a

series, and we assume that the frequencies we have found hitherto will more

or less continue. Or take gambling, from which the whole subject took its rise.

We are not interested in the mere fact that there are 36 possible results of a

throw with two dice. What we are interested in is the fact (if it be a fact) that

in a long series of throws each of these 36 possibilities will be realized anapproximately equal number of times. This is a fact which does not follow

from the mere existence of 36 possibilities. When you meet a stranger, there

are exactly two possibilities: on the one hand, he may be called Ebenezer

Wilkes Smith; on the other hand, he may not. But in a long life, during which

I  have  met  a  great  many  strangers,  I  have  only  once  found  the  former

possibility realized. The pure mathematical theory, which merely enumerates

possible  cases,  is  devoid  of  practical  interest  unless  we  know  that  each

possible  case  occurs  approximately  with  equal  frequency,  or  with  some

known  frequency.  And  this,  if  we  are  considering  events,  not  a  logical

schema, can only be known through actual statistics, the use of which, it may

be said, must proceed more or less as in Reichenbach ‘s theory.

  This argument, also, I shall admit provisionally; it will be examined afresh

when we come to consider induction.

  There is an objection of a quite different kind to Reichenbach’s theory as

he states it, and that has to do with his introducing series where only classes

seem to be logically relevant. Let us take an illustration: what is the chance

that an integer chosen at random will be a prime? If we take the integers in

their natural order, the chance, on his definition, is zero; for if n is an integer,

the number of primes less than or equal to n is approximately n/logn if n is

large, so that the chance of an integer less than n being a prime approximates

to 1/logn, and the limit of 1/logn as n is indefinitely increased is zero. But now

suppose we rearrange the integers on the following plan: Put first the first 9

primes, then the first number that is not a prime, then the next 9 primes, and

then the second number that is not a prime, and so on indefinitely. When the

integers are arranged in this order, Reichenbach’s definition shows that the


chance of a number selected at random being a prime is . We could even

arrange the integers so that the chance of a number not being a prime would

be zero. To get this result, begin with the first non-prime, i.e. 4, and put after
    th
the n number which is not a prime the n primes next after those already

placed; this series begins 4, 1, 6, 2, 3, 8, 5, 7, 11, 9, 13, 17, 19, 23, 10, 29, 31, 37,

41, 43, 12. …

                                                                  th
  In this arrangement, there will be, before the (n + 1) non-prime, n non-primes and ½ n (n + 1) primes; thus as n increases, the ratio of the number of

non-primes to the number of primes approaches 0 as a limit.

  From  this  illustration  it  is  obvious  that,  if  Reichenbach’s  definition  is

accepted,  given  any  class  A  having  as  many  terms  as  there  are  natural

numbers, and given any infinite sub-class B, the chance that an A selected at

random will be a B will be anything from 0 to 1 (both included), according to

the way in which we choose to distribute the B’s among the A’s.

  It follows that, if probability is to apply to infinite collections, it must

apply to series, not to classes. This seems strange.

  It is true that, where empirical data are concerned, they are all given in a

time-order, and therefore as a series. If we choose to assume that there is

going to be an infinite number of events of the kind we are investigating, then

we can also decide that our definition of probability is to apply only so long as

the events are arranged in temporal sequence. But outside pure mathematics

no series are known to be infinite, and most are, as far as we can judge, finite.

What is the chance that a man of 60 will die of cancer? Surely we can estimate

this without assuming that the number of men who, before time ends, will

have died of cancer, is infinite. But according to the letter of Reichenbach’s

definition this should be impossible.

  If probabilities depend upon taking events in their temporal order, rather

than in any other order of which they are susceptible, then probability cannot

be a branch of logic, but must be part of the study of the course of nature. This

is not Reichenbach’s view; he holds, on the contrary, that all true logic is

probability-logic, and that the classical logic is at fault because it classifies

propositions as true or false, not as having this or that degree of probability.

He  should,  therefore,  be  able  to  state  what  is  fundamental  in  probability-

theory in abstract logical terms, without introducing accidental features of the

actual world, such as time.

  There is great difficulty in combining a statistical view of probability with

the  view,  which  Reichenbach  also  holds,  that  all  propositions  are  only

probable in varying degrees that fall short of certainty. The difficulty is that

we seem committed to an endless regress. Suppose we say it is probable that a

man who has plague will die of it. This means that, if we could ascertain thewhole series of men who, from the earliest times till the extinction of the

human race, will have suffered from plague, we should find that more than

half of them will have died of it. Since the future and much of the past are

unrecorded, we assume that the recorded cases are a fair sample. But now we

are  to  remember  that  all  our  knowledge  is  only  probable;  therefore  if,  in

compiling our statistics, we find it recorded that Mr. A had plague and died of

it, we must not regard this item as certain, but only as probable. To find out

how  probable  it  is,  we  must  include  it  in  a  series,  say  of  official  death

certificates, and we must find some means of ascertaining what proportion of

death certificates are correct. Here a single item in our statistics will be: “Mr.

Brown was officially certified to have died, but turned out to be still alive”.

But this, in turn, is to be only probable, and must therefore be one of a series

of recorded official errors, some of which turned out to be not errors. That is

to  say,  we  must  collect  cases  where  it  was  falsely  believed  that  a  person

certified dead had been found to be still alive. To this process there can be no

end, if all our knowledge is only probable, and probability is only statistical. If

we are to avoid an endless regress, and if all our knowledge is to be only

probable, “probability” will have to be interpreted as “degree of credibility”,

and  will  have  to  be  estimated  otherwise  than  by  statistics.  Statistical

probability can only be estimated on a basis of certainty, actual or postulated.

  I  shall  return  to  Reichenbach  in  connection  with  induction.  For  the

present,  I  wish  to  make  clear  my  own  view  as  to  the  connection  of

mathematical  probability  with  the  course  of  nature.  Let  us  take  as  an

illustration a case of Bernoulli’s law of large numbers, choosing the simplest

possible case. We have seen that, if we make up all possible integers consisting

of n digits, each either 1 or 2, then, if n is large—say not less than 1,000—a vast

majority of the possible integers have an approximately equal number of 1’s

and  2’s.  This  is  merely  an  application  of  the  fact  that,  in  the  binomial

                      n
expansion of (x + y), when n is large the sum of the coefficients near the

                                                                                    n
middle falls not far short of the sum of all the coefficients, which is 2. But

what has this to do with the statement that, if I toss a penny often, I shall

probably get an approximately equal number of heads and tails? The one is a

logical fact, the other, apparently, an empirical fact; what is the connectionbetween them?

  With  some  interpretations  of  “probability”,  a  statement  containing  the

word “probable” can never be an empirical statement. It is admitted that what

is improbable may happen, and what is probable may fail to happen. It follows

that what does happen does not show that a previous judgment of probability

was  either  right  or  wrong;  every  imaginable  course  of  events  is  logically

compatible with every imaginable anterior estimate of probabilities. This can

only be denied by maintaining that what is very improbable does not happen,

which we have no right to maintain. In particular, if induction asserts only

probabilities, then whatever may happen is logically compatible both with the

truth and with the falsehood of induction. Therefore the inductive principle

has no empirical content. This is a reductio ad absurdum, and shows that we

must connect the probable with the actual more closely than is sometimes

done.

  If we adhere to the finite frequency theory—and so far I have seen no

reason for not doing so—we shall say that, if we assert “a is an A” to be

probably given “a is a B”, we mean that, in fact, most members of B are

members of A. This is a statement of fact, but not a statement about a. And if

I say that an inductive argument (suitably formulated and limited) makes its

conclusion probable, I mean that it is one of a class of arguments, most of

which have conclusions that are true.

  What, now, can I mean when I say that the chance of heads is a half? To

begin with, this, if true, is an empirical fact; it does not follow from the fact

that, in tossing a coin, there are only two possibilities, heads and tails. If it did,

we could infer that the chance of a stranger being called Ebenezer Wilkes

Smith is a half, since there are only two alternatives, that he is so called or

that he isn’t. With some coins, heads come oftener than tails; with others, tails

oftener than heads. When I say, without specifying the coin, that the chance

of heads is a half, what do I mean?

  My assertion, like all other empirical assertions that pretend to numerical

exactitude, must be only approximate. When I say that a man’s height is 6 ft. 1

in., I am allowed a margin of error; even if I have said it on oath, I cannot be

convicted of perjury if it turns out that I am a hundredth of an inch out.Similarly, I must not be held to have made a false statement about the penny

if it turns out that 0 · 500001 would have been a more accurate estimate than 0

· 5. It is doubtful, however, whether any evidence could make me think 0 ·

500001 a better estimate than 0 · 5. In probability, as elsewhere, we take the

simplest hypothesis which approximately fits the facts. Take (say) the law of

falling bodies. Galileo made a certain number of observations, which fitted

                                              2
more or less with the formula s = ½gt. No doubt he could have found a

function f(t) such that s = f(t) would have fitted his observations more exactly,

                                                                      1
but he preferred a simple formula which fitted well enough. In the same way,

if I tossed a coin 2,000 times and got 999 heads and 1,001 tails, I should take

the chance of heads to be a half. But what exactly should I mean by this

statement?

  This question shows the strength of Reichenbach’s definition. According

to him, I mean that, if I continue long enough, the proportion of heads will

come, in time, to be permanently very near ½; in fact, it will come to differ

from ½ by less than any fraction however small. This is a prophecy; if it is

correct, my estimate of the probability is correct, but if not, not. What can the

finite frequency theory oppose to this?

  We must distinguish between what the probability is and what it probably

is. As to what the probability is, that depends upon the class of tosses we are

considering. If we are considering tosses with a given coin, then if, in the

whole of its existence, this coin is going to have given m heads out of a total

of n  tosses,  the  probability  of  heads  with  that  coin  is m/n.  If  we  are

considering coins in general, n will have to be the total number of tosses of

coins throughout the past and future history of the world, and m the number

of these that will have been heads. We may, to make the problem less vast,

confine ourselves to tosses this year in England, or to tosses tabulated by

students of probability. In all these cases m and n are finite numbers, and m/n

is the probability of heads with the given conditions.

  But none of the above probabilities are known. We are therefore driven to

make estimates of them, that is to say, to find some way of deciding what

they probably are. If we are to adhere to the finite frequency theory, this will

mean that our series of heads and tails must be one of some finite class ofseries, and that we must have some relevant knowledge about this whole

class. We will suppose it to have been observed that, in every series of 10,000

                                                                                    th
or more tosses with a given coin, the proportion of heads after the 5,000 toss

has never varied by more than 2ε, where ε is small. We can then say: In every

observed case, the proportion of heads after 5,000 tosses with a given coin has

always remained between p − ε and p + ε, where p is a constant depending on

the  coin.  To  argue  from  this  to  a  case  not  yet  observed  is  a  matter  of

induction. If this is to be valid, we shall need an axiom to the effect that (in

certain circumstances) a characteristic which is present in all observed cases is

present in a large proportion of all cases; or, at any rate, we shall need some

axiom  from  which  this  results.  We  shall  then  be  able  to  infer  a  probable

probability from observed frequencies, interpreting probability in accordance

with the finite frequency theory.

  The above is only an outline suggestion of a theory. The main point that I

wish  to  emphasize  is  that,  on  the  theory  I  advocate,  every  probability

statement (as opposed to a merely doubtful statement) is a statement of fact,

as  to  some  proportion  in  a  series.  In  particular,  the  inductive  principle,

whether true or false, will have to assert that, as a fact, most series of certain

kinds have, throughout, any characteristic of a certain sort which is present in

a large number of successive terms of the series. If this is a fact, inductive

arguments may yield probabilities; if not, not. I do not at present inquire how

we are to know whether it is a fact or not; that is a problem which I shall not

consider until the last section of our inquiry.

  It will be seen that, in the above discussion, we have been led to agree

with Reichenbach on many points, while consistently disagreeing as to the

definition of probability. The main objection which I feel to his definition is

that  the  frequency  on  which  it  depends  is  hypothetical  and  for  ever

unascertainable. I disagree also in distinguishing more sharply than he does

between probability and doubtfulness, and in holding that probability-logic is

not logically the fundamental kind, as opposed to certainty-logic.1 Richard von Mises, Wahrscheinlichkeit, Statistik und Wahrheit, 2nd ed. Vienna, 1936 (ist ed. 1928);

  Hans Reichenbach, Wahrscheinlichkeitslehre, Leiden, 193c. See also the latter’s Experience and

    Prediction, 1938.

1 Cf. Jeffreys, Theory of Probability, and Scientific Inference.                                          V 


            Keynes’s Theory of Probability


KEYNES’S Treatise on Probability (1921) sets out a theory which is, in a sense,

the  antithesis  of  the  frequency  theory.  He  holds  that  the  relation  used  in

deduction, namely “p implies q”,  is  the  extreme  form  of  a  relation  which

might be called “p more or less implies q”. “If a knowledge of h”, he says,

“justifies  a  rational  belief  in a  of  degree  α,  we  say  there  is  a  probability

relation of degree α between a and h”. We write this: “a/h = α”. “Between two

sets of propositions there exists a relation, in virtue of which, if we know the

first, we can attach to the latter some degree of rational belief.” Probability is

essentially a relation: “It is as useless to say ‘b is probable’ as ‘b is equal’ or ‘b

is greater than’.” From “a” and “a implies b”, we can conclude “b”, that is to

say, we can drop all mention of the premiss and simply assert the conclusion.

But if a is so related to b that a knowledge of a renders a probable belief in b

rational,  we  cannot  conclude  anything  whatever  about b  which  has  not

reference  to a;  there  is  nothing  corresponding  to  the  dropping  of  a  true

premiss in demonstrative inference.

    Probability, according to Keynes, is a logical relation, which cannot be

defined, unless, perhaps, in terms of degrees of rational belief. But on the

whole  it  would  seem  that  Keynes  inclines  rather  to  defining  “degrees  of

rational belief” in terms of the probability-relation. Rational belief, he says, is

derivative from knowledge: when we have a degree of rational belief in p, it is

because we know some proposition h and also know p/h = α. It follows that

some propositions of the form “p/h = α” must be among our premisses. Our

knowledge is partly direct, partly by argument; our knowledge by argument

proceeds through direct knowledge of propositions of the form “p implies q”

or “q/p = α”. In every argument, when fully analysed, we must have direct

knowledge of the relation of the premisses to the conclusion, whether it be

that of implication or that of probability in some degree. Knowledge of h and

of p/h = α leads to a “rational belief of the appropriate degree” in p. Keynesexplicitly assumes that all direct knowledge is certain, and that a rational

belief which falls short of certainty can only arise through perception of a

probability-relation.

    Probabilities  in  general,  according  to  Keynes,  are  not  numerically

measurable; those that are so form a very special class of probabilities. He

holds that one probability may not be comparable with another, i.e. may be

neither greater nor less than the other, nor yet equal to it. He even holds that

it is sometimes impossible to compare the probabilities of p and not-p on

given evidence. He does not mean that we do not know enough to do this; he

means that there actually is no relation of equality or inequality. He thinks of

probabilities according to the following geometrical scheme: Take two points,

representing the 0 of impossibility and the 1 of certainty; then numerically

measurable possibilities may be pictured as lying on the straight line between

0  and  1,  while  others  lie  on  various  curved  routes  from  0  to  1.  Of  two

probabilities  on  the  same  route,  we  can  say  that  the  one  nearer  1  is  the

greater, but we cannot compare probabilities on different routes, except when

two routes intersect, which may happen.

  Keynes needs, as we have seen, some direct knowledge of probability-

propositions. In order to make a beginning in obtaining such knowledge, he

examines and emends what is called the “principle of non*sufficient reason”,

or, as he prefers to call it, the “principle of indifference”.

  In its crude form, the principle states that if there is no known reason for

one rather than another of several alternatives, then these alternatives are all

equally  probable.  In  this  form,  as  he  points  out,  the  principle  leads  to

contradictions. Suppose, for instance, you know nothing of the colour of a

certain book; then the chances of its being blue or not blue are equal, and

therefore each is 1/2. Similarly the chance of its being black is 1/2. Therefore

the chance of its being blue or black is 1. It follows that all books are either

blue  or  black,  which  is  absurd.  Or  suppose  we  know  that  a  certain  man

inhabits either Great Britain or Ireland, shall we take these as our alternatives,

or shall we take England, Scotland, and Ireland, or shall we take each county

as equally probable? Or, if we know that the specific gravity of a certain

substance lies between 1 and 3, shall we take the intervals 1 to 2 and 2 to 3 asequally probable? But if we consider specific volume, the intervals 1 to 2/3 and

2/3 to 1/3 would be the natural choice, which would make the specific gravity

have equal chances of being between 1 and 3/2 or between 3/2 and 3. Such

paradoxes can be multiplied indefinitely.

  Keynes  does  not,  on  this  account,  totally  abandon  the  principle  of

indifference; he thinks it can be so stated as to avoid the above difficulties and

still be useful. For this purpose, he first defines “irrelevance”.

    Roughly speaking, an added premiss is “irrelevant” if it does not change

the probability; i.e. h is irrelevant in relation to x and h if x/hh = x/h. Thus,
                      11

for example, the fact that a man’s surname begins with M is irrelevant in

estimating his chances of death. The above definition is, however, somewhat

too simple, because h might consist of two parts, of which one increased the
                        1

probability of x while the other diminished it. For example: a white man’s

chances of life are diminished by living in the tropics, but are increased (or so

they say) by being a teetotaller. It may be that the death-rate among white

teetotallers in the tropics is the same as that of white men in general, but we

should not say that being a teetotaller who lives in the tropics was irrelevant.

Therefore we say that h is irrelevant to x/h if there is no part of h which
                            11

alters the probability of x.

  Keynes now states the principle of indifference in the following form: The

probabilities of a and b relative to given evidence are equal if there is no

relevant evidence relating to a without corresponding evidence relating to b;

that is to say, the probabilities of a and b relative to the evidence are equal, if

the evidence is symmetrical with respect to a and b.

  There is, however, still a somewhat difficult proviso to be added. “We

must exclude those cases, in which one of the alternatives involved is itself a

disjunction  of  sub-alternatives of  the  same  form.”  When  this  condition  is

fulfilled,  the  alternatives  are  called indivisible  relatively  to  the  evidence.

Keynes gives a formal definition of “divisible” as follows: An alternative φ(a)

is divisible, relatively to evidence h, if, given h, “φ(a)” is equivalent to “φ(b) or

φ(c)”, where φ(b) and φ(c) are incompatible, but each possible, when h is true.

It  is  essential,  here,  that  φ(a),  φ(b),  φ(c)  are  all  values  of  the  same

propositional function.  Keynes  thus  finally  accepts  as  an  axiom  the  principle  that,  on  given

evidence, φ(a) and φ(b) are equally probable, if (1) the evidence is symmetrical

with  respect  to a and b,  (2)  relatively  to  the  evidence,  φ(a)  and  φ(b) are

indivisible.

  To  the  above  theory  empiricists  might  raise  a  general  objection.  They

might say that the direct knowledge of probability relations which it demands

is  obviously  impossible.  Deductive  demonstrative  logic—so  this  argument

might run—is possible because it consists of tautologies, because it merely re-

states our initial stock of propositions in other words. When it does more than

this—when,  for  instance,  it  infers  “Socrates  is  mortal”  from  “all  men  are

mortal”, it depends upon experience for the meaning of the word “Socrates”.

Nothing  but  tautologies  can  be  known  independently  of  experience,  and

Keynes does not contend that his probability-relations are tautologous. How,

then, can they be known? For clearly they are not known by experience, in

the sense in which judgments of perception are so known; and it is admitted

that  some  of  them  are  not  inferred.  They  would  constitute,  therefore,  if

admitted, a kind of knowledge which empiricism holds to be impossible.

  I have much sympathy with this objection, but I do not think we can

consider it decisive. We shall find, when we come to discuss the principles of

scientific inference, that science is impossible unless we have some knowledge

which we could not have if empiricism, in a strict form, were true. In any

case, we should not assume dogmatically that empiricism is true, though we

are justified in trying to find solutions of our problems which are compatible

with  it.  The  above  objection,  therefore,  though  it  may  cause  a  certain

reluctance to accept Keynes’s theory, should not make us reject it outright.

  There  is  a  difficulty  on  a  question  which  Keynes  seems  not  to  have

adequately considered, namely: Does probability in relation to premisses ever

confer rational credibility on the proposition which is rendered probable, and,

if so, under what circumstances? Keynes says that it is as nonsensical to say

“p is probable” as to say “p is equal” or “p is greater than”. There is, according

to him, nothing analogous to the dropping of a true premiss in deductive

inference. Nevertheless, he says that, if we know h, and we also know p/h = α,

we are entitled to give to p “rational belief in the appropriate degree”. Butwhen we do so we are no longer expressing a relation of p to h; we are using

this relation to infer something about p. This something we may call “rational

credibility”, and we can say; “p is rationally credible to the degree α”. But if

this is to be a true statement about p, not involving mention of h, then h

cannot be arbitrary. For suppose p/h = α and p/h’ = α’, are we, supposing h

and h’ both known, to give to p the degree a or the degree a’ of rational

credibility? It is impossible that both answers should be correct in any given

state of our knowledge.

  If it is true that “probability is the guide of life”, then there must be, in any

given state of our knowledge, one probability which attaches to p more vitally

than any other, and this probability cannot be relative to arbitrary premisses.

We must say that it is the probability which results when h is taken to be all

our  relevant  knowledge.  We  can  say:  Given  any  body  of  propositions

constituting some person’s certain knowledge, and calling the conjunction of

this body of propositions h, there are a number of propositions, not members

of this body, which have probability-relations to it. If p is such a proposition,

and p/h = α, then α is the degree of rational credibility belonging to p for that

person. We must not say that, if h’ is some true proposition, short of h, which

the person in question knows, and if p/h’ = α’, then, for that person, p has the

degree of credibility α’; it will only have this degree of credibility for a person

whose relevant knowledge is summed up by h’. All this, however, no doubt

Keynes would admit. The objection is, in fact, only to a certain looseness of

statement, not to anything essential to the theory.

  A more vital objection is as to our means of knowing such propositions as

p/h = α. I am not now arguing a priori that we cannot know them; I am

merely inquiring how we can. It will be observed that if “probability” cannot

be defined, there must be probability-propositions which cannot be proved,

and which, therefore, if we are to accept them, must be among the premisses

of our knowledge. This is a general feature of all logically articulated systems.

Every such system starts, of necessity, with an initial apparatus of undefined

terms and unproved propositions. It is obvious that an undefined term cannot

appear in an inferred proposition unless it has occurred in at least one of the

unproved propositions; but a defined term need not occur in any unprovedproposition. For example: so long as there were held to be undefined terms in

arithmetic, there had to be also unproved axioms: Peano had three undefined

terms and five axioms. But when numbers and addition are defined logically,

arithmetic needs no unproved propositions beyond those of logic. So, in our

case, if “probability” can be defined, it may be that all propositions in which

the word occurs can be inferred; but if it cannot be defined, there must, if we

are to know anything about it, be propositions, containing the word, which

we know without extraneous evidence.

  It  is  not  quite  clear  what  sort  of  propositions  Keynes  would  admit  as

premisses in our knowledge of probability. Do we directly know propositions

of the form “p/h = α”? And when a probability is not numerically measurable,

what sort of thing is α? Or do we only know equalities and inequalities, i.e.

p/h < q/h, or p/h = q/h) I incline to think that the latter is Keynes’s view. If so,

the fundamental facts in the subject are relations of three propositions, not of

two: we ought to start from a triadic relation




•  meaning: given h, p is less probable than q. We might then say:




  We should assume that P is asymmetrical and transitive with respect to p

and q while h is kept constant. Keynes’s principle of indifference, if accepted,

will then enable us, in certain circumstances, to prove p/h = q/h. And from

this basis the calculus of probabilities, in so far as Keynes considers it valid,

can be built up.

  The above definition of equality can only be adopted if p/h and q/h are

comparable; if (as Keynes holds possible) neither is greater than the other and

yet they are not equal, the definition must be abandoned. We could meet this

difficulty by axioms as to the circumstances under which two probabilities

must  be  comparable.  When  they  are  comparable  they  lie  on  one  route

between o and i. On the right-hand side of the above definition of “p/h = q/h”

we must then add that p/h and q/h are “comparable”.  Let us now re-state Keynes’s principle of indifference. He is concerned to

establish circumstances in which p/h = q/h. This will happen, he says, if two

conditions (sufficient but not necessary) are fulfilled. Let p be of the form φ(a)

and q of the form φ(b); then h must be symmetrical with respect to a and b,

and φ(a), φ(b) must be “indivisible”.

  When we say that h is symmetrical with respect to a and b, we mean,

presumably, that, if h is of the form f (a, b), then




  This will happen, in particular, if f(a, b) is of the form g(a) · g(b), which is

the case when the information that h gives about a and b consists of separate

propositions, one about a and the other about b, and both are values of one

propositional function.




  Our  axiom  must  be  to  the  effect  that,  with  a  suitable  proviso,  the

interchange of φ(a) and φ(b) cannot make any difference. This involves that




•  provided φ(a) and φ(b) are comparable with respect to f(a, b). This follows

if, as a general principle,




•  that is to say, if probability depends not on the particular subject but on

propositional functions. There seems hope, along these lines, of arriving at a

form of the principle of indifference which might have more self-evidence

than Keynes’s.

  Let us, for this purpose, examine his condition of indivisibility. Keynes

defines “φ(a) is divisible” as meaning that there are two arguments b and c

such that “φ(a)” is equivalent to “φ(b) or φ(c)” and φb and φc cannot both be

true, while φb, φc are both possible given h. I do not think this is quite whathe really wishes to say. We get nearer to what he wishes, I think, if we assume

a and b and c to be classes, of which a is the sum of b and c. In that case, φ

must be a function which takes classes as arguments. E.g. let a be an area on a

target, divided into two parts b and c. Let “φa” be “some point of a is hit” and

“ψa” be “some point of a is aimed at”. Then φa is divisible in the above sense,

and we do not have




•  for obviously φa/ψa is greater than φb/ψb.

  But it is not clear that our earlier condition, namely that h should be

symmetrical with respect to a and b, does not suffice. For now h contains the

proposition “b is part of a”, which is not symmetrical.

  Keynes  discusses  the  conditions  for  φa/ψa  =  φb/ψb,  and  gives  as  an

example of failure the case where φx. = . x is Socrates. In that case, no matter

what ψx may be,




•  while if b is not Socrates, φb/ψb = 0.

  To exclude this case, I should make the proviso that “φx” must not contain

“a”. To take an analogous case, put




  Then φa/ψa is the likelihood of a committing suicide if English, whereas

φx/ψx, in general, is the likelihood of a being murdered by some Englishman

who  is  named x.  Obviously,  in  most  cases,  φa/ψa  is  greater  than  φb/ψb,

because  a  man  is  more  likely  to  kill  himself  than  to  kill  another  person

selected at random.

  The essential condition, then, seems to be that “φx” must not contain “a”

or “b”. If this condition is fulfilled, I do not see how we can fail to have  I conclude that what the principle of indifference really asserts is that

probability  is  a  relation  between  propositional  functions,  not  between

propositions. This is what is meant by such phrases as “a random selection”.

This phrase means that we are to consider a term solely as one satisfying a

certain  propositional  function;  what  is  said  is,  then,  really  about  the

propositional function and not about this or that value of it.

    Nevertheless, there remains something substantial which is what really

concerns us. Given a probability-relation between two propositional functions

φx and ψx, we can regard this as a relation between φa and ψ>a, provided

“φx” and “ψx” do not contain “a”. This is a necessary axiom in all applications

of probability in practice, for then it is particular cases that concern us.

  My  conclusion  is  that  the  chief formal  defect  in  Keynes’s  theory  of

probability  consists  in  his  regarding  probability  as  a  relation  between

propositions rather than between propositional functions. The application to

propositions, I should say, belongs to the uses of the theory, not to the theory

itself.                                          VI 


                      Degrees of Credibility



A. General Considerations


THAT ail human knowledge is in a greater or less degree doubtful is a doctrine

that comes to us from antiquity; it was proclaimed by the sceptics, and by the

Academy in its sceptical period. In the modern world it has been strengthened

by  the  progress  of  science.  Shakespeare,  to  represent  the  most  ridiculous

extremes of scepticism, says:


  Doubt that the stars are fire,


  Doubt that the sun doth move.

  The latter, when he wrote, had already been questioned by Copernicus,

and was about to be even more forcibly questioned by Kepler and Galileo. The

former is false, if “fire” is used in its chemical sense. Many things which had

seemed indubitable have turned out to be in all likelihood untrue. Scientific

theories themselves change from time to time, as new evidence accumulates;

no prudent man of science feels the same confidence in a recent scientific

theory as was felt in the Ptolemaic theory throughout the middle ages.

  But although every part of what we should like to consider “knowledge”

may be in some degree doubtful, it is clear that some things are almost certain,

while others are matters of hazardous conjecture. For a rational man, there is

a scale of doubtfulness, from simple logical and arithmetical propositions, and

perceptive judgments, at one end, to such questions as what language the

Myceneans spoke or “what song the Sirens sang”, at the other. Whether any

degree  of  doubtfulness  attaches  to  the  least  dubitable  of  our  beliefs,  is  a

question with which we need not at present concern ourselves; it is enough

that any proposition concerning which we have rational grounds for some

degree of belief or disbelief can, in theory, be placed in a scale between certaintruth  and  certain  falsehood.  Whether  these  limits  are  themselves  to  be

included, we may leave an open question.

  There  is  a  certain  connection  between  mathematical  probability and

degrees  of  credibility.  The  connection  is  this:  When,  in  relation  to  all  the

available evidence, a proposition has a certain mathematical probability, then

this measures its degree of credibility. For instance, if you are about to throw

dice, the proposition “double sixes will be thrown” has only one thirty-fifth of

the credibility attaching to the proposition “double sixes will not be thrown”.

Thus the rational man, who attaches to each proposition the right degree of

credibility, will be guided by the mathematical theory of probability when it is

applicable.

  The  concept  “degree  of  credibility”,  however,  is  applicable  much  more

widely than that of mathematical probability; I hold that it applies to every

proposition except such as neither are data nor are related to data in any way

which is favourable or unfavourable to their acceptance. I hold, in particular,

that  it  applies  to  propositions  that  come  as  near  as  is  possible  to  merely

expressing data. If this view is to be logically tenable, we must hold that the

degree of credibility attaching to a proposition is itself sometimes a datum. I

think we should also hold that the degree of credibility to be attached to a

datum is sometimes a datum, and sometimes (perhaps always) falls short of

certainty. We may hold, in such a case, that there is only one datum, namely,

a proposition with a degree of credibility attached to it, or we may hold that

the  datum  and  its  degree  of  credibility  are  two  separate  data.  I  shall  not

consider which of these two views should be adopted.

  A proposition which is not a datum may derive credibility from various

different sources; a man who wishes to prove his innocence of a crime may

argue both from an alibi and from his previous good character. The grounds

in favour of a scientific hypothesis are practically always composite. If it is

admitted that a datum may not be certain, its degree of credibility may be

increased by an argument, or, on the contrary, may be rendered very small by

a counter-argument.

  The degree of credibility conferred by an argument is not capable of being

estimated simply. Take, first, the simplest possible case, namely that in whichthe premisses are certain and the argument, if valid, is demonstrative. At each

step  we  have  to  “see”  that  the  conclusion  of  this  step  follows  from  its

premisses. Sometimes this is easy, for example if the argument is a syllogism

in Barbara. In such a case, the degree of credibility attaching to the connection

of premisses and conclusion is almost certainty, and the conclusion has almost

the same degree of credibility as the premisses. But in the case of a difficult

mathematical argument the chance of an error in reasoning is much greater.

The logical connection may be completely obvious to a good mathematician,

while to a pupil it is barely perceptible, and that only at moments. The pupil’s

grounds for believing in the validity of the step are not purely logical; they are

in  part  arguments  from  authority.  These  arguments  are  by  no  means

demonstrative, for even the best mathematicians sometimes make mistakes.

On such grounds, as Hume points out, the conclusion of a long argument has

less certainty than the conclusion of a short one, for at each step there is some

risk of error.

  By means of certain simplifying hypotheses, this source of uncertainty

could be brought within the scope of the mathematical theory of probability.

Suppose  it  established  that,  in  a  certain  branch  of  mathematics,  good

mathematicians are right in a step in their arguments in a proportion x of all

cases; then the chance that they are right throughout an argument of n steps is

n
x. It follows that a long argument which has not been verified by repetition

runs an appreciable risk of error, even if x is nearly 1. But repetition can

reduce the risk until it becomes very small. All this is within the scope of the

mathematical theory.

  What,  however,  is  not  within  the  scope  of  that  theory  is  the  private

conviction  of  the  individual  mathematician  as  he  takes  each  step.  This

conviction will vary in degree according to the difficulty and complexity of

the step; but in spite of this variability it must be as direct and immediate as

our  confidence  in  objects  of  perception.  To  prove  that  a  certain  premiss

implies a certain conclusion, we must “see” each step; we cannot prove the

validity of the step except by breaking it up into smaller steps, each of which

will then have to be “seen”. Unless this is admitted, all arguments will be lost

in an endless regress.  I have been speaking, so far, of demonstrative inference, but as regards our

present question non-demonstrative inference presents no new problem, for,

as we have seen, even demonstrative inference, when carried out by human

beings, only confers probability on the conclusion. It cannot even be said that

reasoning which professes to be demonstrative always confers a higher degree

of  probability  on  the  conclusion  than  reasoning  which  is avowedly  only

probable; of this there are many examples in traditional metaphysics.

  If—as I believe, and as I shall argue in due course—data, as well as results

of inference, may be destitute of the highest attainable degree of credibility,

the epistemological relation between data and inferred propositions becomes

somewhat complex. I may, for instance, think that I recollect something, but

find reason to believe that what I seemed to recollect never happened; in that

case I may be led by argument to reject a datum. Conversely, when a datum

has, per  se,  no  very  high  degree  of  credibility,  it  may  be  confirmed  by

extraneous evidence; for example, I may have a faint memory of dining with

Mr. So-and-So some time last year, and may find that my diary for last year

has an entry which corroborates my recollection. It follows that every one of

my beliefs may be strengthened or weakened by being brought into relation

with other beliefs.

  The relation between data and inferences, however, remains important,

since the reason for believing no matter what must be found, after sufficient

analysis, in data, and in data alone. (I am here including among data the

principles used in any inferences that may be involved.) What does result is

that the data relevant to some particular belief may be much more numerous

than they appear to be at first sight. Take again the case of memory. The fact

that I remember an occurrence is evidence, though not conclusive evidence,

that  the  occurrence  took  place.  If  I  find  a  contemporary  record  of  the

occurrence, that is confirmatory evidence. If I find many such records, the

confirmatory evidence is strengthened. If the occurrence is one which, like a

transit of Venus, is made almost certain by a well-established scientific theory,

this fact must be added to the records as an additional ground for confidence.

Thus while there are beliefs which are only conclusions of arguments, there

are none which, in a rational articulation of knowledge, are only premisses. Insaying this, I am speaking in terms of epistemology, not of logic.

  Thus an epistemological premiss may be defined as a proposition which

has some degree of rational credibility on its own account, independently of

its  relations  to  other  propositions.  Every  such  proposition  can  be  used  to

confer some degree of credibility on propositions which either follow from it

or  stand  in  a  probability  relation  to  it.  But  at  each  stage  there  is  some

diminution of the original stock of credibility; the case is analogous to that of

a  fortune  which  is  lessened  by  death  duties  on  each  occasion  when  it  is

inherited.  Carrying  the  analogy  a  little  further,  we  may  say  that  intrinsic

credibility is like a fortune acquired by a man’s own efforts, while credibility

as the result of an argument is like inheritance. The analogy holds in that a

man who has made a fortune can also inherit one, though every fortune must

owe its origin to something other than inheritance.

  In  this  chapter  I  propose  to  discuss  credibility,  first  in  relation  to

mathematical  probability,  then  in  relation  to  data,  then  in  relation  to

subjective certainty, and finally in relation to rational behaviour.B. Credibility and Frequency


I am now concerned to discuss the question: In what circumstances is the

credibility of a proposition a derived from the frequency of ψx given some

φx?  In  other  words,  if  “φa”  is  “a  is  an  α”,  in  what  circumstances  is  the

credibility of “a is a β” derived from one or more propositions of the form: “A

proportion m/n of the members of α are members of β”? This question, we

shall find, is not quite so general as the one we ought to ask, but it will be

desirable to discuss it first.

  It seems clear to common sense that, in the typical cases of mathematical

probability, it is equal to degree of credibility. If I draw a card at random from

a pack, the degree of credibility of “the card will be red” is exactly equal to

that of “the card will not be red”, and therefore the degree of credibility of

either  is  1/2,  if  1  represents  certainty.  In  the  case  of  a  die,  the  degree  of

credibility of “1 will come uppermost” is exactly the same as that of “2 will

come uppermost”, or 3 or 4 or 5 or 6. Hence all the derived frequencies of the

mathematical theory can be interpreted as derived degrees of credibility.

  In this translation of mathematical probabilities into degrees of credibility,

we make use of a principle which the mathematical theory does not need. The

mathematical theory merely counts cases; but in the translation we have to

know, or assume, that each case is equally credible. The need of this principle

has long been recognized; it has been called the principle of non-sufficient

reason,  or  (by  Keynes)  the  principle  of  indifference.  We  considered this

principle in connection with Keynes, but we must now consider it on its own

account. Before discussing it, I wish to point out that it is not needed in the

mathematical theory of probability. In that theory, we only need to know the

numbers of various classes. It is only when mathematical probability is taken

as a measure of credibility that the principle is required.

  What  we  need  is  something  like  the  following:  “Given  an  object a,

concerning which we wish to know what degree of credibility to attach to the

proposition ‘a is a β’, and given that the only relevant knowledge we have is

‘a is an α’, then the degree of credibility of ‘a is a β’ is the mathematicalprobability measured by the ratio of the number of members common to α

and β to the number of members of α”.

  Let us illustrate this by considering once more the tallest person in the

United States, and the chance that he lives in Iowa. We have here, on the one

hand, a description d, known to be applicable to one and only one of a number

of named persons A1, A2, … An, where n is the number of inhabitants of the

United States. That is to say, one and only one of the propositions “d = Ar”

(where r runs from 1 to n) is known to be true, but we do not know which. If

this  is  really  all  our  relevant  knowledge,  we  assume  that  any  one  of  the

propositions “d = Ar“ is as credible as any other. In that case, each has a

credibility 1/n. If there are m inhabitants of Iowa, the proposition “d inhabits

Iowa” is equivalent to a disjunction of m of the propositions “d = Ar’, and

therefore  has m  times  the  credibility  of  any  one  of  them,  since  they  are

mutually exclusive. Therefore it has a degree of credibility measured by m/n.

  Of course in the above illustration the propositions “d = Ar“ are not all on

a level. The evidence enables us to exclude children and dwarfs, and probably

women. This shows that the principle may be difficult to apply, but does not

show that it is false.

  The case of drawing a card from a pack comes nearer to realizing the

conditions required by the principle. Here the description “d” is “the card I am

about to draw”. The 52 cards all have what we may regard as names: “2 of

spades”, etc. We have thus 52 propositions “d = Ar”, of which one and only

one is true, but we have no evidence whatever inclining us to one rather than

another. Therefore the credibility of each is 1/52. This, if admitted, connects

credibility with mathematical probability.

  We  may  therefore  enunciate,  as  a  possible  form  of  the  “principle  of

indifference”, the following axiom:

  “Given a description d, concerning which we know that it is applicable to

one and only one of the objects a, a, … a,  and  given  that  we  have  no
                                          12n

knowledge  bearing  on  the  question  which  of  these  objects  the  description

applies to, then the n propositions ‘d = a’ (1 ⩽ r ⩽ n) are all equally credible,
                                              r

and therefore each has a credibility measured by 1/n.”

  This axiom is more restricted than the principle of non-sufficient reason asusually  enunciated.  We  have  to  inquire  whether  it  will  suffice,  and  also

whether we have reason to believe it.

  Let us first compare the above with Keynes’s principle of indifference,

discussed in an earlier chapter. It will be remembered that his principle says:

the probabilities of p and q relative to given evidence are equal if (1) the

evidence is symmetrical with respect to p and q, (2) p and q are “indivisible”,

i.e. neither is a disjunction of propositions of the same form as itself. We

decided that this could be simplified: what is needed, we said, is that p and q

should be values of one propositional function—say p = φ(a) and q = φ(b); that

“φx” should not contain either a or b; and that, if the evidence contains a

mention of a, say in the form ψ(a), it must also contain ψ(b), and vice versa,

where ψx, in turn, must not mention a or b. This principle is somewhat more

general  than  the  one  enunciated  in  the  previous  paragraph:  it  implies  the

latter, but I doubt whether the latter implies it. We may perhaps accept the

more general principle, and re-state it as follows:

  “Given two propositional functions φx, ψx, neither of which mentions a or

b, or, if it does so, mentions them symmetrically, then, given ψa and ψb, the

two propositions φa, φb have equal credibility”.

  This  principle,  if  accepted,  enables  us  to  infer  credibility  from

mathematical probability, and makes all the propositions of the mathematical

theory available for measuring degrees of credibility in the cases to which the

mathematical theory is applicable.

  Let us apply the above principle to the case of n balls in a bag, each of

which  is  known  to  be  either  white  or  black;  the  question  is:  what  is  the

probability that there are x white balls? Laplace assumed that every value of x

from 0 to n is equally likely, so that the probability of a given x is 1/(n + 1).

From a purely mathematical standpoint, this is legitimate, provided we start

from the propositional function:


  x = the number of white balls.

  But if we start from the propositional function:


  x is a white ball,•  we obtain a quite different result. In this case, there are many ways of

choosing x balls. The first ball can be chosen in n ways; when it has been

chosen, the next can be chosen in n − 1 ways, and so on. Thus the number of

ways of choosing x balls is


  n times (n − 1) times (n − 2) times … times (n − x + 1).

  This is the number of ways in which there can be x white balls. To get the

probability of x white balls, we have to divide this number by the sum of the

numbers of ways of choosing 0 white balls, or 1, or 2, or 3, or … or n. This sum

                            n
is easily shown to be 2. Therefore the chance of exactly x white balls is

                                                  n
obtained by dividing the above number by 2. Let us call it “p(n, r)”.

  This has a maximum when x = ½n if n is even, or when x = ½n ± ½ if n is

odd. Its value when x or n − x is small is very small if n is large. From the

purely mathematical point of view, these two very different results are equally

legitimate. But when we come to the measurement of degrees of credibility,

there is a great difference between them. Let us have some way, independent

of colour, by which we can distinguish the balls; e.g. let them be successively

drawn out of the bag, and let us call the one first drawn d, the one drawn
                                                                      1

second d, and so on. Put “a” for “white” and “b” for “black”, and put “φa” for
        2

“white is the colour of d”, “φb” for “black is the colour of d”. The evidence is
                          11

that φa or φb is true, but not both. This is symmetrical, and therefore, on the

evidence, φa and φb have equal credibility, i.e. “d is white” and “d is black”
                                                          11

have equal credibility. The same reasoning applies to d, d, … d. Thus in the
                                                                23n

case of each ball the degrees of credibility of white and black are equal. And

therefore, as a simple calculation shows, the degree of credibility of x white

balls is p(n, x), where it is assumed that x lies between 0 and n, both included.

  It is to be observed that, in measuring degrees of credibility, we suppose

the data not only true, but exhaustive in relation to our knowledge, i.e. we

assume that we know nothing relevant except what is mentioned in the data.

Therefore for a given person at a given time there is only one right value for

the degree of credibility of a given proposition, whereas in the mathematical

theory many values are equally legitimate in relation to many different data,

which may be purely hypothetical.  In  applying  the  results  of  the  mathematical  calculus  of  probability  to

degrees of credibility, we must be careful to fulfil two conditions. First, the

cases  which  form  the  basis  of  the  mathematical  enumeration  must  all  be

equally credible on the evidence; second, the evidence must include all our

relevant knowledge. As to the former of these conditions a few words must be

said.

  Every  mathematical  calculation  of  probability  starts  from  some

fundamental class, such as a certain number of tosses of a coin, a certain

number of throws of a die, a pack of cards, a collection of balls in a bag. Each

member of this fundamental class counts as one. From it we manufacture

other logically derivative classes, e.g. a class of n series of 100 tosses of a coin.

Out of these n series we can pick out the sub-class of those that consist of 50

heads and 50 tails. Or, starting from a pack of cards, we can consider the class

of possible “hands”, i.e. selections of 13 cards, and proceed to inquire how

many of these contain 11 cards of one suit. The point is that the frequencies

that are calculated always apply to classes having some structure logically

defined in relation to the fundamental class, whereas the fundamental class,

for the purposes of the problem, is regarded as composed of members having

no logical structure, i.e. their logical structure is irrelevant.

  So long as we confine ourselves to the calculation of frequencies, i.e. to the

mathematical theory of probability, we can take any class as our fundamental

class, and calculate frequencies in relation to it. It is not necessary to make an

assumption to the effect that all the members of the class are equally probable;

all that we need to say is that, for the purpose in hand, each member of the

class is to count as one. But when we wish to ascertain degrees of credibility,

it is necessary that our basic class should consist of propositions which are all

equally  credible  in  relation  to  the  evidence.  Keynes’s  “indivisibility”  is

intended  to  secure  this.  I  should  prefer  to  say  that  the  members  of  the

fundamental class must have “relative simplicity”, i.e. they must not have a

structure definable in terms of the data. Take, e.g., white and black balls in a

bag.  Each  ball  has,  in  fact,  an  incredibly  complicated  structure,  since  it

consists of billions of molecules; but this is quite irrelevant to our problem. On

the other hand, a collection of m balls chosen from a fundamental class of nballs  has  a  logical  structure  relatively  to  the  fundamental  class.  If  each

member of the fundamental class has a name, every sub-class of m terms can

be defined. All calculations of probability have to do with classes which can

be defined in terms of the fundamental class. But the fundamental class itself

must consist of members which cannot be logically defined in terms of the

data. I think that when this condition is fulfilled the principle of indifference is

always satisfied.

  At this point, however, a caution is necessary. There are two ways in

which “a is an α” may become probable, either (1) because it is certain that a

belongs to a class most of which are α’s, or (2) because it is probable that a

belongs to a class all of which are α’s. For instance, we may say “Mr. A. is

probably mortal” if we are sure that most men are mortal, or if we have

reason to think it probable that all men are mortal. When we make a throw

with two dice, we can say “probably we shall not throw double sixes”, because

we know that most throws are not double sixes. On the other hand, suppose I

have evidence suggesting, but not proving, that a certain bacillus is always

present in a certain disease; I may then say, in a given case of this disease, that

probably the bacillus in question is present. There is in each case a kind of

syllogism. In the first case,


  Most A is B;


  This is an A;


    Therefore this is probably a B.

  In the second case,


  Probably all A is B;


  This is an A;


    Therefore this is probably a B.

  The second case, however, is more difficult to reduce to a frequency. Let

us inquire whether this is possible.

  In some cases, this is clearly possible. E.g. most words do not contain theletter Z. Therefore, if some word is chosen at haphazard, it is probable that all

its letters are other than Z. Thus if A = the class of letters in the word in

question, and B = the class of letters other than Z, we get a case of our second

pseudo-syllogism. The word, of course, must be defined in some way which

                                                                              th
leaves us in temporary ignorance as to what it is, e.g. the 8,000 word in

Hamlet,  or  the  third  word  on  p.  248  of  the Concise  Oxford  Dictionary.

Assuming that you do not at present know what these words are, you will be

wise to bet against their containing a Z.

  In all cases of our second pseudo-syllogism, it is clear that what I have

been  calling  the  “fundamental  class”  is  given  as  a  class  of  classes,  and

therefore its logical structure is essential. To generalize the above instance: let

κ be a class of classes, such that most of its members are entirely contained in

a certain class β; then, from “x is an α” and “α is a κ” we can conclude “x is

probably a β”. (In the above instance, κ was the class of words, α the class of

letters in a certain word, and β the alphabet without Z.) The odd thing is that,

denoting by “sum of κ” the class of members of members of κ, our premisses

do not suffice to prove that a member of the sum of κ is probably a member of

β. For example, let κ consist of the three words STRENGTH, QUAIL, MUCK,

together with all words containing no letter occurring in any of these three.

Then the sum of κ consists of all the letters of the alphabet, possibly excepting

1
Z. But “x is an α and α is a κ” makes it probable that x is not one of the letters

occurring in the above three words, while “x is a member of the sum of κ”

does not make this probable. This illustrates the complications that arise when

the fundamental class has a structure which is relevant to the probabilities.

But in such cases as the above it is still possible to measure credibility by

frequency, though less simply.

  There is, however, another and more important class of cases, which we

cannot adequately discuss except in connection with induction. These are the

cases where we have inductive evidence making it probable that all A is B,

and we infer that a particular A is probably a B, e.g. probably all men are

mortal  (not  all  men  are  probably  mortal),  therefore  Socrates  is  probably

mortal. This is a pseudo-syllogism of our second kind. But if the “probably” in

“probably all men are mortal” can be reduced to a frequency, it certainlycannot be so reduced at all simply. I will therefore leave this class of cases to

be discussed at a later stage.

  There are, we shall find, various examples of degrees of credibility not

derivable from frequencies. These I shall now proceed to consider.C. Credibility of Data


In the present section I propose to advocate an unorthodox opinion, namely,

that a datum may be uncertain. There have been hitherto two views: first, that

in  a  proper  articulation  of  knowledge  we  start  from  premisses  which  are

certain in their own right, and may be defined as “data”; second, that, since no

knowledge is certain, there are no data, but our rational beliefs form a closed

system in which each part lends support to every other part. The former is the

traditional view, inherited from the Greeks, enshrined in Euclid and theology;

the latter is a view first advocated, if I am not mistaken, by Hegel, but most

influentially supported, in our day, by John Dewey. The view which I am

about to set forth is a compromise, but one somewhat more in favour of the

traditional theory than of that advocated by Hegel and Dewey.

  I define a “datum” as a proposition which has some degree of rational

credibility on its own account, independently of any argument derived from

other propositions. It is obvious that the conclusion of an argument cannot

derive from the argument a higher degree of credibility than that belonging to

the premisses; consequently, if there is such a thing as rational belief, there

must be rational beliefs not wholly based on argument. It does not follow that

there  are  beliefs  which  owe none  of  their  credibility  to  argument,  for  a

proposition may be both inherently credible and also a conclusion from other

propositions  that  are  inherently  credible.  But  it  does  follow  that  every

proposition which is rationally credible in any degree must be so either (a)

solely in its own right, or (b) solely as the conclusion from premisses which

are rationally credible in their own right, or (c) because it has some degree of

credibility in its own right, and also follows, by a demonstrative or probable

inference, from premisses which have some degree of credibility in their own

right. If all propositions which have any credibility in their own right are

certain,  case  (c)  has  no  importance,  since  no  argument  can  make  such

propositions more certain. But on the view which I advocate, case (c) is of the

greatest importance.

  The traditional view is adopted by Keynes, and set forth by him in hisTreatise on Probability, p. 16. He says:

  “In order that we may have a rational belief in p of a lower degree of

probability than certainty, it is necessary that we know a set of propositions h,

and also know some secondary proposition q asserting a probability-relation

between p and h.

  “In the above account one possibility has been ruled out. It is assumed that

we cannot have a rational belief in p of a degree less than certainty except

through knowing a secondary proposition of the prescribed type. Such belief

can only arise, that is to say, by means of the perception of some probability-

relation. … All knowledge which is obtained in a manner strictly direct by

contemplation  of  the  objects  of  acquaintance  and  without  any  admixture

whatever of argument and the contemplation of the logical bearing of any

other knowledge on this, corresponds to certain rational belief and not to a

merely probable degree of rational belief.”

  I propose to controvert this view. For this purpose I shall consider (1) faint

perception, (2) uncertain memory, (3) dim awareness of logical connection.


(1)  Faint perception.—Consider such familiar experiences as the following. (a)

  You hear an aeroplane going away; at first you are sure you hear it, and at

  last you are sure you do not hear it, but in the interval there is a period

  during which you are not sure whether you still hear it or not. (b) You are

  watching  Venus  during  the  dawn;  at  first  you  see  the  planet  shining

  brightly, and at last you know that daylight has made it invisible, but

  between these two times you may be in doubt whether you are still seeing

  it or not. (c) In the course of travel you have attracted a number of fleas;

  you set to work to get rid of them, and in the end you are sure you have

    succeeded, but in the meantime you are troubled by occasional doubtful

  itches. (d) By mistake you make tea in a pot that has contained vinegar;

  the  result  is  appalling.  You  rinse  the  pot  and  try  again,  but  still  the

    offensive flavour is unmistakeable. After a second rinsing you are doubtful

  whether you still taste the vinegar; after a third you are sure you do not.

  (e) Your drains are out of order, and you call in the plumber. At first, after

  his visit, you feel sure that the offensive odour is gone, but gradually,  through varying stages of doubt, you become certain that it has returned.

•  Such experiences are familiar to every one, and must be taken account of in

any theory as to the knowledge based on sense-perception.

(2)   Uncertain  memory.—In The  Tempest  (Act  I,  Scene  II),  Prospero  asks

  Miranda to look into “the dark backward and abysm of time”; she says

  “had  I  not  four  or  five  women  once  that  tended  me?”  and  Prospero

    confirms her doubtful recollection. We all have memories of this kind,

  about which we do not feel sure. Usually, if it is worth while, we can

  discover from other evidence whether they are veridical or not, but that is

    irrelevant to our present thesis, which is that they have a certain degree of

    credibility on their own account, though this degree may fall far short of

  full certainty. A recollection which has a fairly high degree of credibility

    contributes its quota to our grounds for believing in some past occurrence

  for which we have other evidence. But here a distinction is necessary. The

  past event I uncertainly remembered has partial credibility in itself; but

  when I adduce the recollection as a ground for belief, I am no longer

  treating the past occurrence as a datum, for it is not it but the present

    recollecting that is my datum. My recollecting confers some credibility on

  what is recollected; how much credibility, we can more or less ascertain

    inductively by a statistical inquiry into the frequency of errors of memory.

  But this is a different matter from past occurrences as data. That such data

  must be supplied by memory is a thesis which I have argued elsewhere.

(3)  Dim awareness of logical connection.—Any person whose mathematical

  abilities are not almost superhuman must, if he has studied mathematics,

  have often had the experience of being hardly able to “see” a certain step

  in a proof. The process of following a proof is facilitated by making the

  steps very small, but however small we make them some of them may

  remain difficult if the subject-matter is very complex. It is obvious that, if

  we have made the steps as small as possible, each step must be a datum,

  for otherwise every attempt at proof would involve an endless regress.

  Consider, say, a syllogism in Barbara. I say “all men are mortal”, and you

  agree.  I  say  “Socrates  is  a  man”,  and  you  agree.  I  then  say  “therefore

  Socrates is mortal”, and you say “I don’t see how that follows”. What,  then, can I do? I can say: “Don’t you see that if f(x) is always true, then

  f(a) is true? and don’t you see that therefore if φ(x) always implies ψ(x),

  then φ (Socrates) implies ψ (Socrates)? and don’t you see that I can put ‘x

  is a man’ for ‘φx’ and ‘x is mortal’ for ‘ψx’? And don’t you see that this

  proves my point?” A pupil who could follow this but not the original

    syllogism would be a psychological monstrosity. And even if there were

  such a pupil, he would still have to “see” the steps of my new argument.


It  follows  that,  when  an  argument  is  stated  as  simply  as  possible,  the

connection asserted in every step has to be a datum. But it is impossible that

the connection in every step should have the highest degree of credibility,

because even the best mathematicians sometimes make mistakes. In fact, our

perceptions of the logical connections between propositions, like our sense-

perceptions and our memories, can be ordered by their degrees of credibility:

in some, we see the logical connection so clearly that we cannot be made to

doubt it, while in others our perception of the connection is so faint that we

are not sure whether we see it or not.

  I  shall  henceforth  assume  that  a  datum,  in  the  sense  defined  at  the

beginning of this section, may be uncertain in a greater or less degree. We

can, theoretically, make a connection between this kind of uncertainty and the

kind derived from mathematical probability, if we suppose that an uncertainty

of one kind can be judged greater than, equal to, or less than, one of the other

kind. For example, when I think I hear a faint sound, but am not sure, I may

theoretically be able to say: The occurrence of this sound has the same degree

of rational credibility s the occurrence of double sixes with dice. In some

degree, such comparisons could be tested, by collecting evidence of mistakes

as to faint sensations and working out their frequency. All this is vague, and I

do  not  see  how  to  make  it  precise.  But  at  any  rate  it  suggests  that  the

uncertainty  of  data  is  quantitative,  and  can  be  equal  or  unequal  to  the

uncertainty derived from a probability inference. I shall assume this to be the

case,  while  admitting  that,  in  practice,  the  numerical  measurement  of  the

uncertainty of a datum is seldom possible. We may say that the uncertainty is

a half when the doubt is such as to leave an even balance between belief anddisbelief. But such a balance can only be established by introspection, and is

incapable of being confirmed by any sort of test.

  The admission of uncertainty in data complicates the process of estimating

the  rational  credibility  of  a  proposition.  Let  us  suppose  that  a  certain

proposition p has a degree of credibility x on its own account, as a datum; and

let  us  suppose  that  there  is  also  a  conjunction h  of  propositions,  having

intrinsic  credibility y,  from  which  it  follows,  by  an  argument  having

credibility z, that p has a degree of credibility w. What, then, is the total

credibility of p? Perhaps we might be inclined to say that it is x + yzw. But h

also is sure to have a derived as well as an intrinsic credibility, and this will

increase  the  credibility  of x.  In  fact,  the  complications  will  soon  become

unmanageable. This causes a certain approximation to the theory of Hegel

and Dewey.

  Given  a  number  of  propositions,  each  having  a  fairly  high  degree  of

intrinsic credibility, and given a system of inferences by virtue of which these

various propositions increase each other’s credibility, it may be possible in the

end to arrive at a body of interconnected propositions having, as a whole, a

very high degree of credibility. Within this body, some are only inferred, but

none are only premisses, for those which are premisses are also conclusions.

The edifice of knowledge may be compared to a bridge resting on many piers,

each of which not only supports the roadway but helps the other piers to

stand firm owing to interconnecting girders. The piers are the analogues of

the propositions having some intrinsic credibility, while the upper portions of

the bridge are the analogues of what is only inferred. But although each pier

may be strengthened by the other piers, it is the solid ground that supports the

whole, and in like manner it is intrinsic credibility that supports the whole

edifice of knowledge.D. Degrees of Subjective Certainty


Subjective certainty is a psychological concept, while credibility is at least in

part logical. The question whether there is any connection between them is a

form of the question whether we know anything. Such a question cannot be

discussed on a basis of complete scepticism; unless we are prepared to assert

something, no argument is possible.

  Let us first distinguish three kinds of certainty.


(1)  A propositional function is certain with respect to another when the class

  of terms satisfying the second is part of the class of terms satisfying the

  first. E.g. “x is an animal” is certain in relation to “x is a rational animal”.

  This meaning of certainty belongs to mathematical probability. We will

  call this kind of certainty “logical”.

(2)  A proposition is certain when it has the highest degree of credibility,

  either intrinsically or as a result of argument. Perhaps no proposition is

  certain in this sense, i.e. however certain it may be in relation to a given

  person’s  knowledge,  further  knowledge  might  increase  its  degree  of

    credibility. We will call this kind of certainty “epistemological”.

(3)  A person is certain of a proposition when he feels no doubt whatever of its

  truth.  This  is  a  purely  psychological  concept,  and  we  will  call  it

    “psychological certainty”.


Short  of  subjective  certainty,  a  man  may  be  more  or  less  convinced  of

something. We feel sure that the sun will rise to-morrow, and that Napoleon

existed; we are less sure of quantum theory and the existence of Zoroaster;

still less sure that Eddington got the number of electrons exactly right, or that

there was a king called Agamemnon at the siege of Troy. These are matters as

to which there is fairly general agreement, but there are other matters as to

which disagreement is the rule. Some people feel no doubt that Churchill is

good  and  Stalin  bad,  others  think  the  opposite;  some  people  were  utterly

certain that God was on the side of the Allies, others thought that He was onthe side of the Germans. Subjective certainty, therefore, is no guarantee of

truth, or even of a high degree of credibility.

  Error is not only the absolute error of believing what is false, but also the

quantitative error of believing more or less strongly than is warranted by the

degree of credibility properly attaching to the proposition believed in relation

to the believer’s knowledge. A man who is quite convinced that a certain

horse will win the Derby is in error even if the horse does win.

  Scientific  method,  broadly  speaking,  consists  of  techniques  and  rules

designed to make degrees of belief coincide as nearly as possible with degrees

of credibility. We cannot, however, begin to seek such a harmony unless we

can  start  from  propositions  which  are  both  epistemologically  credible  and

subjectively nearly certain. This suggests a Cartesian scrutiny, but one which,

if it is to be fruitful, must have some non-sceptical guiding principle. If there

were  no  relation  at  all  between  credibility  and  subjective certainty,  there

could be no such thing as knowledge. We assume in practice that a class of

beliefs may be regarded as true if (a) they are firmly believed by all who have

carefully considered them, (b) there is no positive argument against them, (c)

there is no known reason for supposing that mankind would believe them if

they  were  untrue.  On  this  basis,  it  is  generally  held  that  judgements  of

perception on the one hand, and logic and mathematics on the other, contain

what is most certain in our knowledge. We shall see that, if we are to arrive at

science, logic and mathematics will have to be supplemented by certain extra-

logical principles, of which induction has hitherto (I think mistakenly) been

the  one  most  generally  recognized.  These  extra-logical  principles  raise

problems which it will be our business to investigate.

  Perfect rationality consists, not in believing what is true, but in attaching

to  every  proposition  a  degree  of  belief  corresponding  to  its  degree  of

credibility.  In  regard  to  empirical  propositions,  the  degree  of  credibility

changes when fresh evidence accrues. In mathematics, the rational man who

is not a mathematician will believe what he is told; he will therefore change’

his  beliefs  when  mathematicians  discover  errors  in  the  work  of  their

predecessors. The mathematician himself may be completely rational in spite

of making a mistake, if the mistake is one which at the time is very difficult todetect.

    Whether  we  ought  to  aim  at  rationality  is  an  ethical  question.  I  shall

consider some aspects of it in the following section.E. Probability and Conduct


Bishop Butler’s statement that probability is the guide of life is very familiar.

Let us consider briefly what it can mean, how far it is true, and what is

involved in believing it to have the degree of truth that it seems to possess.

  Most ethical theories are of one of two kinds. According to the first kind,

good conduct is conduct obeying certain rules; according to the second, it is

conduct  designed  to  realize  certain  ends.  There  are  theories  which  are  of

neither of these two kinds, but for our purposes we may ignore them.

  The first type of theory is exemplified by Kant and the Decalogue. The

Decalogue, it is true, is not a pure example of this type of theory, since reasons

are  given  for  some  of  the  commandments. You  must  not  worship  graven

images, because God will be jealous; you should honour your parents, because

it diminishes your chances of death. It is of course easy to find reasons against

murder and theft, but none are given in the Ten Commandments. If reasons

are  given,  there  will  be  exceptions,  and  common  sense  has  in  general

recognized them, but none are admitted in the text.

  When ethics is considered to consist of rules of conduct, probability plays

no part in it. It is only in the second type of ethical theory, that in which

virtue consists in aiming at certain ends, that probability is relevant. So far as

the relation to probability is concerned, it makes very little difference what

end is chosen. For the sake of definiteness, let us suppose the end to be the

greatest possible excess of pleasure over pain, a pleasure and a pain being

considered equal when a person who has the choice is indifferent whether he

has both or neither. We may designate this end briefly as that of maximizing

pleasure.

  We cannot say that the virtuous man will act in the way that will in fact

maximize pleasure, since he may have no reason to expect this result. It would

have been a good thing if Hitler’s mother had killed him in infancy, but she

could not know this. We must therefore say that the virtuous man will act in

the  way  which,  so  far  as  his  knowledge  goes,  will probably  maximize

pleasure.  The  kind  of  probability  that  is  involved  is  obviously  degree  ofcredibility.

  The probabilities concerned are to be estimated by the rules for computing

“expectation”. That is to say, if there is a probability p that a certain act will

have among its consequences a pleasure of magnitude x, this contributes an

amount p x to the expectation. Since distant consequences seldom have any

appreciable probability, this justifies the practical man in usually confining his

attention to the less remote consequences of his action.

  There  is  another  consideration:  the  calculations  involved  are  often

difficult, and are most difficult when the felicific properties of two possible

actions are nearly equal, in which case the choice is unimportant. Therefore as

a rule it is not worth while to determine with any care which action will

produce the most pleasure. This is the reason in favour of rules of action, even

if our ultimate ethic rejects them: they can be right in the great majority of

cases,  and  save  us  the  trouble  and  waste  of  time involved  in  estimating

probable  effects.  But  the  rules  of  action  themselves  should  be  carefully

justified by their felicific character, and where really important decisions are

concerned it may be necessary to remember that the rules are not absolute.

Currency  reform  usually  involves  something  like  theft,  and  war  involves

killing. The statesman who has to decide whether to reform the currency or to

declare  war  has  to  go  behind  rules  and  do  his  best  to  estimate  probable

consequences. It is only in this sense that probability can be the guide of life,

and that only in certain circumstances.

  There is, however, another and humbler sense of the dictum, which was

perhaps that intended by the Bishop. This is, that we should, in practice, treat

as certain whatever has a very high degree of probability. This is merely a

matter of common sense, and raises no issue that is of interest to the theory of

probability.1 Whether Z is to be included depends upon whether “ZOO” is allowed to count as a word.                                        VII 


                  Probability and Induction



A. Statement of the Problem


THE  problem  of  induction  is  a  complex  one,  having  various  aspects  and

branches.  I  shall  begin  by  stating  the  problem  of  induction  by  simple

enumeration.


I.  The fundamental question, to which others are subsidiary, is this: Given

  that a number of instances of a class α have all been found to belong to a

  class β, does this make it probable, (a) that the next instance of α will be a

  β, or (b) that all α’s are β’s?

II.  If either of these is not true universally, are there dis coverable limitations

  on α and β which make it true?

III.  If either is true with suitable limitations, is it, when so limited, a law of

  logic or a law of nature?

IV.    Is  it  derivable  from  some  other  principle,  such  as  natural  kinds,  or

  Keynes’s limitation of variety, or the reign of law, or the uniformity of

  nature, or what not?

V.  Should the principle of induction be stated in a different form, viz: Given a

  hypothesis h which has many known true consequences and no known

  false ones, does this fact make h probable? And if not generally, does it do

  so in suitable circumstances?

VI.  What is the minimum form of the inductive postulate which will, if true,

  validate accepted scientific inferences?

VII.  Is there any reason, and if so what, to suppose this minimum postulate

  true? Or, if there is no such reason, is there nevertheless reason to act as if

  it were true?


There is need, in these discussions, to remember the ambiguity in the word“probable”  as  commonly  used.  When  I  say  that,  in  certain  circumstances,

“probably” the next α will be a β, I shall hope to be able to interpret this

according  to  the  finite  frequency  theory.  But  if  I  say  that  the  inductive

principle is “probably” true, I shall have to be using the word “probably” to

express a high degree of credibility. Confusions may easily arise through not

keeping these two meanings of the word “probable” sufficiently separate.

  The discussions upon which we shall be engaged have a history which

may be considered to begin with Hume. On a large number of subsidiary

points definite results have been obtained; sometimes these points were not

recognized, at first, to be subsidiary. But investigation has made it, by now,

fairly clear that the technical discussions which reach results throw little light

on the main problem, which remains substantially as Hume left it.B. Induction by Simple Enumeration


Induction by simple enumeration is the following principle: “Given a number

n of α’s which have been found to be β’s, and no α which has been found to

be not a β, then the two statements: (a) ‘the next α will be a β’, (b) ‘all α’s are

β’s’, both have a probability which increases as n increases, and approaches

certainty as a limit as n approaches infinity.”

  I shall call (a) “particular induction” and (b) “general induction”. Thus (a)

will argue from our knowledge of the past mortality of human beings that

probably Mr. So-and-So will die, whereas (b) will argue that probably all men

are mortal.

  Before  proceeding  to  more  difficult  or  doubtful  points,  there  are  some

rather  important  questions  which  can  be  decided  without  great  difficulty.

These are:


(1)  If induction is to serve the purposes which we expect it to serve in science,

    “probability” must be so interpreted that a probability-statement asserts a

  fact;  this  requires  that  the  kind  of  probability  involved  should  be

  derivative from truth and falsehood, not an indefinable; and this, in turn,

  makes the finite-frequency interpretation more or less inevitable.

(2)  Induction appears to be invalid as applied to the series of natural numbers.

(3)  Induction is not valid as a logical principle.

(4)  Induction requires that the instances upon which it is based should be

  given as a series, not merely as a class.

(5)  Whatever limitation may be necessary to make the principle valid must be

  stated in terms of the intensions by which the classes α and β are defined,

  not in terms of extensions.

(6)  If the number of things in the universe is finite, or if some finite class is

  alone relevant to the induction, then induction, for a sufficient n, becomes

    demonstrable; but in practice this is unimportant, because the n concerned

  would have to be larger than it ever can be in any actual investigation.  I shall now proceed to prove these propositions.


(1)  If “probability” is taken as an indefinable, we are obliged to admit that the

    improbable may happen, and that, therefore, a probability-proposition tells

  us  nothing  about  the  course  of  nature.  If  this  view  is  adopted,  the

    inductive  principle  may  be  valid,  and  yet  every  inference  made  in

    accordance with it may turn out to be false; this is improbable, but not

    impossible. Consequently a world in which induction is true is empirically

    indistinguishable from one in which it is false. It follows that there can

  never be any evidence for or against the principle, and that it cannot help

  us to infer what will happen. If the principle is to serve its purpose, we

  must interpret “probable” as meaning “what in fact usually happens”; that

  is to say, we must interpret a probability as a frequency.

(2)   Induction  in  arithmetic.—It  is  easy  in  arithmetic  to  give  examples  of

    inductions which lead to true conclusions, and others which lead to false

    conclusions. Jevons gives the two instances:


  5 15. 35. 45. 65, 95

  •  7, 17, 37, 47,67, 97

•  In the first row, every number ends in 5 and is divisible by 5; this may lead

to the conjecture that every number that ends in 5 is divisible by 5, which is

true. In the second row, every number ends in 7 and is prime; this might

lead to the conjecture that every number ending in 7 is prime, which would

be false.

•  Or take: “Every even integer is the sum of 2 primes”. This is true in every

case in which it has been tested, and the number of such cases is enormous.

  Nevertheless there remains a reasonable doubt as to whether it is always

true.

•    As  a  striking  example  of  failure  of  induction  in  arithmetic,  take  the

            1
following:

  Put π(x) = number of primes ⩽ x  It is known that, when x is large, π(x) and li(x) are nearly equal. It is also

known that, for every known prime,




  Gauss conjectured that this inequality always holds. It has been tested for

                    7
all primes up to 10 and for a good many beyond this, and no particular case

of its falsity has been discovered. Nevertheless Littlewood proved in 1912 that

there are an infinite number of primes for which it is false, and Skewes (L.M.S.

Journal, 1933) proved that it is false for some number less than








  It will be seen that Gauss’s conjecture, though it turned out to be false,

had in its favour vastly better inductive evidence than exists for even our most

firmly rooted empirical generalizations.

  Without going so deeply into the theory of numbers, it is easy to construct

false  inductions  in  arithmetic  in  any  required  number.  For  instance,  no

number less than n is divisible by n. We can make n as large as we please, and

thus  obtain  as  much  inductive  evidence  as  we  choose  in  favour  of  the

generalization: “No number is divisible by n”.

  It is obvious that any n integers must possess many common properties

which most integers do not possess. For one thing, if m is the greatest of them,they all possess the infinitely rare property of being not greater than m. There

is therefore no validity in either a general or a particular induction as applied

to integers, unless the property to which induction is to be applied is somehow

limited.  I  do  not  know  how  to  state  such  a  limitation,  and  yet  any  good

mathematician will have a feeling, analogous to common sense, as to the sort

of property that is likely to allow an induction which turns out to be valid. If

                                    222
you have noticed that 1 + 3 = 2, 1 + 3 + 5 = 3, 1 + 3 + 5 + 7 = 4, you will be

inclined to conjecture that




•    and  this  conjecture  can  easily  be  proved  correct.  Similarly  if  you  have

                  332333233332
noticed that 1 + 2 = 3, 1 + 2 + 3 = 6, 1 + 2 + 3 + 4 = 10, you may

conjecture that the sum of the first n cubes is always a square number, and

this again is easily proved. Mathematical intuition is by no means infallible

as regards such inductions, but in the case of good mathematicians it seems

to be oftener right than wrong. I do not know how to make explicit what

guides mathematical intuition in such cases. Meanwhile, we can only say

that no known limitation will make induction valid as applied to the natural

numbers.

(3)   Induction  invalid  as  a  logical  principle.—It  is  obvious  that,  if  we  are

  allowed to select our class β as we choose, we can easily make sure that

  our induction shall fail. Let a, a, … a be the hitherto observed members
                                      12n

  of α, all of which have been found to be members of β, and let a be the
                                                                                n+1

  next member of α. So far as pure logic is concerned, β might consist only

  of the terms a, a … a; or it might consist of everything in the universe
                    12n

  except a; or it might consist of any class intermediate between these
            n+1

  two. In any of these cases the induction to a would be false.
                                                      n+1

•  It is obvious (an objector may say) that β must not be what might be called

a “manufactured” class, i.e. one defined partly by extension. In the sort of

cases contemplated in inductive inference, β is always a class known in

intension, but not in extension except as regards the observed members a,
                                                                                          1

a . . a and such other members of β, not members of α, as may happen to
  2nhave been observed.

•  It is very easy to make up obviously invalid inductions. A rustic might say:

all the cattle I have ever seen were in Herefordshire, therefore probably all

cattle are in that county. Or we might argue: No man now alive has died,

therefore probably all the men now alive are immortal. The fallacies in such

inductions are fairly obvious, but they would not be fallacies if induction

were a purely logical principle.

•  It is therefore clear that, if induction is to be not demonstrably false, the

class β must have certain characteristics, or be related in some specific way

to the class α. I am not contending that with these limitations the principle

must be true; I am contending that without them it must be false.

(4)  In empirical material instances come in a time-order, and therefore are

  always  serial.  When  we  consider  whether  induction  is  applicable  in

  arithmetic, we naturally think of the numbers as arranged in order of

    magnitude. But if we are allowed to arrange them as we like, we can

  obtain strange results; for instance, as we saw, we can prove that it is

  infinitely improbable that a number chosen at haphazard will not be a

  prime.

•  It is essential to the enunciation of particular induction that there should be

a next instance, which demands a serial arrangement.

•  If there is to be any plausibility about general induction, we must be given

that the first n members of α are found to be members of β, not merely that

α  and  β  have n  members  in  common.  This  again  requires  a  serial

  arrangement.

(5)  Assuming it admitted that, if an inductive inference is to be valid, there

  must be some relation between α and β, or some characteristic of one of

  them, in virtue of which it is valid, it is clear that this relation must be

  between intensions,  e.g.  between  “human”  and  “mortal”,  or  between

    “ruminant”  and  “dividing  the  hoof”.  We  seek  to  infer  an  extensional

  relation, but we do not originally know the extensions of α and β when

  we  are  dealing  with  empirically  given  classes  of  which  new  members

  become known from time to time. Everyone would admit “dogs bark” as a

  good induction; we expect a correlation between the visual appearance of  an animal and the noise it makes. This expectation is, of course, the result

  of another, wider induction, but that is not at the moment the point that

  concerns me. What concerns me is the correlation of a kind of shape with

  a kind of noise, both intensions, and the fact that certain intensions seem

  to us more likely to be inductively related than certain others.

(6)  This point is obvious. If the universe is finite, complete enumeration is

  theoretically  possible,  and  before  it  has  been  achieved  the  ordinary

  calculus of probability shows that an induction is probably valid. But in

  practice  this  consideration  has  no  importance,  because  of  the

    disproportion  between  the  number  of  things  we  can  observe  and  the

  number of things in the universe.


Let us now revert to the general principle, remembering that we have to seek

some limitation which will make it possibly valid. Take particular induction

first. This says that, if a random selection of n members of α is found to

consist wholly of members of β, it is probable that the next member of α will

be a β, i.e. most of the remaining α’s are β’s. This itself need only be probable,

We may suppose α to be a finite class, containing (say) N members.

  Of these we know that at least n are members of β. If the total number of

members  of  α  that  are  members  of  β  is m,  the  total  number  of  ways  of





                                                          1
selecting n  terms  is ,  and  the  total  number  of





ways of selecting n terms that are α’s is  Therefore

the chance of a selection consisting wholly of α’s is  If p is the a priori likelihood of m being the number of terms common to
      m

α and β, then the likelihood after experience is







  Let us call this q.
                      m

  If  the  number  of  members  common  to  α  and  β  is m,  then  after

withdrawing n α’s that are β’s there remain m − n β’s and N − n not - β’s.

Therefore, from the hypothesis that α and β have m members in common, we




get  a  probability   of  another  β.  Therefore  the  total

probability is








  The value of this depends entirely on the p’s, which there is no valid way
                                                      m

of estimating. If we assume with Laplace that every value of m is equally

probable we get Laplace’s result, that the chance of the next α being a β is 





              . If we assume that, a priori, each α is equally likely to be a β and

not be a β, we get the value 1/2. Even with Laplace’s hypothesis the general





induction has only a probability , which is usually small.  We  need  therefore  some  hypothesis  which  makes p  large  when m is
                                                                    m

nearly N. This will have to depend upon the nature of the classes α and β if it

is to have any chance of validity.C. Mathematical Treatment of Induction


From the time of Laplace onward, various attempts have been made to show

that  the  probable  truth  of  an  inductive  inference  follows  from  the

mathematical  theory  of  probability.  It  is  now  generally  agreed  that  these

attempts were all unsuccessful, and that, if inductive arguments are to be

valid, it must be in virtue of some extra-logical characteristic of the actual

world,  as  opposed  to  the  various  logically  possible  worlds  that  may  be

contemplated by the logician.

  The  first  of  such  arguments  is  due  to  Laplace.  In  its  valid,  purely

mathematical, form, it is as follows:

  There are n + 1 bags, similar in external appearance, and each containing

n balls. In the first, all the balls are black; in the second, one is white and the

                            th
rest black; in the (r + 1), r are white and the rest black. One of these, of

which the composition is unknown, is selected, and m balls are withdrawn

from it. They prove to be all white. What is the probability (a) that the next

ball drawn will be white, (b) that we have chosen the bag consisting wholly of

white balls?

  The  answer  is:  (a)  the  chance  that  the  next  ball  will  be  white  is 





              ; (b) the chance that we have chosen the bag in which all the





balls are white is .

  This  valid  result  has  a  straightforward  interpretation  on  the  finite

frequency theory. But Laplace infers that if m A’s have been found to be B’s,





the chance that the next A will be a B is  and the chance that allA’s are B’s is . He gets this result by assuming that, given n

objects of which we know nothing, the probabilities that 0, 1, 2, … n of them

are B’s are all equal. This, of course, is an absurd assumption. If we replace it

by the slightly less absurd assumption that each of the objects has an equal

chance of being a B or not a B, the chance that the next A will be a B remains

1/2, however many A’s have been found to be B’s.

  Even  if  his  argument  were  accepted,  the  general  induction  remains

improbable if n is much greater than m, though the particular induction may

become highly probable. In fact, however, his argument is only a historical

curiosity.

  Keynes, in his Treatise on Probability, has done the best that can be done

for  induction  on  purely  mathematical  lines,  and  has  decided  that  it  is

inadequate. His result is as follows.

  Let g be a generalization, x, x, … observed instances favourable to it, and
                                  12

h the general circumstances so far as they are relevant.

  Assume




  Put




  Thus p  is  the  probability  of  the  general  induction  after n favourable
          n

instances. Write ḡ for the negation of g, and p for g/h, the a priori probability
                                                    0

of the generalization.

  Then  As n increases, this approaches 1 as a limit if







•  approaches zero as a limit; and this happens if there are finite quantities ε

and η such that, for all sufficiently great r’s,




  Let us consider these two conditions. The first says that there is a quantity

1 — ε, less than 1, such that, if the generalization is false, the probability of the

next  instance  being  favourable  will  always,  after  a  certain  number  of

favourable instances, be less than this. Consider, as an instance of its failure,

the generalization “all numbers are non-prime”. As we move up the number-

series, primes become rarer, and the chance of the next number after r non-

primes being itself a non-prime increases, and approaches certainty as a limit

if r is kept constant. This condition, therefore, may fail.

  But the second condition, that g must, antecedently to the beginning of the

induction, have a probability greater than some finite probability, is more

difficult. In general, it is hard to see any way in which this probability can be

estimated. What would be the probability of “all swans are white” for a person

who had never seen a swan or been told anything about the colour of swans?

Such questions are both obscure and vague, and Keynes recognizes that they

                                  1
make his result unsatisfactory.

  There is one simple hypothesis which would give the finite probability

that Keynes wants. Let us suppose that the number of things in the universe is

finite, say N. Let β be a class of n things, and let α be a random selection of m

things. Then the number of possible α’s is•  and the number of these that are contained in β is





    Therefore the chance of “all α’s are β’s” is







•  which is finite. That is to say, every generalization as to which we have no

evidence has a finite chance of being true.

  I fear, however, that, if N is as large as Eddington maintained, the number

of favourable instances required to make an inductive generalization probable

in any high degree would be far in excess of what is practically attainable.

This way of escape, therefore, while excellent in theory, will not serve to

justify scientific practice.

  Induction  in  the  advanced  sciences  proceeds  on  a  somewhat  different

system from that of simple enumeration. There is first a body of observed

facts, then a general theory consistent with them all, and then inferences from

the theory which subsequent observation confirms or confutes. The argument

here depends upon the principle of inverse probability. Let p be a general

theory, h  the  previously  known  data,  and q  a  new  experimental  datum

relevant to p. Then







  In the most important case, q follows from p and h, so that q/p h = 1. In

this case, therefore,  It follows that, if q/h is very small, the verification of q greatly increases

the probability of p. This, however, does not have quite the consequences one

might hope. We have, putting “” for “not-p”,




•  because, given h, p implies q. Thus if







•  we have







  This will be a high probability if y is small. Now two circumstances may

make y small: (1) p/h is large, (2) if  q/h is small, i.e. if q would be improbable

if p were false. The difficulties in the way of estimating these two factors are

much the same as those that appear in Keynes’s discussion. To obtain an

estimate of p/h, we shall need some way of evaluating the probability of p

antecedently to the special evidence that has suggested it, and it is not easy to

see  how  this  can  be  done.  The  only  thing  that  seems  clear  is  that,  if  a

suggested  law  is  to  have  an  appreciable  probability  antecedently  to  any

evidence in its favour, that must be in virtue of a principle to the effect that

some fairly simple law is bound to be true. But this is a difficult matter, to

which I shall return at a later stage.

  The  probability  of q/h,  in  certain  kinds  of  cases,  is  more  possible  toestimate approximately. Let us take the case of the discovery of Neptune. In

this case, p is the law of gravitation, h is the observations of planetary motions

before the discovery of Neptune, and q is the existence of Neptune at the place

where calculations showed that it should be. Thus  q/h is the probability that

Neptune would be where it was, given that the law of gravitation was false.

Here we must make a proviso as to the sense in which we should use the word

“false”. It would not be right to take Einstein’s theory as showing Newton’s to

be  “false”  in  the relevant  sense.  All  quantitative  scientific  theories,  when

asserted,  should  be  asserted  with  a  margin  of  error;  when  this  is  done,

Newton’s theory of gravitation remains true of planetary motions.

  The following argument looks hopeful, but is not in fact valid.

  In our case, apart from p or some other general law, h is irrelevant to q,

that is to say, observations of other planets make the existence of Neptune

neither more nor less probable than it was before. As for other laws, Bode’s

law might be held to make it more or less probable that there would be a

planet having more or less the orbit of Neptune, but would not indicate the

part of its orbit that it had reached at a given date. If we suppose that Bode’s

law, and any other relevant law except gravitation, conferred a probability x

on the hypothesis of a planet roughly in the orbit of Neptune, and suppose the

apparent position of Neptune was calculated with a margin of error θ, then

the probability of Neptune being found where it was would be θ/2π. Now θ

was very small, and it cannot be maintained that x was large. Therefore  q/h,

which was x θ/2π, was certainly very small. Suppose we take x to be 1/10 and

θ to be 6 minutes, then







    Therefore if we suppose that p/h = 1/36, we shall have y = 1/1000 and  Thus even if, before the discovery of Neptune, the law of gravitation was

as improbable as double sixes with dice, it had afterwards odds of 1000 to 1 in

its favour.

  This argument, extended to all the observed facts of planetary motions,

apparently  shows  that,  if  the  law  of  gravitation  had  even  a  very  small

probability at the time when it was first enunciated, it soon became virtually

certain. But it does nothing to help us to gauge this initial probability, and

therefore would fail, even if valid, to give us a firm basis for the theoretical

inference from observation to theory.

    Moreover the above argument is open to objection, in view of the fact that

the law of gravitation is not the only law which would lead to the expectation

of Neptune being where it was. Suppose the law of gravitation to have been

true until the time t, where t is any moment subsequent to the discovery of

Neptune; then we should still have q/p’h = 1, where p’ is the hypothesis that

the law was true only until the time t. There was therefore better reason to

expect the finding of Neptune than would result from pure chance, or from

this together with Bode’s law. What was rendered highly probable was that

the law had held until then. To infer that it would hold in future required a

principle  not  derivable  from  anything  in  the  mathematical  theory  of

probability.  This  consideration  destroys  the  whole  force  of  the  inductive

argument  for  general  theories,  unless  the  argument  is  reinforced  by  some

principle such as the uniformity of nature is supposed to be. Here again, we

find that induction needs the support of some extra-logical general principle

not based upon experience.D. Reichenbach’s Theory


The  peculiarity  of  Reichenbach’s  theory  of  probability  is  that  induction  is

involved  in  the  very  definition  of  a  probability.  His  theory  is  as  follows

(somewhat simplified).

  Given a statistical series, e.g. such as in vital statistics, and given two

overlapping classes α and β to which some members of the series belong, we

often find that, when the number of items is large, the percentage of members

of β that are members of α remains approximately constant. Suppose that,

when the number of items exceeds (say) 10,000, it is found that the proportion

of  recorded  β’s  that  are  α’s  is  never  far  from m/n,  and  that  this  rational

fraction is nearer the average observed proportion than any other. We then

“posit” that, however far the series may be prolonged, the proportion will

always remain nearly m/n. We define the probability of a β being an α as the

limit  of  the  observed  frequency  when  the  number  of  observations  is

indefinitely increased, and in virtue of our “posit” we assume that this limit

exists  and  is  in  the  neighbourhood  of m/n,  where m/n  is  the  observed

frequency in the largest obtainable sample.

    Reichenbach asserts with emphasis that no proposition is certain; all are

only  probable  in  varying  degrees,  and  every  probability  is  the  limit  of  a

frequency.  He  admits  that,  in  consequence  of  this  doctrine,  the  items  by

means  of  which  the  frequency  is  computed  are  themselves  only  probable.

Take e.g. the death rate: when a man is judged dead, he may be still alive,

therefore  every  item  in  mortality  statistics  is  doubtful.  This means,  by

definition, that the record of a death must be one of a series of records, some

correct, some erroneous. But those we take to be correct are only probably

correct, and must be members of some new series. All this he admits, but he

says that at some stage we break off the endless regress, and adopt what he

                      1
calls a “blind posit”. A “blind posit” is a decision to treat some proposition as

true although we have no good ground for doing so.

  There  are  in  this  theory  two  kinds  of  “blind  posits”,  namely:  (1)  the

ultimate  items  in  the  statistical  series  which  we  choose  to  regard  asfundamental; (2) the assumption that the frequency found in a finite number

of  observations  will  remain  approximately  constant  however  much  the

number of observations may be increased. Reichenbach considers his theory

completely empirical, because he does not assert that his “posits” are true.

  I am not now concerned with Reichenbach’s general theory, which was

considered in an earlier chapter. I am concerned now only with his theory of

induction.  The  gist  of  his  theory  of  this:  If  his  inductive  posit  is  true,

prediction is possible, and if not, not. Therefore the only way in which we can

obtain any probability in favour of one prediction rather than another is to

suppose his posit true. I am not concerned to deny that some posit is necessary

if there is to be any probability in favour of predictions, but I am concerned to

deny that the posit required is Reichenbach’s.

  His posit is this: Given any two classes α and β, and given that instances

of α present themselves in a temporal sequence, if it is found that, after a

sufficient number of α’s have been examined, the proportion that are β’s is

always  roughly m/n,  then  this  proportion  will  continue  however  many

instances of α may be subsequently observed.

  We may observe, in the first place, that this posit is only apparently more

general than one applying to the case in which all observed α’s are β’s. For in

Reichenbach’s hypothesis every segment of the series of α’s has the property

that about m/n of its members are β’s, and the more specialized posit can be

applied  to  the  segments.  We  can  therefore  confine  ourselves  to  the  more

specialized posit.

    Reichenbach’s posit is therefore equivalent to the following: When a large

number of α’s have been observed, and have all been found to be β’s, we shall

assume that very nearly all α’s are β’s. This assumption is necessary (so he

maintains) for the definition of probability, and for all scientific prediction.

  I  think  this  posit  can  be  shown  to  be  false.  Suppose a, a,  .  . a are
                                                                        12n

members of α which have been observed and have been found to belong to a

certain class β. Suppose that a is the next α to be observed. If it is a β,
                                    n+1

substitute  for  β  the  class  consisting  of  β  without a.  For  this  class  the
                                                                n+1

induction  breaks  down.  This  sort  of  argument  is  obviously  capable  of

extension. It follows that, if induction is to have any chance of validity, α andβ must be not any classes, but classes having certain properties or relations. I

do not mean that induction must be valid when there is a suitable relation

between α and β, but only that in that case it may be valid, whereas in its

general form it can be proved to be false.

  It might seem evident that α and β must not be what might be called

“manufactured” classes. I should call β without a, which occurred above, a
                                                          n+1

“manufactured” class. Broadly speaking, I mean by a “manufactured” class

one which is defined, at least in part, by mentioning that such and such a term

is, or is not, a member of it. Thus “mankind” is not a manufactured class, but

“all mankind except Socrates” is a manufactured class. If a, a, . . a are the n
                                                                    12n

+ 1 members of α first observed, then a, a, . . a have the property of not
                                                12n

being a, but we must not inductively infer that a has this property, no
      n+1n + 1

matter how great n may be. The classes α and β must be defined by intension,

not by mention of their membership. Whatever relation justifies induction

must be a relation of concepts, and since different concepts may define the

same  class,  it  may  happen  that  there  are  a  pair  of  concepts  which  are

inductively related and respectively define α and β, while other pairs, which

also define α and β, are not inductively related. E.g. it may be permissible to

infer from experience that featherless bipeds are mortal, but not that rational

beings living on earth are mortal, in spite of the fact that the two concepts

happen to define the same class.

    Mathematical  logic,  as  hitherto  developed,  aims  always  at  being  as

extensional as possible. This is perhaps a more or less accidental characteristic,

resulting from the influence of arithmetic on the thoughts and purposes of

mathematical logicians. The problem of induction, on the contrary, demands

intensional treatment. The classes α, β that occur in an inductive inference

are, it is true, given in extension so far as the observed instances a, a, . . a
                                                                                12n

are concerned, but beyond that point it is essential that, as yet, both classes

are only known in intension. E.g. α may be the class of persons in whose

blood there are certain bacilli, and β the class of persons showing certain

symptoms. It is of the essence of induction that the extensions of these two

classes are not known in advance. In practice, we consider certain inductions

worth testing, and others not, and we seem to be guided by a feeling as to thekinds of intensions that are likely to be connected.

    Reichenbach ‘s posit for induction is therefore both too general and too

extensional. Something more limited and intensional is required if it is not to

be demonstrably false.

    Something must be said about Reichenbach’s theory of different levels of

frequency, leading up to a set of probabilities which are “blind posits”. This is

bound up with his doctrine that probability should be substituted for truth in

logic.  Let  us  examine  the  theory  in  an  instance,  e.g.  the  chance  that  an

Englishman of sixty will die within a year.

  The first stage is straightforward: accepting the records as accurate, we

divide the number who have died within the last year by the total number.

But we now remember that each item in the statistics may be mistaken. To

estimate the probability of this, we must get some set of similar statistics

which  has  been  carefully  scrutinized,  and  discover  what  percentage  of

mistakes  it  contained.  Then  we  remember  that  those  who  thought  they

recognized a mistake may have been mistaken, and we set to work to get

statistics of mistakes about mistakes. At some stage in this regress we must

stop; wherever we stop, we must conventionally assign a “weight” which will

presumably be either certainty or the probability which we guess would have

resulted from carrying our regress one stage further.

  There are various objections to this procedure considered as a theory of

knowledge.

  To  begin  with,  the  late  stages  in  the  regress  are  usually  much  more

difficult and uncertain than the earlier stages; we are not likely, e.g. to reach

the same degree of accuracy in an estimate of mistakes in official statistics as

is reached in the official statistics themselves.

  Secondly, the blind posits with which we have to start are an attempt to

get the best of both worlds: they serve the same purpose as is served in my

system  by  data  which  may  be  erroneous,  but  by  calling  them  “posits”

Reichenbach seeks to avoid the responsibility of considering them “true”. I

cannot see what ground he can have for making one posit rather than another

except  that  he  thinks  it  more  likely  to  be  “true”;  and  since,  by  his  own

confession, this does not mean (when we are at the stage of blind posits) thatthere  is  some  known  frequency  which  makes  the  posit  probable,  he  is

committed  to  some  other  criterion  than  frequency  for  choosing  among

hypotheses.  What  this  may  be,  he  does  not  tell  us,  because  he  does  not

perceive its necessity.

  Thirdly, when we abandon the purely practical necessity of blind posits to

end the endless regress, and consider what, in pure theory, Reichenbach can

mean  by  a  probability,  we  find  ourselves  entangled  in  inextricable

complications. At the first level, we say the probability that an α will be a β is

m/n; at the second level, we assign to this statement a probability m/n, by
1122

making it one of some series of similar statements; at the third level, we assign

a probability m/n to the statement that there is a probability m/nfavour of
                3322

our first probability m/n; and so we go on for ever. If this endless regress
                          11

could be carried out, the ultimate probability in favour of the rightness of our

initial estimate m/n would be an infinite product
                  11







•  which may be expected to be zero. It would therefore seem that, in choosing

the estimate which is most probable at the first level, we are almost sure to

be wrong; but in general it will remain the best estimate open to us.

  The endless regress in the very definition of “probable” is intolerable. We

must, if we are to avoid it, admit that each item in our original statistics is

either true or false, and that the value m/n, obtained for our first probability,
                                              11

is either right or wrong; in fact, we must apply the dichotomy of true-or-false

as absolutely to probability judgments as to others. Reichenbach’s position is,

stated completely, as follows:


•  There is a proposition p, say “this α is a β”.
                            1

•  There is a proposition p, saying p has the probability x.
                            211

•  ” ” ” p, ” p ” ” ” x.
            322

•  ” ” ” p, ” p ” ” ” x.
            433  This series is infinite, and leads (one is to suppose) to a limit-proposition,

which  alone  we  have  a  right  to  assert.  But  I  do  not  see  how  this  limit-

proposition is to be expressed. The trouble is that, as regards all the members

of the series before it, we have no reason, on Reichenbach’s principles, to

regard them as more likely to be true than to be false; they have, in fact, no

probability that we can estimate.

  I conclude that the attempt to dispense with the concepts “true” and false”

is a failure, and that judgments of probability are not essentially different

from  other  judgments,  but  fall  equally  within  the  absolute  true-false

dichotomy.E. Conclusions


Induction,  ever  since  Hume,  has  played  so  large  a  part  in  discussions  of

scientific  method  that  it  is  important  to  be  clear  as  to  what  (if  I  am  not

mistaken) the above arguments have established.

  First: there is nothing in the mathematical theory of probability to justify

us in regarding either a particular or a general induction as probable, however

large may be the ascertained number of favourable instances.

    Second: if no limitation is placed upon the character of the intensional

definition of the classes A and B concerned in an induction, the principle of

induction can be shown to be not only doubtful but false. That is to say, given

that n members of a certain class A belong to a certain other class B, the

values of “B” for which the next member of A does not belong to B are more

numerous than the values for which the next member does belong to B, unless

n falls not far short of the total number of things in the universe.

  Third:  what  is  called  “hypothetical  induction”,  in  which  some  general

theory is regarded as probable because all its hitherto observed consequences

have been verified, does not differ in any essential respect from induction by

simple enumeration. For, if p is the theory in question, A the class of relevant

phenomena, and B the class of consequences of p, then p is equivalent to “all

A is B”, and the evidence for p is obtained by a simple enumeration.

  Fourth: if an inductive argument is ever to be valid, the inductive principle

must be stated with some hitherto undiscovered limitation. Scientific common

sense, in practice, shrinks from various kinds of induction, rightly, as I think.

But  what  guides  scientific  common  sense  has  not,  so  far,  been  explicitly

formulated.

  Fifth: scientific inferences, if they are in general valid, must be so in virtue

of some law or laws of nature, stating a synthetic property of the actual world,

or several such properties. The truth of propositions asserting such properties

cannot be made even probable by any argument from experience, since such

arguments, when they go beyond hitherto recorded experience, depend for

their validity upon the very principles in question.  It remains to inquire what those principles are, and in what sense, if any,

we can be said to know them.1 See Hardy, Ramanujan, pp. 16, 17.

1 “N!” means the product of all whole numbers from 1 to N.

1 I shall return to this subject in Part VI, Chapter II.

1 Experience and Prediction, p. 401.                                      part VI


            Postulates of Scientific Inference                                          I 


                        Kinds of Knowledge


IN  seeking  the  postulates  of  scientific  inference  there  are  two  kinds  of

problems. On the one hand there is analysis of what is generally accepted as

valid inference, with a view to discovering what principles are involved; this

inquiry is purely logical. On the other hand there is the difficulty that there is,

prima facie,  little  reason  to  suppose  these  principles  true,  and  still  less  to

suppose them known to be true. I think that the question in what sense, if any,

these  principles  can  be  known,  requires  an  analysis  of  the  concept  of

“knowledge”. This concept is too often treated as though its meaning were

obvious and unitary. My own belief is that many philosophical difficulties and

controversies  arise  from  insufficient  realization  of  the  difference  between

different  kinds  of  knowledge,  and  of  the  vagueness  and  uncertainty  that

characterizes most of what we believe ourselves to know. There is another

thing which it is important to remember whenever mental concepts are being

discussed, and that is our evolutionary continuity with the lower animals.

“Knowledge”, in particular, must not be defined in a manner which assumes

an  impassable  gulf  between  ourselves  and  our  ancestors  who  had  not  the

advantage of language.

  What  passes  for  knowledge  is  of  two  kinds:  first,  knowledge  of  facts;

second,  knowledge  of  the  general  connections  between  facts.  Very  closely

connected with this distinction is another: there is knowledge which may be

described as “mirroring”, and knowledge which consists in capacity to handle.

Leibniz’s monads “mirror” the universe, and in this sense “know” it; but since

monads never interact, they cannot “handle” anything external to themselves.

This  is  the  logical  extreme  of  one  conception  of  “knowledge”.  The  logical

extreme of the other conception is pragmatism, which was first promulgated

by Marx in his Theses on Feuerbach (1845): “The question whether objective

truth belongs to human thinking is not a question of theory, but a practical

question.  The  truth,  i.e.  the  reality  and  power,  of  thought  must  bedemonstrated in practice. … Philosophers have only interpreted the world in

various ways, but the real task is to alter it.”

  Both these conceptions, that of Leibniz and that of Marx, are, I suggest,

incomplete.  Speaking  very  roughly  and  approximately,  the  former  is

applicable  to  knowledge  of  facts,  the  latter  to  knowledge  of  general

connections  between  facts.  I  am  speaking  in  each  case  of  non-inferential

knowledge. Our inquiries in connection with probability have shown us that

there  must  be  non-inferential  knowledge,  not  only  of  facts,  but  also  of

connections between facts.

  Our knowledge of facts, in so far as it is not inferential, has two sources,

sensation and memory. Of these, sensation is the more fundamental, since we

can  only  remember  what  has  been  a  sensible  experience.  But  although

sensation  is  a source  of  knowledge,  it  is  not  itself,  in  any  usual  sense,

knowledge. When we speak of “knowledge”, we generally imply a distinction

between the knowing and what is known, but in sensation there is no such

distinction. “Perception”, as the word is used by most psychologists, is of the

nature of knowledge, but it is so because of the adjuncts which are added to

pure  sensation  by  experience,  or,  possibly,  by  congenital  dispositions.  But

these  adjuncts  can  only  count  as  “knowledge”  if  there  are  connections

between the sensation and other facts outside my momentary mental state,

and these connections must be suitably related to the connection between the

pure  sensation  and  the  rest  of  the  mental  state  called  a  perceiving.  The

passage from sensation to perception, therefore, involves connections between

facts, not only facts. It involves these, however, only if perception is to be

regarded as a form of knowledge; as a psychological occurrence, perception is

a mere fact, but one which might not be veridical as regards what it adds to

sensation. It is only veridical if there are certain connections among facts, e.g.

between the visual appearance of iron and hardness.

  Memory is the purest example of mirror-knowledge. When I remember a

piece of music or a friend’s face, my state of mind resembles, though with a

difference, what it was when I heard the music or saw the face. If I have

sufficient skill, I can play the music or paint the face from memory, and then

compare my playing or painting with the original, or rather with somethingwhich I have reason to believe closely similar to the original. But we trust our

memory, up to a point, even if it does not pass this test. If our friend appears

with a black eye, we say “how did you get that injury?” not “I had forgotten

that you had a black eye”. The tests of memory, as we have already had

occasion to notice, are only confirmations; a considerable degree of credibility

attaches to a memory on its own account, particularly if it is vivid and recent.

  A memory is accurate, not in proportion to the help it gives in handling

present and future facts, but in proportion to its resemblance to a past fact.

When Herbert Spencer, after fifty years, saw again the lady he had loved as a

young man, whom he had imagined still young, it was the very accuracy of

his memory which incapacitated him from handling the present fact. In regard

to memory, the definition of “truth”, and therefore of “knowledge”, lies in the

resemblance of present imagining to past sensible experience. Capacity for

handling  present  and  future  facts  may  be  confirmatory  in  certain

circumstances,  but  can  never define  what  we  mean  when  we  say  that  a

certain memory is “knowledge”.

    Sensation, perception, and memory are essentially preverbal experiences;

we may suppose that they are not so very different in animals from what they

are in ourselves. When we come to knowledge expressed in words, we seem

inevitably to lose something of the particularity of the experience that we seek

to describe, since all words classify. But here there is an important point that

needs to be emphasized: although, in a sense, words classify, the person who

uses them need not be doing so. A child learns to respond to stimuli of a

certain kind by the word “cat”; this is a causal law, analogous to the fact that a

match responds to a certain kind of stimulus by lighting. But the match is not

classifying the stimulus as “ignitory”, and the child need not be classifying the

stimulus when he makes the response “cat”. In fact, we get into an endless

regress if we do not realize that the use of such a word as “cat” does not

presuppose classification. No one can utter twice a given instance of the word

“cat”; the classifying of the various instances as instances of the word is a

process  exactly  analogous  to  that  of  classifying  animals  as  instances  of  a

species.  In  fact,  therefore,  classification  is  later  than  the  beginnings  of

language.  All  that  is  involved  in  the  original  activity  that  looks  likeclassification is a closer similarity in responses to certain stimuli than in the

stimuli. Two instances of the word “animal” are more similar than a mouse

and a hippopotamus. That is why language helps when we want to consider

what all animals have in common.

  When I have a memory-picture of some event, what is meant by calling it

“true”  is  in  no  degree  conventional.  It  is  “true”  in  so  far  as  it  has  the

resemblance which an image has to its prototype. And if the image is felt as a

memory, not as mere imagination, it is “knowledge” in the same degree in

which it is “true”.

  But as soon as words are involved, a conventional element enters in. A

child, seeing a mole, may say “mouse”; this is an error in convention, like

being  rude  to  an  aunt.  But  if  a  person  who  is  thoroughly  master  of  the

language sees a mole for a moment out of the corner of his eye and says

“mouse”, his error is not conventional, and if he has further opportunities of

observation he will say “no, I see it was a mole”. Before any verbal statement

can  be  considered  to  embody  knowledge  or  error,  definitions,  nominal  or

ostensive,  of  all  the  words  involved  must  be  furnished.  All  ostensive

definitions, and therefore all definitions, are somewhat vague. Chimpanzees

are  certainly  apes,  but  in  the  course  of  evolution  there  must  have  been

animals  which  were  intermediate  between  apes  and  men.  Every  empirical

concept is certainly applicable to some objects, and certainly inapplicable to

others, but in between there is a region of doubtful objects. In regard to such

objects classificatory statements may be more or less true, or may be so near

the middle of the doubtful region that it is futile to consider them either true

or false.

  Scientific  technique  is  largely  concerned  to  diminish  this  area  of

uncertainty.  Measurements  are  conducted  to  so  and  so  many  significant

figures, and the probable error is given. Sometimes “natural kinds” make error

practically  impossible.  In  the  existing  world,  there  is  probably  no  animal

which is not quite indubitably a mouse or quite indubitably not a mouse; the

doubtful cases that must have arisen in the course of evolution no longer exist.

In physics, atoms are of a finite number of discrete kinds; “uranium 235” is a

concept  which  is  always  unambiguously  applicable  or  unambiguously  notapplicable  to  a  given  atom.  Speaking  generally,  the  uncertainty  due  to

vagueness is limited and manageable, and exists in only a small proportion of

the statements that we wish to make—at any rate where scientific technique is

available.

  Ignoring vagueness, what is involved when we make such a statement as

“there’s a mouse”? A visual sensation causes us to believe that there is, in the

direction in which we are looking, an animal with a past and a future, and

with whatever characteristics (over and above visual appearance) make up for

us the definition of the word “mouse”. If we are justified in this very complex

belief, there must be in the outer world connections between facts similar to

the connections between the visual sensation and the beliefs that it causes. If

there are not these connections—if, e.g. the mouse is not “real” but in a film—

our beliefs are erroneous. In this way connections between facts are relevant

in  judging  the  truth  or  falsehood  of  what  might  pass  as  judgments  of

perception.

  A part—but not, I think, the whole—of what is asserted when I say “there’s

a mouse”, consists of expectations and hypothetical expectations. We think

that, if we continue to watch, we shall either continue to see the mouse, or see

it hide in some hole or crack; we should be astonished if it suddenly vanished

in the middle of the floor, though in a cinema it could easily be arranged that

this should happen. We think that, if we touched it, it would feel like a mouse.

We think that, if it moves, it will move like a mouse and not like a frog. If we

happen to be anatomists, we may think that if we dissected it we should find

the organs of a mouse. But when I say that we “think” all these things, that is

altogether  too  definite.  We  shall  think  them  if  we  are  asked;  we  shall  be

surprised  if  anything  contrary  to  them  occurs;  but  as  a  rule  what  can  be

developed into these thoughts is something rather vague and unformulated. I

think we may say that an object perceived normally arouses two sorts of

response, on the one hand certain more or less subconscious expectations, and

on the other hand certain impulses to action, though the action may consist

only  of  continued  observation.  There  is  a  certain  degree  of  connection

between  these  two  kinds  of  response.  For  instance,  continued  observation

involves expectation that the object will continue to exist; we have no suchresponse to a flash of lightning.

  Often expectation is much more definite than it is in such cases as we

have been considering. You see a door shutting in the wind, and expect to

hear it bang. You see an acquaintance approaching, and expect him to shake

hands. You see the sun setting, and expect it to disappear below the horizon. A

very large part of daily life is made up of expectations; if we found ourselves

in an environment so strange that we did not know what to expect, we should

be violently terrified. (See photographs of herds of elephants stampeding at

the first sight of an aeroplane.) The desire to know what to expect is a large

part of the love of home, and also of the impulse to scientific investigation.

Men of science, when compelled to travel, have invented the homogeneity of

space because they feel uncomfortable in the thought that “there’s no place

like home”.

    Expectations, when reflected upon, involve belief in causal laws. But in

their primitive form they seem to involve no such belief, though they are only

true in the degree in which the relevant causal laws are true. There are three

stages in the development of expectation. In the most primitive stage, the

presence  of  A  causes  expectation  of  B,  but  without  any  awareness  of  the

connection; in the second stage, we believe “A is present, therefore B will be”;

in the third, we rise to the general hypothetical “if A is present, B will be”. The

passage from the second stage to the third is by no means easy; uneducated

people find great difficulty in a hypothetical of which the hypothesis is not

known to be true.

    Although these three states of mind are different, the condition for the

truth of the belief involved is, in general, the same, namely, the existence of a

causal connection between A and B. Of course, in the first form, where the

presence of A causes the expectation of B, B may happen to occur by chance,

and the expectation will then be verified; but this will not happen usually

unless there is some degree of connection between A and B. In the second

form, where we say “A, therefore B”, the word “therefore” needs interpreting,

but as usually understood it would not be held to be justified if the connection

of A and B was fortuitous and for this occasion only. In the third form, the

causal law is explicitly asserted.  The  question  arises:  in  what  circumstances  can  such  beliefs  count  as

“knowledge”? This question is involved in any attempt to answer the question:

“In what sense do we know the necessary postulates of scientific inference?”

    Knowledge,  I  maintain,  is  a  matter  of  degree.  We  may  not  know

“Certainly A is always followed by B”, but we may know “Probably A is

usually followed by B”, where “probably” is to be taken in the sense of “degree

of credibility”. It is in this milder form that I shall inquire in what sense, and

to what degree, our expectations can count as “knowledge”.

  We  must  first  consider  what  we  are  to  mean  by  “expectation”,

remembering that we are concerned with something that may exist among

dumb animals, and that does not presuppose language. Expecting is a form of

believing, and much of what is to be said about it applies to believing in

general, but in the present context it is expecting alone that concerns us. The

state of expecting, in its more emphatic forms, is one with which we are all

familiar. Before a race, you wait expectantly for the pistol shot which is the

signal for starting. In a quarry in which blasting operations take place, when

an explosion is due you acquire a certain tenseness while waiting for it. When

you go to meet a friend at a crowded station, you scan faces with the expected

face  in  your  imagination.  These  various  states  are  partly  mental,  partly

physical; there is adjustment of muscles and sense-organs, and usually also

something imagined (which may be only words). At a certain moment, either

something happens which gives you the feeling “quite so”, or you have the

feeling “how surprising”. In the former case your expectation was “true”, in

the latter “false”.

  Various different physical and mental states may all be expectations of the

same event. There may be varying amounts of imagery, varying degrees of

muscular adjustment, varying intensities of adaptation of sense-organs. When

what is expected is not immediate or not interesting, expectation may consist

merely in belief in a certain sentence in the future tense, as, e.g. “there will be

an  eclipse  of  the  moon  to-morrow  night”.  “An  expectation  of  B”  may  be

defined as any state of mind and body such that, if B occurs at the appropriate

time, we have the feeling “quite so”, and if B does not occur we have the

feeling “how surprising”. I do not think there is any other way of definingwhat is in common among all the states that are expectations of a given event.

  We have already defined what makes an expectation “true”; it is “true”

when it is followed by the “quite-so” feeling. We have now to inquire what

makes an expectation “knowledge”. Since every case of knowledge is a case of

true belief, but not vice versa, we have to inquire what must be added to truth

to make a true expectation count as “knowledge”.

  It is easy to think of cases of true expectation which is not knowledge.

Suppose you are impressed by a sage with a long white beard, splendid robes,

and a store of oriental wisdom. Suppose he says (and you believe him) that he

has the power of foretelling the future. And suppose you toss a coin, he says it

will  come  heads,  and  it-  does  come  heads  You  will  have  had  a  true

expectation, but not knowledge, unless his pretensions were justified. Or, to

take a simpler example, suppose you are expecting Mr. X to ring you up on

the telephone. The telephone bell rings, but it is not Mr. X. In this case your

expectation that the bell would ring, though true, was not knowledge. Or

suppose that, being of a sceptical and contrary turn of mind, you expect rain

because the weather forecast says it will be fine, and it then rains, it would be

an insult to the meteorologists to call your expectation “knowledge”.

  It is clear that an expectation is not knowledge if it is the result of an

argument  which  has  false  premisses.  If  I  think  that  A  is  almost  always

followed by B, and therefore, having seen A, I expect B; if, in fact, A is very

seldom followed by B, but this happens to be one of the rare cases where it is

so followed, then my true expectation of B cannot count as knowledge.

  But these are not the really difficult cases. The expectations of animals,

and  of  men  except  in  rare  scientific  moments,  are  caused  by  experiences

which a logician might take as premisses for an induction. My dog, when I

take out her lead, becomes excited in expectation of a walk. She behaves as if

she reasoned: “Taking out the lead (A) has invariably, in my experience, been

followed  by  a  walk  (B);  therefore  probably  it  will  be  so  followed  on  this

occasion”. The dog, of course, goes through no such process of reasoning. But

the dog is so constituted that, if A has been frequently followed by B in her

experience,  and  B  is  emotionally  interesting,  A  causes  her  to  expect  B.

Sometimes the dog is right in this expectation, sometimes wrong. Supposethat, in fact, A is always, or nearly always, followed by B; can we say, in that

case, that the dog is right to expect B?

  We may carry our question a stage further. Suppose that, although A is in

fact always followed by B, this generalization only happens to be right, and

most logically similar generalizations are wrong. In that case we must regard

it  as  a  stroke  of  luck  for  the  dog  that  she  has  hit  on  a  case  in  which  a

fallacious process, by chance, leads to a true result. I do not think that in such

a case the dog’s expectation can be regarded as “knowledge”.

  But now let us suppose, not only that A is, in fact, almost always followed

by B, but further that the experienced cases of A being followed by B belong

to a definable class of cases in which generalization is nearly always in fact

true.  Shall  we,  now,  admit  the  dog’s  expectation  as  “knowledge”?  I  am

assuming that, although generalizations of the kind considered are in fact

almost always true, we know of no reason why they should be. My own view

is  that,  in  such  a  case,  the  dog’s  expectation  should  be  admitted  as

“knowledge”. And, if so, scientific inductions also are “knowledge”, provided

the world has certain characteristics. I leave on one side, for the moment, the

question  whether,  and  in  what  sense,  we  know  that  it  has  these

characteristics.

  We have been, throughout this book, assuming the substantial truth of

science, and asking ourselves what are the processes by which we come to

know  science.  We  are  therefore  justified  in  assuming  that  animals  have

become adapted to their environment more or less as biologists say they have.

Now animals have, on the one hand, certain congenital propensities, and, on

the other hand, an aptitude for the acquisition of habits. Both of these, in a

species which succeeds in surviving, must have a certain conformity with the

facts of the environment. The animal must eat the right food, mate with a

member of its own species, and (among the higher animals) learn to avoid

dangers. The habits which it acquires would not be useful unless there were

certain  causal  uniformities  in  the  world.  These  uniformities  need  not  be

absolute: you can poison rats by mixing arsenic with what seems to them

attractive  food.  But  unless  the  food  that  attracts  them  were  usually

wholesome, rats would die out. All the higher animals quickly acquire thehabit of looking for food in places where they have found food before; this

habit is useful, but only on the assumption of certain uniformities. Thus the

survival of animals rests upon their tendency to act in certain ways which

owe their advantageous quality to the fact that generalization is more often

justified than pure logic would lead us to suppose.

  But what, the reader may impatiently ask, have the habits of animals to do

with  knowledge?  According  to  the  traditional  conception  of  “knowledge”,

nothing; according to the conception that I wish to advocate, everything. In

the traditional conception, knowledge, at its best, is an intimate and almost

mystical contact between subject and object, of which some may hereafter

have complete  experience  in  the  beatific  vision.  Something  of  this  direct

contact,  we  are  told  to  suppose,  exists  in  perception.  As  for  connections

between  facts,  the  older  rationalists  assimilated  natural  laws  to  logical

principles, either directly or by a détour through God’s goodness and wisdom.

All this is out of date, except as regards perception, which many still regard as

giving direct knowledge, and not as the complex and inaccurate mixture of

sensation, habit, and physical causation that I have been arguing it to be.

Believing in general, we have seen, has only a rather roundabout relation to

what is said to be believed: when I believe, without words, that there is about

to be an explosion, it is impossible to say at all accurately what it is that is

occurring  in  me.  Believing,  in  fact,  has  a  complex  and  somewhat  vague

relation to what is believed, just as perceiving has to what is perceived.

  But  what  we  have  now  to  consider  is  not  belief  or  knowledge  as  to

particular facts, but as to relations among facts, such as are involved when we

believe “if A, then B”.

  The connections with which I am concerned are such as have a certain

generality. Within a complete complex of compresence I can perceive parts

having  spatial  and  temporal  relations;  such  relations  are  among  particular

perceptual data, and are not what I wish to consider. The relations that I wish

to consider are general, like the connection of the dog’s lead with a walk. But

when I say that they are “general”, I do not mean necessarily that they have

no exceptions; I mean only that they are true in such a large majority of

instances that in each particular case there is a high degree of credibility inthe absence of evidence to the contrary as regards that particular case. Such

are the generalizations upon which we base our conduct in daily life, e.g.

“bread nourishes”, “dogs bark”, “rattlesnakes are dangerous”. It is clear that

such beliefs, in the form in which they appear in books on logic, have an

ancestry  which  must,  if  traced  far  enough,  take  us  back  to  the  habits  of

animals. It is this ancestry that I wish to trace.

  The  purely  logical  analysis  of  “dogs  bark”  soon  reaches  complexities

which make it incredible that ordinary folk can seem to understand anything

so remote, mysterious, and universal. The first stage, for the logician, is to

substitute: “Whatever x may be, either x is not a dog or x barks”. But since

dogs only bark sometimes, you have to substitute for “x barks” the statement

“there is a time t at which x barks”. Then you must substitute one or other of

the two alternative definitions of “t” given in Part IV. In the end you will

arrive at a statement of enormous length, not only about dogs, but about

everything in the universe, and so complicated that it cannot be understood

except by a person with a considerable training in mathematical logic. But

suppose you have to explain your statement “dogs bark” to such a person, but

as he is a foreigner with only a mathematician’s knowledge of English, he

does not know the word “dog” or the word “bark”. What will you do? You

will certainly not go through the above logical rigmarole. You will point to

your dog and say “dog”; you will then excite him till he barks, and say “bark”.

The foreigner will then understand you, although, as a logician, he has no

business  to  do  so.  This  makes  it  clear  that  the  psychology  of  general

propositions is something very different from their logic. The psychology is

what does take place when we believe them; the logic is perhaps what ought

to take place if we were logical saints.

  We all believe that all men are mortal. What happens at a moment when

we  are  actively  believing  this?  Perhaps  only  a  belief  that  the  words  are

correct,  without  any  thought  as  to  what  they  signify.  But  if  we  try  to

penetrate to what the words signify, what do we do? We certainly do not see

spread out before the mind’s eye a vast series of death-beds, one for each man.

What we shall really think, if we take the trouble, will probably be something

like this: “there’s old So-and-So; he’s 99, and as vigorous as ever, but I supposehe’ll die some day. And young Such-and-Such: in spite of his athletic prowess

and his boundless vitality, he won’t last for ever. And then there was Xerxes’s

army, the thought of whose mortality caused him to weep; they are all dead.

And I myself, though I find it hard to imagine the world without me, I shall

die, but not just yet, I hope. And so on, with whoever you like to mention”.

Without all this irrelevant detail, it is difficult to grasp a general proposition,

except as a form of words of which the interpretation remains vague. In fact,

in the above long elucidation, the general proposition never emerges except

obscurely in the words “and so on”.

  I suggest that what really constitutes belief in a general proposition is a

mental habit: when you think of a particular man, you think “yes, mortal”,

provided  the  question  of  mortality  arises. That  is  the  real  point  of  the

apparently irrelevant detail: it makes you aware of what it is to believe “all

men are mortal”.

  If this is granted, we can allow a preverbal form of a general belief. If an

animal has a habit such that, in the presence of an instance of A, it behaves in

a  manner  in  which,  before  the  acquisition  of  the  habit,  it  behaved  in  the

presence of an instance of B, then I shall say that the animal believes the

general  proposition:  “Every  instance  (or  nearly  every  instance)  of  A  is

followed (or accompanied) by an instance of B”. That is to say, the animal

believes what this form of words signifies.

  If this is granted, it becomes obvious that animal habit is essential to the

understanding of the psychology and biological origin of general beliefs.

  Further, since appropriate habits are what is required for manipulation,

the above theory can be brought into relation with the pragmatist theory of

“truth”, though only as regards general laws, not as regards knowledge of

particular  facts.  There  are  here,  however,  various  complications  and

limitations which it is not necessary to our present purpose to examine.

  Returning  to  the  definition  of  “knowledge”,  I  shall  say  that  an  animal

“knows” the general proposition “A is usually followed by B” if the following

conditions are fulfilled:


(1)  The animal has had repeated experience of A being followed by B.(2)  This experience has caused the animal to behave in the presence of A

  more or less as it previously behaved in the presence of B.

(3)  A is in fact usually followed by B.

(4)  A and B are of such a character, or are so related, that, in most cases

  where  this  character  or  relation  exists,  the  frequency  of  the  observed

  sequences is evidence of the probability of a general if not invariable law

  of sequence.


  It is evident that the fourth condition raises difficult problems. These will

be dealt with in subsequent Chapters.                                          II 


                      The Role of Induction


THE form of inference called “induction by simple enumeration” (which I shall

call simply “induction”) has occupied, from Francis Bacon to Reichenbach, a

very peculiar position in most accounts of scientific inference: it has been

considered to be, like the hangman, necessary but unpleasant, and not to be

talked of if the subject could possibly be avoided—except by those who, like

Hume, refuse to be limited by the canons of good taste. For my part, I hold

that the work of Keynes, considered in an earlier Chapter (Part V, Chapter

VIII), suggests a change of emphasis, making induction no longer a premiss,

but  an  application  of  mathematical  probability  to  premisses  arrived  at

independently of induction. Nevertheless, inductive evidence is essential to

the justification of accepted generalizations, both those of science and those of

daily life. I wish to make clear, in this Chapter, both how induction is useful

and why it is not a premiss.

  We have seen, in earlier Chapters, how, when we begin to reflect, we find

ourselves already believing innumerable generalizations, such as “dogs bark”

or  “fire  burns”,  which  have  been caused  by  past  experience  through  the

mechanism of the conditioned reflex and habit-formation. When we come to

think about our beliefs, if we have a bent towards logic, we wonder whether

the cause of our belief can be accepted as a ground for it, and thus, since

repetition  is  the  cause,  we  are  led  to  a  desire  to  justify  induction.  It  has

emerged, however, from our earlier inquiries, that we have to find a way of

justifying  some  inductions  but  not  others.  To  justify  induction  as  such  is

impossible, since it can be shown to lead quite as often to falsehood as to

truth.  Nevertheless  it  remains  important  as  a  means  of  increasing  the

probability of generalizations in suitable cases. We have a feeling as to what

are  suitable  cases,  which,  though  extremely  fallible,  suffices  to  rule  out  a

number of fallacious kinds of induction which logicians can invent but which

no  sane  person  would  accept.  Our  purpose  must  be  to  substitute  for  thisfeeling something which, while not running counter to it, shall be at once

more explicit and more reliable.

  It  is  obvious  that  a  conditioned  reflex,  or  “animal  induction”, is  not

generated whenever A and B frequently occur together or in quick succession.

A and B must be the sort of thing that the animal is inclined to notice. If B is

emotionally interesting, a much smaller number of repetitions is required than

if it is not. The inductions of animals and savages in regard to matters that

vitally  concern  their  welfare  are  extraordinarily  rash;  proneness  to

generalization is much diminished by education. But as against this must be

set the fact that a scientific training causes things to be noticed which an

animal would never observe. An animal notices when and where it finds food,

and  is  stimulated  by  the  smell  of  food,  but  does  not  notice  the  chemical

ingredients of soil or the effect of fertilizers. An animal also is incapable of

hypothesis; it cannot say: “I have noticed several occasions when A has been

followed by B; perhaps this is always the case, and in any event it is worth

while to look out for other instances”. But although the man of science, when

he is on the look-out for an induction, notices many things that an animal

would not notice, he is still limited, as regards the A and B of his induction, to

certain kinds of things which seem to him plausible. How far this involuntary

and hardly conscious limitation is in line with the limitations that must be

imposed on induction to make it valid, is a difficult and obscure question, as

to which I do not venture an opinion.

  As regards the scientific use of induction, I accept the results reached by

Keynes,  which  were  explained  in  an  earlier  Chapter.  It  will  perhaps  be

advantageous, at this stage, to re-state these results.

  Keynes supposes some generalization, such as “all A is B”, for which, in

advance of any observed instances, there is a  probability p.  He  supposes,
                                                                        0

further, that a number of favourable instances x, x, … x are observed, and
                                                        12n

that  no  unfavourable  instances  are  observed.  The  probability  of  the

generalization is to become p after the first favourable instance, p after the
                                  12

first two, and so on, so that p is its probability after n favourable instances.
                                  n

We want to know in what circumstances p tends to 1 as its limit when n is
                                                  n

indefinitely increased. For this purpose we must consider the probability thatwe should have observed the n favourable instances and no unfavourable ones

if the generalization were false. Suppose we call this probability q. Keynes
                                                                                n

shows that p tends to 1 as a limit when n increases, if the ratio of q to p
              nn0

tends to zero as n increases. This requires that p should be finite, and that q
                                                        0n

should tend to zero as n increases. Induction alone cannot tell us when, if

ever, these conditions are fulfilled.

  Consider  the  condition  that p  should  be  finite.  This  means  that  the
                                        0

suggested generalization “all A is B”, before we have observed any instances

either favourable or unfavourable, has something to be said in its favour, so

that it is at any rate a hypothesis worth examining. The probability p, in
                                                                                      0

Keynes’s treatment, is relative to the general data h, which, apparently, may

include anything except instances of A’s that are, or are not, B’s. It is very

difficult to prevent oneself from thinking of the data as consisting, at least in

part, of analogous generalizations which are regarded as well established, and

from  which  we  derive  inductive  evidence  in  favour  of  “all  A  is  B”.  For

instance,  you  want  to  prove  that  all  copper  conducts  electricity.  Before

experimenting with copper you try a number of other elements, and find that

each element has a characteristic behaviour in regard to the conduction of

electricity. You therefore conclude, inductively, that either all copper conducts

electricity,  or  no  copper  does  so;  your  generalization,  therefore,  has  an

appreciable  probability  before  your  observations  begin.  But  since  this

argument uses induction, it is useless for our purpose. Before we make the

induction  that  all  elements  have  a  characteristic  behaviour  as  regards  the

conduction  of  electricity,  we  must  ask  what  was  the  probability  of this

induction  before  we  had  any  instances  of  its  truth  or  falsehood.  We  may

subsume this induction, in turn, under a wider one; we may say: “a large

number of properties have been tested, and in regard to each of these every

element has been found to have a characteristic behaviour, therefore probably

conduction  of  electricity  is  also  such  a  property”.  But  to  this  process  of

subsuming inductions under wider ones there must in practice be a limit, and

wherever we have to stop, in any given state of our knowledge, the data

which are summed up in Keynes’s h must not be such as to be only relevant if

induction is assumed.  We have therefore to seek for principles, other than induction, such that,

given certain data not of the form “this A is a B”, the generalization “all A is

B” has a finite probability. Given such principles, and given a generalization

to  which  they  apply,  induction  can  make  the  generalization  increasingly

probable, with a probability which approaches certainty as a limit when the

number  of  favourable  instances  in  indefinitely  increased.  In  such an

argument, the principles in question are premisses, but induction is not, for in

the  form  in  which  it  is  used  it  is  an  analytic  consequence  of  the  finite-

frequency theory of probability.

  Our  problem,  therefore,  is  to  find  principles  which  will  make  suitable

generalizations probable in advance of evidence.

  It remains to consider Keynes’s other condition, namely that q should
                                                                                n

tend  to  zero  as n increases. Here q  is  the  probability  that  all  the  first n
                                          n

instances will be favourable although the generalization is false. Suppose—to

repeat  an  earlier  illustration—that  you  are  a  census  officer,  concerned  to

ascertain the names of the inhabitants of a certain Welsh village. The first n

inhabitants  whom  you  question  are  all  called  Williams.  Then q  is  the
                                                                                  n

likelihood of this happening if the inhabitants are not all called Williams. In

this case, when n becomes equal to the number of inhabitants of the village,

there is no longer any one left who might be not called Williams, and q is
                                                                                        n

therefore zero. But such complete enumeration is usually impossible. As a

rule, A will be a class of events which keep on happening and cannot be

observed until they do, so that A cannot be completely enumerated until the

end of time. Nor can we guess how many members A has, nor even whether it

is a class with a finite number of members. It is such cases that we have to

think of in connection with Keynes’s condition that q must tend to zero as n
                                                              n

increases.

  Keynes brings this condition into another form, by making q the product
                                                                            n

of n different probabilities. Suppose Q is the probability that the first A will
                                            1

be a B if the generalization is false, Q2 the probability that the second A will

be a B if the generalization is false and the first A is a B, Q3 the probability

that the third A will be a B if the generalization is false and the first two A’s

are B’s, and so on. Then q is the product of Q1, Q2, Q3, Q, where Qn is the
                              nn                        th
probability that the n A will be a B given that the generalization is false and

the first n — 1 A’s are all B’s. If there is any number less than 1 such that all
                                                                                          th
the Q’s are less than this number, then the product of n Q’s is less than the n

power of this number, and therefore tends to zero as n increases. Thus our

condition is satisfied if there is some probability short of certainty, say P, such

that, given that the generalization is false and that n — 1 A’s have been found
                                    th
to be B’s, the chance that the n A will be found to be a B is always less than

P provided n is sufficiently great.

  It is difficult to see how this condition can fail in empirical material. If it

fails, then, if ε is any fraction, however small, and n is any number, however

large, and if the first n A’s are all B’s, but not all A’s are B’s, there is a number

                                                th
m such that the likelihood of the (n + m) A not being a B is less than ε. We

may put this in another way. Whatever n may be, let it be given that the first

n A’s, but not all A’s, are B’s. If we now arrange the later A’s, not in order of

their occurrence, but in the order of the probability of their being B’s, then the

limit  of  these  probabilities  is  certainty.  This  is  what  must  happen  if  the

condition fails.

    Obviously this condition is less interesting, and much easier to fulfil, than

the earlier condition, that our generalization must have a finite probability in

advance of favourable instances. If we can find a principle insuring such finite

probability in the case of a given generalization, then we have a right to use

induction to make the generalization probable. But in the absence of some

such  principle  inductions  cannot  be  accepted  as  making  generalizations

probable.

  In  the  above  discussion  I  have  followed  Keynes  in  considering  only

evidence for “all A is B”. But in practice, especially in the early stages of an

investigation,  it  is  often  useful  to  know  that  most  A  is  B.  Suppose,  for

instance, there are two diseases, one common and one rare, which have very

similar symptoms in their early stages. A medical man, when he comes across

these symptoms, will do right to conclude that he probably has to deal with a

case of the commoner disease. It very frequently happens that laws which we

believe  to  be  without  exceptions  are  discovered  by  way  of  prior

generalizations which apply to most cases, but not to all. And obviously lessevidence  is  required  to  establish  the  probability  of  “most  A  is  B”  than  to

establish that of “all A is B”.

  From a practical point of view, the difference is negligible. If it is certain

that m/n of the A’s are B’s, m/n is the probability that the next A will be a B.

If it is probable, but not certain, that all A’s are B’s, it is again probable that

the next A will be a B. So far as expectations about the next A are concerned,

it therefore comes to the same thing to be sure that most A’s are B’s, or to

think it probable that all A’s are B’s. The case most likely to occur in practice

is that in which it is probable that most A’s are B’s. This often suffices for

rational expectation, and therefore for guidance in practice.                                          III 


      The Postulate of Natural Kinds, or of


                            Limited Variety


IN seeking the postulate or postulates required to make inductive probabilities

approach certainty as a limit, there are two desiderata. On the one hand, the

postulate or postulates must be sufficient, from a purely logical point of view,

to do the work that is asked of them. On the other hand—and this is the more

difficult requirement—they must be such that some inferences which depend

upon  them  for  their  validity  are,  to  common  sense,  more  or  less

unquestionable. For example: you find two verbally identical copies of the

same book, and you assume unhesitatingly that they have a common causal

antecedent. In such a case, though every one will admit the inference, the

principle which justifies it is obscure, and is only to be discovered by careful

analysis. I do not demand that a general postulate arrived at by this method

should itself possess any degree of self-evidence, but I do demand that some

inferences which, logically, depend upon it, shall be such as any person who

understands them, except a sceptical philosopher, will consider so obvious as

to be scarcely worth stating. There must, of course, be no positive grounds for

regarding  a  suggested  postulate  as  false.  In  particular,  it  should  be  self-

confirmatory, not self-refuting, i.e. inductions which assume it should have

conclusions consistent with it.

  In  the  present  Chapter  I  propose  to  consider  a  postulate  suggested  by

Keynes, and called by him the “postulate of limited variety”. It is closely akin

to, if not identical with, an older postulate, that of natural kinds. We shall find

that the postulate is adequate logically as a basis for induction. I think, also,

that it can be stated in a form in which science to some degree confirms it. It

therefore satisfies two of the three requisites of a postulate. But it does not, in

my opinion, satisfy the third, namely that of being discoverable, by analysis,

as implicit in arguments which we all accept. On this ground, it seems to menecessary to seek other postulates, which I shall do in subsequent Chapters.

  Keynes’s postulate arises directly out of his discussion of induction, and is

designed to confer on certain generalizations that finite antecedent probability

which he has shown to be necessary. Before we consider it, let us examine an

argument which might seem to show that no postulate is necessary, since

every imaginable generalization has a finite antecedent probability which is

never less than a certain minimum.

  Let us take a case which arises in real life, and has a certain approximation

to pure chance, namely that of passengers on a big liner arriving with their

baggage at the custom house. Most pieces of baggage have a number of labels,

one showing the name of the owner, and others advertising hotels in which he

has  stayed.  We  can  then  consider  the  antecedent  probability  of  such  a

generalization as “every trunk having label A has also label B”.

  To complete the analogy with logic, let us suppose that there are also

negative labels, that no trunk has both the label “A” and the label “not-A”, but

that every trunk has one or other of these two. In the absence of further

information, if we select two labels A and B at haphazard, what is the chance

that every trunk having the label A also has the label B? Since every trunk has

either the label B or the label not-B, the chance that any given trunk has the

label B is a half. (I am assuming that we know nothing about B, and, in

particular, that we do not know whether it is a positive or a negative label.) It

follows that, if n trunks have the label A, the chance that they all have the
                  n
label B is 1 in 2. This is finite, and if N is the total number of trunks, it is

                      N
never less than 1 in 2.

  It follows from the above argument that, if the number of “things” in the

universe is some finite number N, the generalization “all A is B” always has

                                                            N
an  antecedent  probability  at  least  as  great  as  1/2.  This  is  the  antecedent

probability if everything has the property A; if only some things have this

property, the antecedent probability is greater. Therefore in theory a sufficient

postulate to add to Keynes’s theory of induction would be the assumption that

the  number  of  “things”  in  the  universe  is  finite.  This  is  equivalent  to  the

assumption that the number of space-time points is finite. This, in turn—if we

adopt the suggestion of an earlier Chapter, according to which a space-timepoint is a bundle of compresent qualities—is equivalent to the assumption that

the number of qualities is finite.

  I have no doubt that this assumption is a logically sufficient postulate.

There are, however, two objections to it. One is that science affords no way of

deciding whether it is true, so that it is not self-confirmatory; the other is that

N would have to be so large that no induction we can actually carry out

would achieve any tolerable degree of probability. Let us, therefore, put aside

the above suggestion as a mere curiosity, and proceed to consider Keynes’s

more practical hypothesis.

  What Keynes requires is that certain kinds of generalization should be

known to have a higher initial probability than belongs to generalizations that

are entirely random. He suggests, for this purpose, a postulate to the effect

that the qualities things may have fall into groups, and that a group may

become determinate when only some of the qualities composing it are given.

He supposes:

  “That the almost innumerable apparent properties of any given object all

arise out of a finite number of generator properties, which we may call φ1, φ2,

φ3,. … Some arise out of φ1 alone, some out of φ1 in conjunction with φ2, and

so on. The properties which arise out of φ1 alone form one group; those which

arise out of φ1 φ2 in conjunction form another group, and so on. Since the

number of generator properties is finite, the number of groups is also finite. If

a set of apparent properties arise (say) out of three generator properties φ1, φ2,

φ3 then this set of properties may be said to specify the group φ1 φ2 φ3. Since

the total number of apparent properties is assumed to be greater than that of

the generator properties, and since the number of groups is finite, it follows

that, if two sets of apparent properties are taken, there is, in the absence of

evidence to the contrary, a finite probability that the second set will belong to

the group specified by the first set.”

  The number of independent groups of the above sort is called the amount

of “variety” in the universe, or in whatever part of it is relevant to a particular

argument. Keynes’s statement of his postulate is as follows:

  “As a logical foundation for Analogy, therefore, we seem to need some

such assumption as that the amount of variety in the universe is limited insuch a way that there is no one object so complex that its qualities fall into an

infinite  number  of  independent  groups  (i.e.  groups  which  might  exist

independently as well as in conjunction); or rather that none of the objects

about which we generalize are as complex as this; or at least that, though

some  objects  may  be  infinitely  complex,  we  sometimes  have  a  finite

probability that an object about which we seek to generalize is not infinitely

          1
complex”.

  It was proved by Nicod that the postulate in the above form is not quite

adequate. It is not enough that every object should be of finite complexity; we

need that there should be a finite number such that no object has qualities

belonging to more than this number of independent groups. I shall consider

this emendation made.

  We shall, I think, best understand the scope of Keynes’s postulate if we

take a zoological illustration, say a cow. A cow is an animal, a vertebrate, a

mammal, a ruminant, and a member of one species of ruminants. Each of

these  classificatory  words  is  capable  of  various  definitions  which,  though

differing  in  intension,  give  the  same  extension.  How,  for  example,  do  we

distinguish a cow from other ruminants? Most of us are content with external

appearance: a cow is an animal that looks like a cow. This is quite adequate

for  practical  purposes,  but  a  zoologist  can  enumerate  a  variety  of

characteristics common and peculiar to cows, any one of which might be used

to  define  the  word  “cow”.  The  same  applies  to  “ruminant”,  “mammal”,

“vertebrate”,  and  “animal”.  Each  of  these  words  is  capable  of  various

definitions that are extensionally equivalent, though we do not know of any

reason why they should be so. It is obvious that, if this sort of thing happens

often, generalizations have a much greater antecedent probability than they

would have if properties were distributed at random.

  Let us endeavour to state Keynes’s hypothesis in somewhat more detail.

He  supposes  that—either  in  general  or  in  some  specified  province—it  is

possible to pick out a finite set of fundamental properties, such that, when we

know which of these properties an individual possesses, we can know (at least

in theory) what some, at least, of his other properties are, not because there is

a logical connection, but because in fact certain properties never occur exceptin conjunction with certain others, as, for example, all ruminants divide the

hoof. The hypothesis is analogous to the Mendelian theory of genes, according

to which a finite number of genes determine the whole congenital character of

an animal or plant. Keynes supposes that there is a finite number of groups of

qualities, and that two qualities belonging to the same group have the same

extension. If n is the number of such groups, and if two qualities are selected

at random, there is a chance 1/n that they belong to the same group, and that

therefore  all  individuals  possessing  either  quality  possess  the  other.  This

suffices to give Keynes the basis that he needs for validating induction.

  The postulate, as Keynes points out, can be weakened in various ways

without ceasing to be effective. One of these is that we need not suppose that

all properties belong to such groups as he postulates; it is sufficient if a finite

proportion do so. It is sufficient to justify some inductions, though not all, if

there is some definable class of properties all of which belong to Keynesian

groups. We can more or less distinguish characters distinctive of a species

from others which vary from individual to individual. Colour, for example, is

known to be very variable among animals, and therefore the stock fallacious

induction “all swans are white” was always less reliable than (say) “all swans

have long necks”. We may call a character “specific” when it belongs to all

members  of  some  species,  a  “species”  being  a  class  having  a  variety  of

common  properties  which  are  found  together  for  no  known  reason.  It  is

generally held that spatio-temporal position is never a specific character. It is

true that marsupials in a wild state only occur in Australia, but they do not

cease to be marsupials when they are brought to Zoos elsewhere.

  Induction may be needed to determine whether a given character is, or is

not, specific; but if we suppose that specific characters are a finite proportion

of all characters, this use of induction will be justifiable.

  For many purposes it suffices if we can establish that a great majority of

A’s are B’s; we may therefore soften Keynes’s postulate by supposing it to say

that certain characters are usually conjoined. If a “natural kind” is defined by

means  of  a  number  of  properties  A1,  A2,  …  An,  (not  known  to  be

interdependent),  we  may,  for  some  purposes,  consider  that  an  individual

which has all these qualities except one is still to be considered a member ofthe kind—for example, Manx cats are cats in spite of having no tail. Moreover

a great many distinctive characters are capable of continuous modification, so

that there are border-line cases where we cannot say definitely whether a

given character is present or absent. A natural kind is like what in topology is

called  a  neighbourhood,  but  an  intensional,  not  an  extensional,

neighbourhood. Cats, for example, are like a star cluster: they are not all in

one intensional place, but most of them are crowded together close to an

intensional  centre.  Assuming  evolution,  there  must  have  been  outlying

members so aberrant that we should hardly know whether to regard them as

part of the cluster or not. This view of natural kinds has the advantage that it

needs no modification before incorporation in advanced science.

  Such  considerations  suggest,  however,  a  transformation  of  Keynes’s

postulate into something more flexible and less reminiscent of a logical text-

book than the principle that he enunciates. It would seem that there must be

laws making certain kinds of combination more stable than other kinds, and

demanding that, when one character is slightly altered, another shall undergo

some  correlated  slight  alteration.  This  process  leads  to  functional  laws  of

correlation as probably more fundamental than natural kinds.

  The above line of thought seems appropriate in biology, but a somewhat

different  line  is  suggested  by  the  modern  theory  of  the  atom.  During  the

eighteenth  and  nineteenth  centuries  it  was  found  that  the  enormous

multiplicity of observed substances could be accounted for by supposing them

all  composed  out  of  ninety-two  elements  (some  still  unobserved).  Each

element, until our own century, had a number of properties which were found

to  coexist,  though  for  no  known  reason.  Atomic  weight,  melting  point,

appearance, etc., made each element a natural kind, as precisely as in biology

before evolution. At last, however, it appeared that the differences between

different elements were differences of structure, and were consequences of

laws which were the same for all elements. There are still natural kinds—at

the moment there are electrons, positrons, neutrons, and protons—but it is

hoped  that  these  are  not  ultimate,  and  may  be  reduced  to  differences  of

structure. Already in quantum theory their existence is somewhat shadowy

and unsubstantial. This suggests that in physics, as in biology since Darwin,the doctrine of natural kinds may prove to have been only a temporary phase.

  I conclude that the doctrine of natural kinds, though useful in establishing

such  pre-scientific  inductions  as  “dogs  bark”  and  “cats  mew”,  is  only  an

approximate and transitional assumption on the road towards fundamental

laws of a different kind. Both on this ground and because of its arbitrary

character, I cannot accept it as one of the postulates of scientific inference.1 Treatise on Probability, Chapter XXII, p. 258.                                          IV 


      Knowledge Transcending Experience


SOME modern empiricists—in particular, the majority of logical positivists—

have, in my opinion, misconceived the relation of knowledge to experience.

This has arisen, if I am not mistaken, from two errors: first, an inadequate

analysis  of  the  concept  “experience”,  and  second,  a  mistake  as  to  what  is

involved  in  the  belief  that  some  assigned  property  belongs  to  some

(undetermined)  subject.  Two  specific  problems  arise,  one  as  regards

significance, the other as regards knowledge of what are called “existence

propositions”, i.e. propositions of the form “something has this property”. It is

maintained, on the one hand, that a statement is not “significant” unless there

is some known method of verifying it; on the other hand, that we cannot

know “something has this property” unless we can mention a specific subject

that  has  the  property.  In  the  present  Chapter  I  wish  to  give  reasons  for

rejecting both these opinions.

  Before examining the abstract logic of these two problems, let us consider

them, for a moment, from a common-sense point of view.

  To begin with verification: There are some who maintain that, if atomic

warfare is not checked, it may lead to the extermination of life on this planet.

I am not concerned to maintain that this opinion is true, but only that it is

significant. It is, however, one which cannot be verified, for who would be left

to verify it if life were extinct? Only Berkeley’s God, whom, I am sure, logical

positivists would not wish to invoke. Going backwards instead of forwards,

we all believe that there was a time before there was life on the earth. Those

who regard verifiability as necessary to significance do not mean to deny such

possibilities,  but  in  order  to  admit  them  they  are  compelled  to  define

“verifiability”  somewhat  loosely.  Sometimes  a  proposition  is  regarded  as

“verifiable” if there is any empirical evidence in its favour. That is to say, “all

A is B” is “verifiable” if we know of one A that is B and do not know of one

that is not B. This view, however, leads to logical absurdities. Suppose there isno single member of A concerning which we know whether it is a B, but there

is an object x, not a member of A, which we know to be a B. Let A’ be the

class consisting of the class A together with the object x. Then “all A’ is B” is

verifiable in terms of the definition. Since this implies “all A is B”, it follows

that “all A is B” is verifiable. Consequently every generalization of the form

“all A is B” is verifiable if there is, anywhere, a single object known to be a B.

  Consider now a generalization of a different sort, such as we may wish to

make in connection with the doctrine of natural kinds. The generalizations I

have in mind are those of the form: “all predicates of the class A are true of

the  object  B”.  Applying  the  same  definition  of  “verifiability”,  this  is

“verifiable”  if  some,  or  at  least  one,  of  the  predicates  of  the  class  A  is

empirically  known  to  be  true  of  B.  If  this  is  not  the  case,  let  P  be  some

predicate known to be true of B, and let A’ be the class consisting of the class

A  together  with  P.  Then  “all  predicates  of  the  class  A’  are  true  of  B”  is

verifiable, and so, therefore, is “all predicates of the class A are true of B”.

  From these two processes it follows that, if anything is known to have any

predicate,  all  generalizations  are  “verifiable”.  This  consequence  was  not

intended, and shows that the above wide definition of “verifiability” is useless.

But  unless  we  allow  some  such  wide  definition,  we  cannot  escape  from

paradoxes.

  Let  us  next  consider  propositions  containing  the  word  “some”,  or  an

equivalent, e.g. “some men are black”, or “some quadrupeds have no tails”. As

a rule, such propositions are known by means of instances. If I am asked “how

do you know that some quadrupeds have no tails?” I may reply “because I

once had a Manx cat, and it had no tail”. The view which I wish to combat

maintains that this is the only way of knowing such propositions. This view

has been maintained by Brouwer in mathematics, and is maintained by some

other philosophers in regard to empirical objects.

  The  paradoxes  resulting  from  this  opinion  are  very  similar  to  those

resulting from the above doctrine as to verifiability. Take such a proposition

as “rain sometimes falls in places where there is no one to see it”. No sane

person would deny this, but it is impossible to mention a raindrop that has

never been noticed. To deny that we know that there are occurrences notobserved by any one is incompatible with common sense, but is necessary if

we never know such propositions as “there are A’s” except when we can

mention A’s that we have observed. Can any one seriously maintain that the

planet  Neptune  or  the  Antarctic  Continent  did  not  exist  until  it  was

discovered?  Again  only  a  Berkeleian  God  will  enable  us  to  escape  from

paradoxes. Or again: we all believe that there is iron in the interior of the

earth, but we cannot give instances beyond the depth of the deepest mine.

  Adherents  of  the  doctrine  that  I  am  combating  interpret  such  facts

hypothetically. They say that the statement “there is undiscovered iron” is an

abbreviation, and that the full statement should be: “if I did certain things, I

should  discover  iron”.  Suppose,  for  the  sake  of  precision,  we  take  the

statement “there is iron more than 1,000 miles below the surface of the earth”.

It is unlikely that anybody will ever find this iron, and, in any case, how can it

be known what a person would find? Only by knowing what is there to be

found. A hypothetical of which the hypothesis will probably always be false

tells  us  nothing.  Or  consider:  “there  was  once  a  world  without  life”.  This

cannot mean: “If I had been alive then, I should have seen that nothing was

alive”.

  Let  us  now  consider  the  above  two  doctrines  more  formally,  from  a

strictly logical point of view.A. Meaning and Verification


There is a theory that the meaning of a proposition consists in its method of

verification.  It  follows  (a)  that  what  cannot  be  verified  or  falsified  is

meaningless, (b) that two propositions verified by the same occurrences have

the same meaning.

  I reject both, and I do not think that those who advocate them have fully

realized their implications.

  First: practically all the advocates of the above view regard verification as

a social matter. This means that they take up the problem at a late stage, and

are unaware of its earlier stages. Other people’s observations are not data for

me. The hypothesis that nothing exists except what I perceive and remember

is for me identical, in all its verifiable consequences, with the hypothesis that

there are other people who also perceive and remember. If we are to believe in

the  existence  of  these  other  people—as  we  must  do  if  we  are  to  admit

testimony—we must reject the identification of meaning with verification.

    “Verification” is often defined very loosely. The only strict meaning of

verification is the following: A proposition asserting a finite number of future

occurrences is “verified” when all these occurrences have taken place, and are,

at some moment, perceived or remembered by some one person. But this is

not the sense in which the word is usually employed. It is customary to say

that a general proposition is “verified” when all those of its consequences

which it has been possible to test have been found to be true. It is always

assumed that, in that case, probably the consequences which have not been

tested are also true. But this is not the point with which I am concerned at

present. The point with which I am concerned at the moment is the theory

that  two  propositions  whose  verified  consequences  are  identical  have  the

same significance. I say “verified”, not “verifiable”; for we cannot know, until

the  last  man  perishes,  whether  the  “verifiable”  consequences  are  identical.

Take,  e.g.  “all  men  are  mortal”.  It  may  be  that  on  February  9,  1991,  an

immortal man will be born. The presently verifiable consequences of “all men

are mortal” are the same as those of “all men born before the time t aremortal, but not all those born later”, where t is any time not more than a

century before the present.

  If we insist upon using the word “verifiable” rather than “verified”, we

cannot  know  that  a  proposition  is  verifiable,  since  this  would  involve

knowledge  of  an  indefinitely  long  future.  In  fact,  that  a  proposition  is

verifiable is itself not verifiable. This is because to state that all the future

consequences of a general proposition are true is itself a general proposition of

which the instances cannot be enumerated, and no general proposition can be

established  on  purely  empirical  evidence  except  one  applying  to  a  list  of

particulars all of which have been observed. E.g. I may say “the inhabitants of

such-and-such a village are Mr. and Mrs. A, Mr. and Mrs. B, etc., and their

families,  all  of  whom  are  known  to  me  personally;  and  all  of  them  are

      1
Welsh”. But when I cannot enumerate the members of a class, I cannot, on

purely empirical grounds, justify any generalization about its members except

what follows analytically from its definition.

  There is however still a point to be made in favour of the verifiers. They

contend that there is a distinction between two kinds of cases. In one, we have

two propositions whose consequences hitherto have been indistinguishable,

but whose future consequences may diverge; e.g. “all men are mortal” and “all

men born before A.D. 2000 are mortal”. In the other, we have two propositions

whose observable consequences can never diverge; this is especially the case

with metaphysical hypotheses. The hypothesis that the starry heavens exist at

all times, and the hypothesis that they only exist when I see them, are exactly

identical in all those of their consequences that I can test. It is specially in such

cases that meaning is identified with verification, and that, therefore, the two

hypotheses are said to have the same significance. And it is this that I am

specially concerned to deny.

    Perhaps the most obvious case is other people’s minds. The hypothesis

that there are other people, having thoughts and feelings more or less like my

own, does not have the same significance as the hypothesis that other people

are only parts of my dreams, and yet the verifiable consequences of the two

hypotheses are identical. We all feel love and hate, sympathy and antipathy,

admiration  and  contempt,  for  what  we  believe  to  be  real  people.  Theemotional  consequences  of  this  belief  are  very  different  from  those  of

solipsism, though the verifiable consequences are not. I should say that two

beliefs  whose  emotional  consequences  differ  have  substantially  distinct

significations.

  But this is a practical argument. I should go further, and say, as a matter

of pure theory, that you cannot, without incurring an endless regress, seek the

significance  of  a  proposition  in  its  consequences,  which  must  be  other

propositions. We cannot explain what is the significance of a belief, or what

makes it true or false, without bringing in the concept “fact”, and when this is

brought  in  the  part  played  by  verification  is  seen  to  be  subsidiary  and

derivative.B. Inferential Existence-Propositions


A form of words containing an undetermined variable—for instance, “x is a

man”—is called a “propositional function” if, when a value is assigned to the

variable,  the  form  of  words  becomes  a  proposition.  Thus  “x  is  a  man”  is

neither true nor false, but if for “x” I put “Mr. Jones” I get a true proposition,

and if I put “Mrs. Jones” I get a false one.

  Besides giving a value to “x”, there are two other ways of obtaining a

proposition from a propositional function. One is to say that the propositions

obtained by giving values to “x” are all true; the other is to say that at least

one of them is true. If “f(x)” is the function in question, we will call the first of

these “f(x) always” and the second “f(x) sometimes” (where it is understood

that “sometimes” means “at least once”). If “f(x)” is “x is not a man or x is

mortal”, we can assert “f(x) always”; if “f(x)” is “x is a man”, we can assert

“f(x)  sometimes”,  which  is  what  we  should  commonly  express  by  saying

“there are men”. If “f(x)” is “I met x and x is a man”, “f(x) sometimes” is “I met

at least one man”.

  We call “f(x) sometimes” an “existence-proposition”, because it says that

something having the property f(x) “exists”. For instance, if you wanted to say

“unicorns exist”, you would first have to define “x is a unicorn” and then

assert that there are values of x for which this is true. In ordinary language,

the  words  “some”,  “a”,  and  “the”  (in  the  singular)  indicate  existence-

propositions.

  There is one obvious way in which we get to know existence-propositions,

and that is by means of instances. If I know “f(a)”, where a is some known

object, I can infer “f(x) sometimes”. The question I wish to discuss is whether

this is the only way in which such propositions can come to be known. I wish

to maintain that it is not.

  In  deductive  logic,  there  are  only  two  ways  in  which  existence-

propositions  can  be  proved.  One  is  the  above,  when  “f(x)  sometimes”  is

deduced from “f(a)” the other is when one existence-proposition is deduced

from  another,  for  instance  “there  are  bipeds”  from  “there  are  featherlessbipeds”. What other methods are possible in non-deductive inference?

    Induction,  when  valid,  gives  another  method.  Suppose  there are  two

classes A and B and a relation R, such that, in a number of observed instances,

we have (writing “a R b” for “a has the relation R to b”)


  a is an A. b is a B. a R b
    1111

  a is an A. b is a B. a R b
    2222

  a is an A. b is a B. a R b
    nnnn

•  and suppose we have no contrary instances. Then in all observed instances,

if a is an A, there is a B to which a has the relation R. If the case is one to

which induction applies, we infer that probably every member of A has the

relation R to some member of B. Consequently, if a is the next observed
                                                                n+1

member of A, we infer as probable: “there is a member of B to which a
                                                                                        n+1

has the relation R”. We infer this, in fact, in many cases in which we cannot

adduce any particular member of B such as we have inferred. To revert to

an earlier illustration, we all believe that probably Napoleon III had a father.

Not even a solipsist, if he allows himself any views as to his own future, can

escape from this sort of induction. Suppose, for instance, that our solipsist

suffers from intermittent sciatica, which comes on every evening; he may

say, on inductive grounds, “probably I shall be suffering pain at 9 p.m. to-

night”. This is an inference to the existence of something transcending his

present experience. “But”, you may say, “it does not transcend his future

  experience”. If the inference is valid it does not; but the question is: “how is

he to know how that the inference is probably valid?” The whole practical

utility of scientific inference consists in giving grounds for anticipating the

future; when the future has come and has verified the inference, memory

has replaced inference, which is no longer needed. We must, therefore, find

grounds for trusting the inference before it is verified. And I defy the world

to  find  any  such  grounds  for  trusting  inferences  which  will  be  verified,

which are not equally grounds for trusting certain inferences which will be

neither verified nor falsified, such as the inference to Napoleon III’s father.

  We are again faced with the question: in what circumstances is inductionvalid? It is futile to say: “Induction is valid when it infers something which

subsequent experience will verify”. This is futile, because it would confine

induction to cases in which it is useless. We must have reasons, in advance of

experience, for expecting something, and exactly similar reasons may lead us

to believe in some thing that we cannot experience, for example, the thoughts

and feelings of other people. The plain fact is that much too much fuss is

made about “experience”.

    Experience  is  needed  for  ostensive  definition,  and  therefore  for  all

understanding of the meanings of words. But the proposition “Mr. A had a

father” is completely intelligible even if I have no idea who Mr. A’s father

was. If Mr. B was in fact Mr. A’s father, “Mr. B” is not a constituent of the

statement “Mr. A had a father”, or, indeed of any statement containing the

words “Mr. A’s father” but not containing the name “Mr. B”. Similarly I may

understand “there was a winged horse” although there never was one, because

the statement means that, putting “fx” for “x has wings and is a horse”, I assert

“fx sometimes”. It must be understood that “x” is not a constituent of “fx

sometimes”  or  of  “fx  always”.  In  fact,  “x”  means  nothing.  That  is  why

beginners find it so hard to make out what it means.

  When  I  infer  something  not  experienced—whether  I  shall  or  shall  not

experience it hereafter—I am never inferring something that I can name, but

only  the  truth  of  an  existence-proposition.  If  induction  is  ever  valid,  it  is

possible  to  know  existence-propositions  without  knowing  any  particular

instance of their truth. Suppose, for instance, that A is a class of which we

have experienced members, and we infer that a member of A will occur. We

have only to substitute “future members of A” for “members of A” to make

our inference apply to a class of which we cannot mention any instance.

  I incline to think that valid inductions, and, generally, inferences going

beyond  my  personal  past  and  present  experience,  always  depend  upon

causation, sometimes supplemented by analogy. But this is a subject for later

Chapters; in the present Chapter I wished only to remove certain a priori

objections to a certain kind of inference—objections which, though a priori,

are urged by those who imagine themselves able to dispense with the a priori

altogether.1  But,  as  we  saw  in Part  II, Chapter  X,  such  general  enumerative  statements  involve  many

    difficulties.                                          V 


                                Causal Lines


THE concept “cause”, as it occurs in the works of most philosophers, is one

which is apparently not used in any advanced science. But the concepts that

are  used  have  been  developed  from  the  primitive  concept  (which  is  that

prevalent among philosophers), and the primitive concept, as I shall try to

show, still has importance as the source of approximate generalizations and

pre-scientific  inductions,  and  as  a  concept  which  is  valid  when  suitably

limited.

    “Cause”, as it occurs, for example, in J. S. Mill, may be defined as follows:

All events can be divided into classes in such a way that every event of a

certain class A is followed by an event of a certain class B, which may or may

not be different from A. Given two such events, the event of class A is called

the “cause” and the event of class B is called the “effect”. If A and B are

quantitative, there will usually be a quantitative relation between cause and

effect—e.g. a bigger charge of gun-powder, when it explodes, will cause a

louder noise. When we have discovered a causal relation, we can, given an A,

infer  a  B.  The  converse  inference,  from  B  to  A,  is  less  reliable,  because

sometimes  a  variety  of  causes  may  all  have  the  same  kind  of  effect.

Nevertheless,  with  proper  precautions,  backward  inference,  from  effects  to

causes, is very often possible.

  Mill supposes that the law of universal causation, more or less as we have

just  enunciated  it,  is  proved,  or  at  least  rendered  extremely  probable,  by

induction. His famous four methods, which are designed, in a given class of

cases, to find out what is the cause and what the effect, assume causation, and

do not depend upon induction otherwise than as induction is supposed to

justify  this  assumption.  But  we  have  seen  that  induction  cannot  prove

causation unless causation is antecedently probable. As a basis for inductive

generalization, however, causation may be something much weaker than it is

usually supposed to be. Suppose we start with the assumption that, givensome event, it is probable (not certain) that there is some class of events to

which it belongs, which is such that most (not necessarily all) members of the

class are followed by events of a certain other class. An assumption of this

sort  may  suffice  to  give  a  high  degree  of  inductive  probability  to

generalizations of the form “most A’s are followed by B’s”, if a great many

instances have been observed of A’s being followed by B’s and no contrary

instances have been observed.

    Whether from pure prejudice, or from the influence of tradition, or for

some other reason, it is easier to believe that there is a law of nature to the

effect that causes are always followed by their effects than to the effect that

this usually happens. We feel that we can imagine, or perhaps even sometimes

perceive,  a  relation  “cause-and-effect”,  which,  when  it  holds,  insures

invariable sequence. The only kind of weakening in the law of causation that

it is easy to admit is one that says, not that a causal relation may be not

invariable, but that in some cases there may be no causal relation. We may

find ourselves compelled to admit that quantum transitions and radio-active

disintegrations in single atoms have no invariable antecedents; although they

are causes, they are not effects, and there is no class of immediate antecedents

which can be regarded as their causes. Such a possibility may be admitted

without destroying the inductive power of evidence for a causal law, provided

it is still held that a large proportion of observable events are both causes and

effects. I shall assume this limitation conceded. That is to say, I shall consider

the law of causation to assert that causal sequences, when they occur, are

invariable,  and  that  they  occur  frequently,  but  not  that every  event  is  a

member of some invariable causal sequence.

  We  must  ask  ourselves:  when  we  assume  causation,  do  we  assume  a

specific  relation,  cause-and-effect,  or  do  we  merely  assume  invariable

sequence? That is to say, when I assert “every event of class A causes an event

of class B”, do I mean merely “every event of class A is followed by an event

of class B”, or do I mean something more? Before Hume, the latter view was

always taken; since Hume, most empiricists have taken the former.

  I am at present only concerned to interpret the law of causation, not to

inquire  into  its  truth.  As  a  matter  of  interpretation  of  what  is  commonlybelieved,  I  do  not  think  that  invariable  sequence  will  suffice.  Suppose  I

discovered  that  throughout  the  nineteenth  century  there  was  only  one

conchologist whose name began with X, and he married his cook. I could then

assert:  “All  nineteenth-century conchologists  whose  names  began  with  X

married their cooks”. But nobody would think this a causal law. Suppose you

had lived in the nineteenth century, and been called Ximenes. Suppose further

that you had a mild penchant  for  conchology  and  a  very  ugly  cook.  You

would not have said to yourself: “I must learn not to take an interest in shells,

because I don’t want to be compelled to marry this worthy but unattractive

female”. On the other hand, though Empedocles was (so far as I know) the

only man who leapt down the crater of Etna, we consider his fate a quite

sufficient reason for not following his example, because we think there was a

causal connection between his leap and his death.

    Geulincx’s two clocks, which both keep perfect time, and of which one

always  strikes  when  the  other  points  to  the  hour,  are  not  such  a  good

example, because there is an indirect causal connection between them. But

there are somewhat similar examples in nature, which afford illustrations.

Take, for example, two clouds of incandescent gas of a given element, both

emit the same spectral lines, but we do not think that either has any effect

upon  the  other.  Generally,  given  any  two  uniform  processes,  when  one

reaches a certain stage, the other also reaches a certain stage, but we do not, in

general, infer a causal connection, for instance between the earth’s rotation

and the period of a Cepheid variable.

  It seems clear, therefore, that invariable concomitance or succession is not

what we mean by causation: it is implied by causation, but not vice versa.

This is not yet to say that causation is a law of nature; it is only a conclusion

as to what is meant by “cause” in common parlance.

  Belief in causation, whether valid or not, is deeply embedded in language.

Consider how Hume, in spite of his desire to be sceptical, allows himself, from

the start, to use the word “impression”. An “impression” should be something

that presses in on one, which is a purely causal conception. The difference

between an “impression” and an “idea” should be that the former, but not the

latter, has a proximate cause which is external. Hume, it is true, professes tofind an intrinsic difference: impressions are distinguished from ideas by their

greater “liveliness”. But this won’t do: some impressions are faint, and some

ideas are lively. For my part, I should define an “impression”, or a “sensation”,

as a mental occurrence of which the proximate cause is physical, while an

“idea” has a proximate cause which is mental. If, as solipsism maintains, no

mental events have external causes, the distinction between “impressions” and

“ideas” is a mistake.

  In dreams we think we have impressions, but when we wake we usually

conclude that we were mistaken. It follows that there is no intrinsic character

which invariably distinguishes impressions from ideas.

  Belief in the external causation of certain kinds of experiences is primitive,

and is, in a certain sense, implicit in animal behaviour. It is involved in the

concept of “perception”. When you “perceive” a table or a person, the sun or

the moon, the noise of an explosion or the smell of a bad drain, it is, for

common sense, because what you are perceiving is there to be perceived. If

you think you are perceiving an object which in fact is not there, you are

dreaming, or suffering a hallucination, or misinterpreting a sensation. But it is

assumed  that  such  occurrences  are  sufficiently  uncommon,  or  sufficiently

queer, to be incapable of deceiving permanently anybody but a lunatic. Most

perceptions,  at  most  times,  are  taken  to  be  either  trustworthy  or  only

momentarily  deceptive;  persons  whose  professed  perceptions  threaten  our

security by their strangeness are locked up in asylums. Thus common sense,

by the help of the law, succeeds in preserving its belief that what seem like

perceptions usually have external causes which more or less resemble their

effects in perception. I think that common sense is in the right in this belief,

except that the resemblance between perception and object is probably less

than common sense supposes. This matter has been already considered; at

present we are concerned with the part played by the concept of “cause”.

  The conception of “cause”, as we have been considering it, is primitive

and. unscientific. In science it is replaced by the conception of “causal laws”.

The need for this development arises as follows. Suppose we have a common-

sense generalization that A causes B—e.g. that acorns cause oaks. If there is

any finite interval of time between A and B, something may happen duringthis time to prevent B—for example, pigs may eat the acorns. We cannot take

account of all the infinite complexity of the world, and we cannot tell, except

through  previous  causal knowledge,  which  among  possible  circumstances

would  prevent  B.  Our  law  therefore  becomes:  “A  will  cause  B  if  nothing

happens to prevent B”. Or, more simply: “A will cause B unless it doesn’t”.

This  is  a  poor  sort  of  law,  and  not  very  useful  as  a  basis  for  scientific

knowledge.

  There are three ways in which science overcomes this difficulty; they are

those  of  (1)  differential  equations,  (2)  quasi-permanence,  (3)  statistical

regularity. I will say something about each of these in turn.


(1)  The use of differential equations is necessary whenever a certain set of

    circumstances  produces  a  tendency  to  a  certain  change  in  the

    circumstances, and this change, in turn, alters the tendency to change.

    Gravitation  affords  the  most  familiar  example:  the  earth  has  at  every

  moment an acceleration towards the sun, but the direction of the sun is

  continually changing. The law of gravitation, therefore, has to state the

    tendency to change (acceleration) at each instant, given the configuration

  at that instant, leaving the total resulting change during a finite time to be

    calculated. Or take the “curve of pursuit”: A man is at one corner of a

  square field, and his dog is at an adjacent corner. The man walks along the

  side of the field that does not take him towards the dog; the dog at each

  instant runs towards his master. What will be the dog’s course? Obviously

  only differential equations will enable us to answer this question since the

  dog’s direction is continually changing.

•  This interpretation of causal laws is a commonplace of classical dynamics,

and need not detain us.

(2)  The importance of quasi-permanence is less conventional, and has been

  less noticed. It may be regarded, in a sense, as an extension of the first law

  of  motion.  The  first  law  of  motion  states  that  a  body  which  is  not

    interfered  with  by  outside  causes  will  continue  to  move  with  uniform

    velocity in a straight line. This implies, first, that the body will continue to

  exist, and secondly, that what may be regarded as “small” causes will  produce only small changes of direction or velocity. All this is vague, but

  it establishes what might be called “normal” expectations.

•  The law of quasi-permanence, as I intend it, is much more general than the

first law of motion, and is designed to explain the success of the common-

sense notion of “things” and the physical notion of “matter” (in classical

physics). For reasons given in earlier Chapters, a “thing” or a piece of matter

is not to be regarded as a single persistent substantial entity, but as a string

of events having a certain kind of causal connection with each other. This

kind is what I call “quasi-permanence”. The causal law that I suggest may be

enunciated as follows: “Given an event at a certain time, then at any slightly

earlier or slightly later time there is, at some neighbouring place, a closely

similar event”. I do not assert that this happens always, but only that it

happens  very  often—sufficiently  often  to  give  a  high  probability  to  an

induction confirming it in a particular case.

•  When “substance” is abandoned, the identity, for common sense, of a thing

or a person at different times must be explained as consisting in what may

be  called  a  “causal  line”.  We  normally recognize  a  thing  or  person  by

qualitative similarity to a former appearance, but it is not this that defines

“identity”. When a friend returns from years in a Japanese prison, we may

say “I should never have known you”. Suppose you know two twins whom

you cannot tell apart; suppose one of them, in battle, loses an eye, an arm,

and a leg. He will then seem much less like his former self than the other

twin  is,  but  we  nevertheless  identify  him,  not  the  other  twin,  with  his

former  self,  because  of  a  certain  kind  of  causal  continuity.  To  oneself,

personal identity is guaranteed by memory, which generates one kind of

“causal line”. A given piece of matter at a given moment may belong to

more than one causal line; for instance, my arm is always the same arm,

though  the  molecules  composing  it  change.  In  the  one  case  we  are

considering  anatomical  and  physiological  causal  lines,  in  the  other  case

those of physics.

•    The  conception  of  “causal  lines”  is  involved,  not  only  in  the  quasi-

  permanence of things and persons, but also in the definition of “perception”.

When I see a number of stars, each produces its separate effect on my retina,which  it  can  only  do  by  means  of  a  causal  line  extending  over  the

  intermediate space. When I see a table or a chair or a page of print, there are

causal lines from its parts to the eye. We can carry the chain of causation

further back, until we reach the sun—if we are seeing by daylight. But when

we go further back than the table or chair or page of print, the causes have

no longer any close resemblance to their effects. They are, moreover, not

events bound up with only one “thing”, but with interactions, e.g. between

the sun and the table. Consequently, the experience that I have when I “see a

table” can give me much knowledge concerning the table, but not much

  knowledge  concerning  earlier  parts  of  the  process  that  ends  in  my

experience. For this reason I am said to be seeing the table, not the sun. But

if  the  sun  is  reflected  in  a  good  mirror  I  am  said  to  be  seeing  the  sun.

  Generally, what is said to be perceived, in the kind of experience called a

  “perception”, is the first term in a causal line that ends at a sense-organ.

  A “causal line”, as I wish to define the term, is a temporal series of events

so  related  that,  given  some  of  them,  something  can  be  inferred  about  the

others whatever may be happening elsewhere. A causal line may always be

regarded as the persistence of something—a person, a table, a photon, or what

not.  Throughout  a  given  causal  line,  there  may  be  constancy  of  quality,

constancy of structure, or gradual change in either, but not sudden change of

any considerable magnitude. I should consider the process from speaker to

listener  in  broadcasting  one  causal  line:  here  the  beginning  and  end  are

similar  in  quality  as  well  as  structure,  but  the  intermediate  links—sound

waves,  electromagnetic  waves,  and  physiological  processes—have  only  a

resemblance of structure to each other and to the initial and final terms of the

series.

  That there are such more or less self-determined causal processes is in no

degree logically necessary, but is, I think, one of the fundamental postulates of

science. It is in virtue of the truth of this postulate—if it is true—that we are

able to acquire partial knowledge in spite of our enormous ignorance. That the

universe is a system of interconnected parts may be true, but can only be

discovered if some parts can, in some degree, be known independently of

other parts. It is this that our postulate makes possible.(3)  On statistical regularity it is not necessary to say much, since it appears to

  be an inference, not a postulate. Its importance in physics began with the

  kinetic theory of gases, which made temperature, for example, a statistical

  concept.  Quantum  theory  has  very  greatly  enhanced  its  status.  It  now

    appears  probable  that  the  fundamental  regularities  of  physics  are

  statistical,  and  are  not  such  as  to  tell  us,  even  in  theory,  what  an

    individual atom will do. The difference between this theory and the older

    individual  determinism  is  unimportant  in  connection  with  our  present

  problem, which is that of finding postulates that give the needed basis for

    inductive  inferences.  These  postulates  need  not  be  either  certain  or

    universal; we require only a probability that some characteristic occurs

  usually in a certain class of cases. And this is just as true in quantum

  mechanics as in classical physics.


Moreover  the  substitution  of  statistical  for  individual  regularities  has  only

been  found  necessary  in  regard  to  atomic  phenomena,  all  of  which  are

inferred. All the phenomena that can be observed are macroscopic, and the

problem of making such phenomena amenable to science remains what it was.                                          VI 


                  Structure and Causal Laws


IT  has  appeared  from  previous  discussions  that  induction  by  simple

enumeration is not a principle by which non-demonstrative inference can be

justified. I believe, myself, that concentration on induction has very much

hindered the progress of the whole inquiry into the postulates of scientific

method. In this Chapter I propose to bring out one such postulate, at first in a

somewhat  vague  form,  but  with  increasing  precision  as  the  discussion

proceeds.

  The principle with which I shall be concerned in this Chapter has to do

with  structure.  We  find  very  frequently  that  many  different  examples  of

approximately the same structure exist in different parts of space-time. The

anatomy of different human beings is more or less constant: the same bones,

the same muscles, the same arteries, and so on, are found in one individual as

in another. There is a lesser degree of identity of structure in all mammals, a

still  lesser  degree  in  all  vertebrates,  and  some  degree,  for  example  cell

structure, in everything living. There are a number of elements, each of which

is characterized by the structure of its nucleus. Coming to artifacts, there are,

for example, many copies of a given book; if they are all of the same edition

they will be very closely similar in structure.

  So far, I have been dealing with what may be called substantial structures,

that is to say, structures in which the structural unit may be considered to be a

piece of matter, but there are other structures where the unit is an event. Take,

for example, a piece of music. You may hear the C Minor Symphony many

times, sometimes well performed, sometimes badly. Whenever you hear it,

that particular hearing consists of a temporal series of noises. Two different

performances  are  not  exactly  identical  in  structure,  and  it  is  the  minute

differences which make the difference between a good performance and a bad

one. But they are all very nearly identical in structure, not only with each

other, but with the score. The reader will remark that “structure” is a veryabstract concept, so abstract that a musical score, a gramophone record, and

the actual performance may all have the very same structure. There is thus an

actual identity of structure, though not in every minute particular, between all

the different examples of a given piece of music, the original manuscript of

the composer, the various printed scores, the gramophone records, and the

performances.  Any  competent  person  who  hears  a  piece  of  music  while

following the score is perceiving the identity of structure between what he

hears and what he sees.

  I come now to another application of the concept of identical structures.

We all believe that we live in a common world, peopled not only by sentient

beings like ourselves, but also by physical objects. I say we all believe this, in

spite of the fact that some philosophers have pretended to doubt it. There are

on  the  one  hand  solipsists  who  maintain  that  they  alone  exist,  and  make

desperate efforts to make others agree with them. Then there are philosophers

who hold that all reality is mental, and that while the feelings we experience

when  we  look  at  the  sun  are  real,  the  sun  itself  is  a  fiction.  And  as  a

development of this view there is the theory of Leibniz, according to which

the  world  consists  of  monads  that  never  interact,  and  perception  is  in  no

degree due to the action of the outer world upon the percipient. In this view

we  may  be  said  to  be  all  dreaming,  but  the  dreams  that  we  all  have  are

identical in structure. These different views, I say, have been advocated by

different philosophers, and I do not think that any of them can be disproved.

On the other hand, none of them can be proved, and, what is more, none of

them can be believed, not even by their advocates. I am concerned at the

moment to search out a principle which, if true, justifies us in adhering to the

common-sense  belief  in  a  common  world  of  mental  and  physical  objects.

Suppose the Prime Minister makes a speech which is broadcast, and suppose a

number  of  people  who  have  listened  to  the  broadcast  afterwards  compare

notes. It will appear that so far as their memory serves they all heard the same

structure of sounds, that is to say if you, having a good memory, ask another

man with a good memory “what did you hear?” you will hear in reply what is

more or less a repetition of what you heard while listening to the broadcast.

You think it unlikely that if you and your friend were each enjoying a privatehallucination there would be such close similarity between his delusion and

yours. You need not, however, rely upon the memory of other people, which

is  fallible.  You  could, if  you  were  a  philosophic  millionaire,  have Hamlet

performed in a theatre in which you were the only live occupant and every

other seat was occupied by a cine-camera. When the performance was over

you could have the various records thrown upon the screen, and you would

find them closely resembling each other and your own memory; you would

infer that during the performance something happened at each of the cine-

cameras which had the same structure as what was happening at you. Both

light  and  sound  have  this  publicity,  that  is  to  say  a  suitably  contrived

instrument at any point within a certain region can be made to construct a

record identical in structure with what a person in that region hears or sees.

The recording instrument may be another person, or may be something purely

mechanical like a camera. So far as identity of structure is concerned there is

no difference between these two cases.

  The conception of an “observer”, which is usually taken for granted by

men  of  science,  is  one  of  which  the  use  and  validity  depends  upon  the

postulate  that  we  are  considering  in  this  Chapter.  To  say  that  many

“observers”  can  observe  the  “same”  occurrence  must  mean  that  this

occurrence has effects upon the various “observers” which have something in

common. If science is to have the publicity that we believe it to have, what

these  effects  have  in  common  must  be  something  which  (within  limits)

enables them to be described in the same words. If these words are as abstract

as those of mathematical physics, applicability of the same words involves

little, if anything, beyond similarity of space-time structure. Professor Milne

(Relativity,  Gravitation  and  World  Structure,  p.  5)  makes  this  similarity  a

fundamental postulate of physics when he says: “When the inner structure of

the system defined is identical from the two points of view (those of different

observers), then its description from the two points of view must be identical.

This is the essence of the principle of relativity”. It is astonishing how much

he derives from this postulate.

    Whenever there is throughout a certain’ neighbourhood and ranged about

a centre a group of complex events all identical in structure, as, for example,what  different  people  and  cameras  see  or  what  different  people  and

gramophone  discs  hear  in  a  given  theatre,  we  unhesitatingly  assume  a

common causal ancestor for all the different complex events. We do this the

more readily, because  the  different  events  differ  according  to  the  laws  of

perspective, and the principles of projective geometry enable us to infer the

approximate position of the object seen in different perspective by the various

spectators. If the object in question is the actor whose performance we have

been applauding, he will emphatically agree that he was the cause of the

various experiences of the members of the audience, and that they cannot

have arisen, as Leibniz supposes, as spontaneous developments in a system of

similar dreams.

  The same sort of principle occurs in many other connections. Take, for

example,  the  association  of  a  shadow  with  the  object  of  which  it  is  the

shadow.  Sometimes,  especially  at  sunset,  or  when  you  are  standing  with

others on the edge of a deep narrow valley and your shadow appears on the

hill opposite, you may have difficulty in deciding to which person a given

shadow belongs, but if you wave your arms and see the shadow wave its arms

you conclude that the shadow is yours; that is to say, you assume a certain

kind of causal connection between it and yourself. This causal connection you

infer from identity of structure in a series of events. In more usual cases you

do  not  need  a  series  of  events,  because  the  similarity  of  shape  will  be

sufficient, this similarity consisting in the identity of the projective properties

of the shadow and your own silhouette. Such identity of structure suffices to

persuade  you  that  there  is  a  causal  connection  between  yourself  and  the

shadow. Let us take another example from a very different field, that of the

brides-in-the-bath murders. A number of middle-aged ladies in different parts

of  the  country,  after  marrying  and  insuring  their  lives  in  favour  of  their

husbands, mysteriously died in their baths. The identity of structure between

these different events led to the assumption of a common causal origin; this

origin was found to be Mr. Smith, who was duly hanged.

  We  have  thus  two  different  cases  of  groups  of  objects  identical  in

structure: in the one case the structural units are material objects and in the

other case they are events. Examples of the former are: atoms of one element,molecules of one compound, crystals of one substance, animals or plants of

one species. Examples of the latter are: what different people simultaneously

see and hear in one neighbourhood, and what at the same time cameras and

gramophone discs record, the simultaneous movements of an object and its

shadow, the connection between different performances of the same piece of

music, and so on.

  We will distinguish the two kinds of structure as “event structures” and

“material structures”. A house has a material structure and the performance of

a  piece  of  music  has  an  event  structure.  The  distinction,  however,  is  not

always the relevant one: for example a printed book has a material structure,

while the same book read aloud has an event structure. A reporter is a man

who has the art of creating a material complex having the same structure as a

given event complex.

  I suggest, as a principle of inference used unconsciously by common sense,

but consciously in science and law, the following postulate: “When a group of

complex events in more or less the same neighbourhood and ranged about a

central event all have a common structure, it is probable that they have a

common  causal  ancestor”.  I  am  using  “probable”  here  in  the  sense  of

frequency; I mean that this happens in most cases. As for what I mean by

“common causal ancestor”, that requires a few words of explanation. I mean

that, taking any one of the complex events in question, it has been preceded

by other events having the same structure, these events forming a series each

of which is temporally and spatially contiguous to the next, and that when

such a backward series is formed for each of the complex events in question,

the various series all meet at last in one complex event having the given

structure and earlier in time than any of the events in the original group. In

the case of the people in the theatre, this event is the performance of the actor

or actors. In the case of a physical object seen simultaneously by a number of

people or photographed simultaneously by a number of cameras, the central

original event is the state of that physical object at the time when the light

rays which make it visible left it. I want to make it clear that the existence of

this central first cause is an inference, although it is one of which common

sense is usually unconscious. It is an inference which has stages, and which isinvolved in regarding noises heard as sometimes expressing the thoughts of

persons other than ourselves. If I hear a man utter a sentence, and I then ask

other people what sentence was uttered and they repeat just what I heard, and

if on another occasion I am absent while he spoke but those who were present

again all utter the same words in answer to my question, our principle leads

me to place the causal centre of these phenomena not in myself, but in the

other person. I know that when I speak and others hear me, the causal centre

consisted of certain thoughts and sensations of my own; when I do not hear

another man speak but those who heard him all agree as to what he said, I

know that I did not have the thoughts and feelings which I should have had if

I had uttered the words, but I infer that such thoughts and feelings existed in

the causal centre of the connected occurrences, i.e. in the speaker whom I did

not hear. This, however, involves the principle of analogy in addition to our

present principle.

  Before  attempting  to  give  more  precision  to  the  principle  that  I  am

suggesting, I will say a little more about its scope and plausibility. Broadly

speaking, what the principle asserts is that coincidences beyond a point are

improbable,  and  become  increasingly  improbable  with  every  increase  in

complexity. I once had a pupil who assured me that his name was Hippocrates

Apostolos; I found this hard to believe, so I pointed him out to someone who

knew  him  and  said:  “What  is  that  young  man’s  name?”  “Hippocrates

Apostolos,” he replied. I tried the experiment again and again with the same

result, and at last I looked up the University Register. In the end, in spite of

the initial improbability of his assertion, I was compelled to believe it. The

name being a complex structure, it seemed exceedingly improbable that if

everybody I asked had merely invented an answer on the spur of the moment

they should all have invented just that answer. If they had said John Smith I

should  have  felt  less  convinced,  because  this  is  a  less  complex  structure.

Eddington used to suggest as a logical possibility that perhaps all the books in

the British Museum had been produced accidentally by monkeys playing with

typewriters. There are here two different kinds of improbability: in the first

place  some  of  the  books  in  the  British  Museum  make  sense,  whereas  the

monkeys might have been expected to produce only nonsense. In the secondplace, there are many copies of most books, and two copies are, as a rule,

verbally identical. We can here secure plausibility by what is apparently an

application of the mathematical theory of probability: given a chance selection

of, say, a hundred letters, they will, in the immense majority of cases, not

constitute a significant English sentence. Suppose now that a book contains

700,000  letters,  the  chance  that,  selected  at  haphazard,  they  will  all  form

themselves  into  significant  sentences is  infinitesimal.  This  is  the  first

improbability, but there is a second. Suppose you have in your hands two

copies of the same book, and suppose you are considering the hypothesis that

the identity between them is due to chance: the chance that the first letter in

the two books will be the same is one in twenty-six, so is the chance that the

second letter will be the same, and so on. Consequently the chance that all the

letters  will  be  the  same  in  two  copies  of  a  book  of  700,000  letters  is  the


700,000th  power  of .  And  now  suppose  you  go  to  a  publisher’s

stockroom and find not merely two copies of the book in question, but some

thousands. The hypothesis of chance becomes exponentially more incredible.

You  feel  obliged,  therefore,  to  invent  some  hypothesis  to  account  for  the

similarity between the different volumes. At this moment the publisher who is

showing you round says: “that is one of our most successful books, and the

author is coming to see me in a few moments; perhaps you would like to meet

him”. You meet him and say “did you write that book?” He replies “yes”. At

this point, in spite of having been reduced to scepticism by Hume, it occurs to

you that perhaps the noises which seem to issue from the publisher and the

author signify what they would signify if you uttered them, and that the

many thousands of identical volumes that you have surveyed have a common

source in the object which says it is the author. While it is telling you how it

came to write the book, you perceive that the facts which have astonished you

will cease to be astonishing if there is a law of nature to the following effect:

“any complex event tends to be followed by other complex events identical, or

approximately identical, with it in structure, and distributing themselves from

next  to  next  throughout  a  certain  region  of  space-time”.  By  this  time  the

author has finished his speech and you take your leave, saying “pleased tohave met you”, since your new principle persuaded you, in spite of Hume, that

you really have met him, and that he is not merely part of your dream.

  The essential point in the principle that I am suggesting is its emphasis on

structure. When we examine causal sequences, we find that the quality of an

event may change completely in the course of such a sequence, and that the

only thing constant is structure. Take, say, broadcasting: a man speaks, and

his speech is a certain structure of sounds; the sounds are followed by events

in  the  microphone  which  are  presumably  not  sounds, these,  in  turn,  are

followed by electromagnetic waves, and these, in turn, are transformed back

into sounds, which, by a masterpiece of ingenuity, are closely similar to those

emitted by the speaker. The intermediate links in this causal chain, however,

do not, so far as we know, resemble the sounds emitted by the speaker except

in structure. (I should observe that the relations by which the structure is

defined  are  throughout  relations  involving  spatio-temporal  contiguity.)

Broadcasting was thought a wonderful invention, but, in fact, it is only very

slightly more complex than ordinary hearing. Consider what happens when

one  man  speaks  and  another  man  hears:  the  speaker  makes  certain

movements  in  his  mouth,  accompanied  by  breath,  which  causes  waves  to

proceed through the air from his mouth to the hearer’s ear. When these waves

reach the ear they cause currents to run along the nerves to the brain, and as

these currents reach the brain the hearer has a series of auditory sensations

closely similar to those which the speaker himself has if he is not deaf. The

only important difference from broadcasting is the omission of the stage of

electromagnetic waves; in each case there is a series of occurrences, some of

one sort and some of another, but all retaining the same structure, and it is

because of the constancy of structure that the speaker is able to communicate

with  the  hearer.  It  appears  generally  that,  if  A  and  B  are  two  complex

structures and A can cause B, then there must be some degree of identity of

structure between A and B. It is because of this principle that a complex of

sensations can give us information about the complex that caused them. If you

see something hexagonal, then, since hexagonality is a structural property, the

physical object which has caused your visual sensation must be hexagonal,

although its hexagonality will be in a space which is not identical with visualspace.

  It is to be observed that what we need in addition to actual experience is

only a principle giving probability to certain sorts of inductions. What I am

suggesting is that we are not merely to seek simple laws such as A causes B,

but are to enunciate a principle of the following sort: given two identical

structures, it is probable that they have a causal connection of one of two

kinds. The first kind consists of those having a common causal ancestor—this

is illustrated by the different visual sensations of a number of people looking

at a given object, and by the different auditory sensations of a number of

people hearing a given speech. The second kind arises where two structures

are composed of similar ingredients and there exists a causal law leading such

ingredients  to  arrange  themselves  in  a  certain  pattern.  The  most  obvious

examples  of  this  kind  are  atoms,  molecules,  and  crystals.  The  similarities

between different animals or plants of a given species may be brought under

either head: if we go no further back in time than a generation of the given

animals or plants we have a case of the second kind, and we have to suppose

that all the sperms of a given species have a certain identity of structure, and

so have all the ova. If, however, We take account of evolution, we can trace

the similarities to a common ancestry, using the word this time in its literal

sense.

    Whether a given set of complexes, all having the same structure, are to be

considered as of the first or of the second kind is not always an easy question,

and does not always have a definite answer, as we have just seen in the case

of two animals of the same species. In general, the former kind of complex has

events as the units of structure, while the latter has persistent physical objects

as  its  structural  units.  But  this  is  not  universally  the  distinguishing  mark.

Take, for example, the relation between writing and speech; the structural

units in speech are events, whereas the structural units in writing are material

objects,  but  when  there  is  identity  of  structure  between  a  spoken  and  a

written discourse either can cause the other, and does so in every case of

dictation or reading aloud. The same sort of thing applies to a piece of music

or a gramophone record. I think, however, that those cases in which a series of

events is represented by a static material structure can only arise where thereis some rule for taking the parts of the material structure in a time order and

so transforming them again into a series of events. A book in a European

language  has  to  be  read  from  left  to  right  and  from  top  to  bottom;  a

gramophone  record  has  to  be  played  with  the  needle  travelling  from  the

circumference towards the centre. Or take an instance where man has not

intervened: the interpretation of rocks by geologists as giving a history of the

world depends upon taking the rocks from the bottom upwards so that the

deepest rocks represent the earliest time.

  On  the  whole  it  may  be  said  that  similarity  of  structure  is  taken  as

showing common causal ancestry whenever the structure is very complex.

The similarities of structure which are not so interpreted occur in chemistry

and physics, and are all fairly simple. It seems to me that what one may say is

as follows: the physical world consists of units of a small number of different

kinds, and there are causal laws governing the simpler structures that can be

built  out  of  such  units,  causing  such  structures  to  fall  into  a  rather  small

number  of  discretely  differing  kinds.  There  are  also  complexes  of  events

which act as causal units, being preceded and followed throughout some finite

time by a series of complexes of events all having approximately the same

structure and inter-related by spatio-temporal contiguity.

  The principle of spatio-temporal contiguity has applications to cases in

which structure plays a subordinate role. Take, for example, echoes: anyone

hearing an echo of his own shout is incapable of doubting that something has

travelled from him to the object from which the echo is reflected and thence

back to him. We find that echoes only occur where there is a surface suitable

for reflecting sound, and that the time between the emission of a sound and

the hearing of the echo is proportional to the distance of such obstacle. It

would be extremely difficult to give a plausible account of echoes, either on a

solipsistic  basis  or  on  the  assumption  that  others’  minds  exist  but  lifeless

physical objects do not, since mountains give much more resounding echoes

than  persons  do.  Or  let  us  consider  again  an  experiment  suggested  in  an

earlier chapter. Suppose a man with a gun stationed at a point where many

roads meet; suppose that at every hundred metres along each of these roads

there is a post, and a man with a flag is stationed at each post up to a distanceof a thousand metres. Each of these men with a flag has orders to wave his

flag as soon as he hears the sound of the shot. A captive balloon is stationed

vertically above the man with the gun and contains an observer who notes the

moment at which each flag is waved; he finds that all the flags that are equi-

distant  from  the  shot  are  waved  at  the  same  moment,  but  that  the  more

distant flags are waved later than the nearer ones, and that the time lag is

proportional  to  the  distance.  All  this  is  explained  very  simply  by  the

hypothesis that there is a physical process which, when it reaches the ear,

causes a sensation of sound, and which travels with a velocity of about five

seconds to the mile. Any other hypothesis to account for the observed facts

would have to be very elaborate and very artificial. The man in the captive

balloon  sees  first  the  firing  of  the  gun  at  the  centre,  after  that  he  sees

successive waving of flags travelling outwards from the centre with a constant

velocity. What is convincing to scientific common sense in this experiment is

the  relation  between  distance  and  time  which  enables  us  to  speak  of  the

velocity of sound.

  Similar considerations to those applying to echoes apply to the reflection

of light, but in this case the argument for identity of structure has a force

which it does not have in the case of sound-echoes. When you see yourself in

a mirror it would be preposterous to suggest that the mirror chooses at that

moment to look like you without there being any causal connection, and, in

fact, the mirror only reflects you when you are in a suitable position, and

reflects any movements that you may make while in front of it. The mirror

can be stopped from reflecting you by the interposition of an opaque object,

which leads irresistibly to the conclusion that the reflection is due to some

process traversing the intervening space between you and the mirror. The

time lag which is noticeable between a sound and its echo is too small to be

appreciable in the case of terrestrial reflection of light, but on the other hand

the argument from identity of structure is very much stronger in the case of

light than in that of sound, because the structures that can be reflected are

much more complex in the case of light than in the case of sound.

  It must be admitted that it is logically possible to confine ourselves to the

solipsistic  hypothesis,  and  to  deny,  in  all  such  cases  as  we  have  beenconsidering, everything except our own experiences, but if we do this many

phenomena,  which  realist  hypotheses  explain  by  simple  laws,  become

hopelessly irregular and staccato.

  I think, therefore, that in the search for empirical laws we may employ the

following principles:


I.  When a number of similar structures of events exist in regions not widely

  separated,  and  are  ranged  about  a  centre,  there  is  an  appreciable

  probability that they have been preceded by a central complex having the

  same  structure,  and  that  they  have  occurred  at  times  differing  from  a

  certain time by amounts proportional to their distance from this central

  structure.

II.  Whenever a system of structurally similar events is found to be connected

  with a centre in the sense that the time when each event occurs differs

  from a certain time by an amount proportional to the distance of the event

  from this centre, there is an appreciable probability that all the events are

  connected with an event at the centre by intermediate links having spatio-

  temporal contiguity with each other.

III.  When a number of structurally similar systems, such as atoms of this or

  that element, are found to be distributed in what appears to be a random

  manner, without reference to a centre, we infer that there are probably

  natural  laws  making  such  structures  more  stable  than  others  that  are

  logically possible, but that are found to occur rarely or never.


  The first two of the above principles apply not only to systems in which

the propagation is spherical, as in light and sound waves, but also when it is

linear, as in the conduction of electricity along a wire. The causal route may

be any continuous curve in space-time. Consider, for instance, the journey of

a telegram which is forwarded from one address to another. But in all cases

our second principle assumes continuity.

  The above three principles, if accepted, will, I think, afford a sufficient a

priori  basis  for  a  large  proportion  of  the  inferences  that  physics  bases  on

observation. I have little doubt that all three principles can be simplified, orperhaps exhibited as consequences of one principle. In the meantime I offer

them  as  a  step  in  the  analysis  of  what  is  to  be  presupposed  in  scientific

inference.

  The principle of constancy of structure in causal series, which we have

been  considering,  while  it  has  great  importance  within  certain  regions,  is

definitely inapplicable in certain others. Let us consider discursively where it

applies and where it is inadequate.

  We have seen that the knowledge obtained through perception is only

possible  in  so  far  as  there  are  more  or  less  independent  causal  chains

proceeding from physical objects to ourselves. We see separate stars because

the light from each goes its way regardless of what else may be happening in

its neighbourhood. We see separate objects in our environment for the same

reason. But the independence of a causal chain is never complete. The light

from a star is slightly deflected by gravitation, and completely obscured by

cloud or fog. Terrestrial objects are seen more or less vaguely, according to

distance, keenness of vision, etc. Sometimes effects of this kind do not alter

structures,  but  only  diminish  the  amount  that  survives.  When  you  see  a

distant mountain on a clear day, you may see accurately what you do see in

the way of structure, but you see less than if you were nearer. When things

are reflected in a good mirror there is no change of structure, except, perhaps,

some omission of detail. But when white light is passed through a prism and

separated into the colours of the rainbow, there is change of structure, and so

there is when a drop of ink falls into a glass of water.

  Sometimes the change of structure is much more complete than in the

above cases. When a charge of dynamite explodes, all the structures involved

are  changed  except  the  atoms;  when  an  atomic  bomb  explodes,  even  the

atoms  change.  When  a  plant  or  animal  grows,  there  is  a  large  degree  of

constancy of structure, but in the moment of fertilization there is a change

which, structurally, is analogous to chemical combination. To such changes

our principle of constancy of structure is inapplicable.

    Natural  processes  are  of  two  kinds.  On  the  one  hand  there  are  those

characterized  by  some  form  of persistence;  on  the  other  hand  there  are

processes  of  synthesis  or  dissolution.  Persistence  is  illustrated  by  “things”,light-rays, and sound waves. Synthesis is illustrated by the presumed building

up  of  heavier  elements  from  hydrogen,  by  chemical  combination,  and  by

fertilization. Dissolution is illustrated by radioactivity, chemical analysis, and

decay of an animal body after death. In synthesis and dissolution structure

changes; in persistence structure remains in some degree constant.

  The principle considered in this chapter has to do only with persistence. It

is concerned to point out that persistence is a very common feature of natural

processes,  that  structure  is  what  is  most  apt  to  persist,  and  that,  when  it

persists, it fills a certain continuous region of space-time which usually has an

origin earlier in time than the rest of the region.

  The principle of constancy of structure has a certain analogy to the first

law of motion. The first law of motion tells what a piece of matter will do

when it is not influenced by its environment; the principle of constancy of

structure applies whenever a process is independent of its environment, but

also  in  various  other  cases.  It  applies  for  instance  to  all  the  stages  that

intervene between the oral movements of a speaker whose speech is being

broadcast and the auditory sensations of his hearers. It applies to echoes and

reflections in mirrors. It applies to every step from an author’s thoughts to the

printed book. In all these cases, though the environment has various effects

upon  the  process,  the  effects  are  such  as,  broadly  speaking,  do  not  affect

structure.

  From  the  standpoint  of  theory  of  knowledge,  the  most  important

application of our principle is to the relation between perception and physical

objects. Our principle implies that, in circumstances which occur frequently

but not invariably, the structure of a percept is the same as that of each of a

series  of  occurrences  leading  backward  in  time  to  an  original  occurrence,

before which there were not spatio-temporally connected events having the

structure  in  question.  This  original  occurrence  is  what  we  are  said  to

“perceive” when it is held that different people can “perceive” the same object.

  The sameness of structure between our sensational experiences and their

physical causes explains how it comes about that naive realism, though false,

gives rise to so little confusion in practice. Given two examples of the same

structure, every statement which is true of the one corresponds to a statementwhich is true of the other; the statement concerning the one is transformed

into the statement concerning the other by substituting corresponding terms

and corresponding relations. Take, for example, speech and writing, and for

the sake of simplicity let us assume a perfect phonetic alphabet. Then to every

shape which is a letter a certain sound corresponds, and to the relation left-to-

right  the  relation  earlier-to-later  corresponds.  It  is  in  virtue  of  this

correspondence that we can speak of an “accurate” written report of a speech,

in spite of the complete difference of quality between the two. In the same

way  perception  may,  in  suitable  circumstances,  give  an  “accurate”

representation  of  a  physical  occurrence,  although  there  may  be  as  much

difference between the occurrence and the percept as there is between speech

and writing.

  Given  two  corresponding  statements  concerning  two  examples  of  the

same structure, they may be related by a dictionary giving the words that

correspond in the two examples. But there is another method, which, though

less desirable, is often employed, and that is, to use the very same words in

making a statement about the one example as in making the corresponding

statement  about  the  other.  We  do  this  habitually  as  regards  speech  and

writing. The word “word” is used equally for what is spoken and for what is

written. So are such words as “sentence”, “statement”, “question”, etc. This

plan,  which  makes  all  our  words  ambiguous,  is  convenient  when  the

difference between the two examples of the same structure is irrelevant to our

purpose, and we wish to say things concerning both at once, e.g.: “Discourse is

composed of sentences, and sentences of words”—using “discourse” as a word

applicable both to speech and writing. And so in a printed book an author

may  speak  of  “the above  statement”  or  of  “an earlier  statement”,  though

strictly “above” is only applicable to print and “earlier” to speech.

  This form of ambiguity is involved when the language of naive realism is

used in spite of the fact that it is recognized to be philosophically unjustifiable.

In so far as physical objects have the same structure as percepts, a given form

of words may be interpreted (in the sense of Part IV, Chapter I) as applying to

objects or to percepts, and will be true of both or of neither. We may say of a

percept that it is blue, and we may say the same of a light-ray. The word“blue” will have a different meaning as applied to a light-ray from that which

it has when applied to a percept, but the meaning, in each case, is part of a

system of interpretation, and so long as we adhere to one system the truth or

falsehood of our statement is independent, within limits, of the system chosen.

It is because there are limits to this principle that philosophy is obliged to

reject  naive  realism.  But  in  spite  of  the  limits  the  principle  is  widely

applicable, and it is for this reason that naive realism is as plausible as it is.                                        VII 


                                  Interaction


WE have been chiefly concerned, in recent chapters, with a kind of causation

that  may  be  called  “intrinsic”.  This  is  the  kind  that  is  interpreted  as  the

persistence of a thing or a process. Owing to the fact that the persistence of

things is taken for granted and regarded as involving identity of substance,

this form of causation has not been recognized as what it is. It may be stated

as follows: “Given an event at a certain time and place, it usually happens

that,  at  every  neighbouring  time,  a  closely  similar  event  occurs  at  some

neighbouring  place”.  This  principle  affords  a  basis  for  a  great  many

inductions,  but  it  does  not, prima  facie,  enable  us  to  deal  with  what

commonly count as interactions, e.g. collisions between billiard balls. It is

causal processes of this kind that are to be considered in the present chapter.

  Consider  two  billiard  balls  which  hit  each  other  after  each  has  been

moving in a straight line. Each billiard ball persists after the collision, and is

regarded as the same  ball  as  it  was,  because  it  satisfies  the  above  law  of

intrinsic  causation.  But  there  is,  so  to  speak,  a  higher  degree  of  intrinsic

causation when no collision is taking place than when the balls meet. At most

times we can say not only that, given the position of a ball at one instant, it

will have some neighbouring position at a slightly later instant; we can say

also  that,  given  the  positions  of  the  ball  at  two  neighbouring  instants,  its

position at a third slightly later instant will be approximately collinear with

the two earlier positions, and its distance from either will be approximately

proportional to the time that has elapsed. That is to say, we have an intrinsic

law of velocity, not only of position. But when there is interaction, there is no

such intrinsic law of velocity. This is the purport of the first two laws of

motion.

  If we assume that, while we are observing the billiard balls, collisions

occupy a small portion of the total time involved, it will follow that at most

times  they  are  moving  approximately  in  straight  lines.  What  we  have  todiscover is a law determining the new direction in which a ball will move
                                                                    th
after a collision. If the smallest measurable angle is 1/n  of  a  degree,  the

number of measurably different directions in which the ball may move is

360n. Therefore, taking any direction which is determined as accurately as is

practically possible, the antecedent probability that the ball will start moving

in this direction is 1/360n. This is finite, though small; therefore induction

from observed collisions can make a generalization probable. That is to say, if

we assume our law of intrinsic causation, the rest of the mathematical theory

of  billiards  can  be  developed  by  means  of  induction  without  any  further

assumption antecedent to experience.

  Our law of intrinsic causation, in the course of the above analysis, became

enlarged to include velocity as well as position, not always, but at most times.

This amounts to assuming that the times when interaction takes place are

exceptional. This, however, is perhaps an overstatement. There is at all times

an interaction between-the billiard table and the billiard ball, which prevents

the ball from falling. But as this is constant it can be ignored, in the sense that

we can state laws for the movements of the ball without mentioning the table,

although, but for the table, these laws would not hold. If the ball collides with

another ball, we cannot state laws as to its motions without mentioning the

other ball, which is thus, in a sense, causally more important than the table.

What we assumed above amounts to this: At most times the approximate laws

governing the history of a “thing” do not involve mention of other “things”;

the  times  when  such  mention  is  essential  are  exceptional.  But  it  is  not

assumed that “intrinsic” laws give more than a first approximation.

    “Intrinsic” laws are to be held to apply not only to position and velocity,

but also to other matters. A red-hot poker, when taken out of the fire, ceases

gradually, not suddenly, to be red-hot. The sound of a bell decays gradually,

though swiftly. Very sudden occurrences, such as an explosion or a flash of

lightning,  are  exceptional.  Being  exceptional,  they  do  not  falsify  the

assumption that, on any given occasion, very sudden change is improbable.

And further, change in the direction of change is much more apt to be sudden

(more  or  less)  than  change  of  position  or  quality;  this  is  the  case  with

collisions of billiard balls.  The above suggestions can easily be brought into harmony with atomic

theory. An atom, it would seem, is at most times in a steady state, i.e. one in

which its history is governed by an intrinsic law; but the approach of a photon

or a neutron or an electron may lead to a more or less sudden change. I do not

wish,  however,  to  exaggerate  this  consonance  or  to  overestimate  its

importance. Our postulates are concerned more with the beginnings of science

than with its advanced results. The theory of impact, for example, was a very

early part of dynamics, using a somewhat primitive conception of “matter”. I

have been suggesting throughout that science necessarily begins with laws

which are only first approximations, and only applicable in most cases, but

which are completely true so long as they are not stated to be more than this.

Our  initial  postulates  must  share  this  character  of  approximation  and

probability. They must state that, in given circumstances, what occurs will

probably be roughly so-and-so. This suffices for a justifiable expectation, i.e.

an expectation having a fairly high degree of intrinsic credibility. As science

advances,  its  laws  acquire  a  higher  degree  of  probability,  and  also  of

exactness. A savage can say: “probably the moon will be full to-morrow”. An

astronomer  can  say:  “Almost  certainly,  the  moon  will  be  full  to-morrow

between 6h. 38m. and 6h. 39m. G.M.T.” But the advance is one of degree, not

of kind. And, throughout, the initial probable and approximate assumptions

remain indispensable.

  It will be observed that I have not introduced a postulate to the effect that

there are natural laws. My reason for not doing so is that, in any verifiable

form, such a postulate would be either false or a tautology. But let us see what

such a postulate could be.

  In any verifiable form, it will have to assert that, given a certain number

of observations of a suitable sort, there is a discoverable formula from which

something can be inferred as to some other phenomena. It will be noticed that

the number of observations concerned is necessarily finite, and that none of

them can be more exact than is rendered possible by the existing technique of

measurement. But here we come up against a difficulty analogous to that

which  confronted  us  when  we  tried  to  take  induction  as  a  postulate.  The

difficulty is that, given any finite set of observations, there are always aninfinite number of formulae verified by all of them. Suppose, for example, we

took  the  recorded  positions  on  the  celestial  sphere  of  Mars  on  Mondays,

Jupiter  on  Tuesdays,  and  so  on  throughout  the  days  of  the  week,  a  little

ingenuity in the use of Fourier series would enable us to construct a number

of  formulae  fitting  them  all  up  to  the  present,  but  mostly  falsified in the

future. It is therefore a tautology that there are formulae fitting any casually

selected set of quantitative observations, but it is false that a formula which

fits past observations affords any ground for predicting the results of future

observations.

  It is customary to add to the postulate that there are natural laws the

explicit or tacit proviso that they must be simple. This, however, is both vague

and teleological. It is not clear what is meant by “simplicity”, and there can be

no a priori reason for expecting laws to be simple except benevolence on the

part of Providence towards the men of science. It would be fallacious to argue

inductively  that,  since  the  laws  we  have  discovered  are  simple,  therefore

probably all laws are simple, for obviously a simple law is easier to discover

than  a  complicated  one.  It  is  true  that  a  number  of  laws  that  are

approximately true are very simple, and no theory of scientific inference is

satisfactory unless it accounts for this fact. But I do not think it should be

accounted for by making simplicity a postulate.

  Let us take an illustration which is historically important, namely the law

of falling bodies. Galileo, by a small number of rather rough measurements,

found that the distance traversed by a body falling vertically is approximately

proportional to the square of the time spent in falling—in other words, that the

acceleration is approximately constant. He assumed that, but for the resistance

of the air, it would be exactly constant, and when, not long afterwards, the air

pump was invented, this assumption appeared to be confirmed. But further

observations suggested that acceleration varies slightly with the latitude, and

subsequent theory suggested that it also varies with the altitude. Thus the

simple  law  turned  out  to  be  only  approximate.  Newton’s  gravitation

substituted a more complicated law, and Einstein’s, in turn, was very much

more  complicated  than  Newton’s.  A  similar  gradual  loss  of  simplicity  has

characterized the history of most of the early discoveries of science.  Nature and Nature’s laws lay hid in night.


  God said: “Let Newton be”, and all was light.


  It did not last. The Devil, shouting “Ho!


  Let Einstein be”, restored the status quo.


This oscillation is typical of the history of science.

  Let us take as another illustration the stages from observation to Kepler’s

first law as applied to Venus.

  The crude matter of observation is a bright dot in the sky, continuously

present while watched on a given fine evening, and slowly approaching the

western horizon. We believe this dot to be the appearance of a “thing”, but it

may not be: the reflection of a search-light on a cloud may be very similar.

The hypothesis that it is the appearance of a “thing” is heightened by the fact

that Venus can be seen in many countries at once. To this “thing” we give the

name “Hesper”. We find that on other occasions there is a morning star, to

which  we  give  the  name  “Phosphor”.  At  last,  as  an  ingenious  hypothesis,

Hesper  and  Phosphor  are  identified;  the  one  star  of  which  both  are

appearances is called “Venus”. This star is supposed to exist at all times, not

only when it is visible.

  The next step is to attempt to find laws determining the position of Venus

on  the  celestial  sphere  at  different  times.  To  a  first  approximation,  Venus

revolves daily with the fixed stars. To advance beyond this point, we assign to

Venus  angular  coordinates  θ,  φ  determined  by  relation  to  the  fixed  stars.

When  this  is  done,  the  changes  in  θ  and  φ  become  slow,  and  given  two

observations at not very distant times, the intermediate values of θ and φ can

be  roughly  determined  by  interpolation.  The  changes  in  θ  and  φ  are

approximately regular, but their laws are very complicated.

  So far we have been content with the hypothesis that all the heavenly

bodies are on the celestial sphere, and all at an equal distance from the earth.

But eclipses and occultations and transits lead to the abandonment of this

hypothesis. The next step is to suppose that the fixed stars and the several

planets each have their own sphere, and each preserve a constant distancefrom the earth. But this hypothesis also has to be abandoned.

  We  thus  arrive  at  the  following  formulation  of  the  problem:  Every

heavenly body has its position determined by three coordinates r, θ, φ, of

which θ and φ are given in observation, but r, the distance from the earth, is

inferred. It is assumed that r, like θ and φ, may vary with the time. Since r is

not observed, we have a free field for the invention of a suitable formula.

Certain observations, especially eclipses and occultations and transits, very

strongly suggest that Venus is always more distant than the moon, and is

sometimes more distant than the sun but sometimes nearer. The problem of

planetary theory is to invent a formula for the variation of r which shall be (a)

in harmony with such observations, (b) as simple as possible. Epicycles were

inferior to Kepler on both counts; Copernicus was superior on (b) but inferior

on (a). Since (a) must always outweigh (b) Kepler prevailed.

  In  the  above  there  are  several  important  steps  not  made  necessary  by

logic.


•  1st: Our visual sensations are assumed to have external causes.

•  2nd: These causes are assumed to persist when they are not causing visual

  sensations.

•   (These two steps are involved in giving the name “Venus”.)

•  3rd: The co-ordinate r is wholly outside observation. No possible system of

assumed values of r is inconsistent with observed facts, except making r

very small.

•    4th:  Kepler’s  formula  for r  is  the simplest  that  is  consistent  with

observation. This is its sole merit.


Observe that induction to the future has no special place in this process. The

essential  thing  is  inference  to unobserved  times.  This  is  involved  in  the

common-sense assumption of quasi-permanent objects, and therefore in the

name “Venus”. It is a mistake to say: “Venus has been observed to move in an

ellipse hitherto, therefore we infer by induction that it will continue to do so”.

No such thing has been observed hitherto. The observations are compatible

with Kepler, but also with a strictly infinite number of other hypotheses.    Mathematical probability does not play any part in the above inferences.

  The hypothesis that the heavenly bodies are permanent “things” is not

logically necessary. Heraclitus said “the sun is new every day”, and probably

preferred this view on scientific grounds, since it was difficult to see how the

sun could work its way underground during the night from west to east. The

hypothesis  embodied  in  Kepler’s  laws  is  not proved  by  observation;  what

observation proves is that the facts are compatible with this hypothesis. This

may be called the hypothesis of “complete realism”. At the other end is the

hypothesis of “complete phenomenalism”, according to which bright dots exist

when observed, but not at other times. Between these two are an infinite

number of other hypotheses, e.g. that Venus is “real” but Mars is not, or that

Venus is “real” on Mondays, Wednesdays, and Fridays, but not on Tuesdays,

Thursdays, and Saturdays. Both extremes and all intermediate hypotheses are

consistent with the observed facts; if we choose between them, our choice

cannot have any basis in observation alone.

  The conclusion to which the above somewhat discursive discussion has

seemed to lead is that the fundamental postulate is that of “causal lines”. This

postulate enables us to infer, from any given event, something (though not

much)  as  to  what  is  probable  at  all  neighbouring  times  and  some

neighbouring places. So long as a causal line is not entangled with another, a

good deal can be inferred, but where there is entanglement (i.e. interaction)

the postulate alone allows a much more restricted inference. However, when

quantitative measurement is possible, the measurably different possibilities

after  an  interaction  are  finite  in  number,  and  therefore  observation  plus

induction can make a general law highly probable. In this kind of way, step by

step, it would seem that scientific generalizations can be justified.                                        VIII 


                                    Analogy


THE  postulates  hitherto  considered  have  been  such  as  are  required  for

knowledge of the physical world. Broadly speaking, they have led us to admit

a certain degree of knowledge as to the space-time structure of the physical

world,  while  leaving  us  completely  agnostic  as  regards  its  qualitative

character.  But  where  other  human  beings  are  concerned,  we  feel  that  we

know more than this; we are convinced that other people have thoughts and

feelings that are qualitatively fairly similar to our own. We are not content to

think that we know only the space-time structure of our friends’ minds, or

their capacity for initiating causal chains that end in sensations of our own. A

philosopher might pretend to think that he knew only this, but let him get

cross with his wife and you will see that he does not regard her as a mere

spatio-temporal edifice of which he knows the logical properties but not a

glimmer of the intrinsic character. We are therefore justified in inferring that

his scepticism is professional rather than sincere.

  The problem with which we are concerned is the following. We observe in

ourselves such occurrences as remembering, reasoning, feeling pleasure and

feeling pain. We think that stocks and stones do not have these experiences,

but that other people do. Most of us have no doubt that the higher animals

feel pleasure and pain, though I was once assured by a fisherman that “fishes

have no sense  nor feeling”. I failed to find out how he had acquired this

knowledge.  Most  people  would  disagree  with  him,  but  would  be  doubtful

about oysters and starfish. However this may be, common sense admits an

increasing doubtfulness as we descend in the animal kingdom, but as regards

human beings it admits no doubt.

  It is clear that belief in the minds of others requires some postulate that is

not required in physics, since physics can be content with a knowledge of

structure. My present purpose is to suggest what this further postulate may be.

  It is clear that we must appeal to something that may be vaguely called“analogy”. The behaviour of other people is in many ways analogous.to our

own, and we suppose that it must have analogous causes. What people say is

what we should say if we had certain thoughts, and so we infer that they

probably  have  these  thoughts.  They  give  us  information  which  we  can

sometimes subsequently verify. They behave in ways in which we behave

when we are pleased (or displeased) in circumstances in which we should be

pleased (or displeased). We may talk over with a friend some incident which

we have both experienced, and find that his reminiscences dovetail with our

own; this is particularly convincing when he remembers something that we

have forgotten but that he recalls to our thoughts. Or again: you set your boy

a problem in arithmetic, and with luck he gets the right answer; this persuades

you that he is capable of arithmetical reasoning. There are, in short, very

many ways in which my responses to stimuli differ from those of “dead”

matter, and in all these ways other people resemble me. As it is clear to me

that the causal laws governing my behaviour have to do with “thoughts”, it is

natural to infer that the same is true of the analogous behaviour of my friends.

  The inference with which we are at present concerned is not merely that

which takes us beyond solipsism, by maintaining that sensations have causes

about which something can be known. This kind of inference, which suffices

for physics, has already been considered. We are concerned now with a much

more specific kind of inference, the kind that is involved in our knowledge of

the thoughts and feelings of others—assuming that we have such knowledge.

It is of course obvious that such knowledge is more or less doubtful. There is

not only the general argument that we may be dreaming; there is also the

possibility  of  ingenious  automata.  There  are  calculating  machines  that  do

sums much better than our schoolboy sons; there are gramophone records that

remember  impeccably  what  So-and-so  said  on  such-and-such  an  occasion;

there are people in the cinema who, though copies of real people, are not

themselves alive. There is no theoretical limit to what ingenuity could achieve

in the way of producing the illusion of life where in fact life is absent.

  But, you will say, in all such cases it was the thoughts of human beings

that produced the ingenious mechanism. Yes, but how do you know this? And

how do you know that the gramophone does not “think”?  There is, in the first place, a difference in the causal laws of observable

behaviour. If I say to a student “write me a paper on Descartes’ reasons for

believing  in  the  existence  of  matter”,  I  shall,  if  he  is  industrious,  cause  a

certain response. A gramophone record might be so constructed as to respond

to  this  stimulus,  perhaps  better  than  the  student,  but  if  so  it  would  be

incapable  of  telling  me  anything  about  any  other  philosopher,  even  if  I

threatened to refuse to give it a degree. One of the most notable peculiarities

of human behaviour is change of response to a given stimulus. An ingenious

person could construct an automaton which would always laugh at his jokes,

however often it heard them; but a human being, after laughing a few times,

will yawn, and end by saying “how I laughed the first time I heard that joke”.

  But  the  differences  in  observable  behaviour  between  living  and  dead

matter do not suffice to prove that there are “thoughts” connected with living

bodies other than my own. It is probably possible theoretically to account for

the  behaviour  of  living  bodies  by  purely  physical  causal  laws,  and  it  is

probably impossible to refute materialism by external observation alone. If we

are to believe that there are thoughts and feelings other than our own, that

must be in virtue of some inference in which our own thoughts and feelings

are relevant, and such an inference must go beyond what is needed in physics.

  I am of course not discussing the history of how we come to believe in

other  minds.  We  find  ourselves  believing  in  them  when  we  first  begin  to

reflect; the thought that Mother may be angry or pleased is one which arises

in early infancy. What I am discussing is the possibility of a postulate which

shall establish a rational connection between this belief and data, e.g. between

the belief “Mother is angry” and the hearing of a loud voice.

  The abstract schema seems to be as follows. We know, from observation

of ourselves, a causal law of the form “A causes B”, where A is a “thought”

and B a physical occurrence. We sometimes observe a B when we cannot

observe any A; we then infer an unobserved A. For example: I know that

when I say “I’m thirsty”, I say so, usually, because I am thirsty, and therefore,

when I hear the sentence “I’m thirsty” at a time when I am not thirsty, I

assume that some one else is thirsty. I assume this the more readily if I see

before me a hot drooping body which goes on to say “I have walked twentydesert miles in this heat with never a drop to drink”. It is evident that my

confidence  in  the  “inference”  is  increased  by  increased  complexity  in  the

datum  and  also  by  increased  certainty  of  the  causal  law  derived  from

subjective observation, provided the causal law is such as to account for the

complexities of the datum.

  It is clear that, in so far as plurality of causes is to be suspected, the kind of

inference we have been considering is not valid. We are supposed to know “A

causes B”, and also to know that B has occurred; if this is to justify us in

inferring A, we must know that only A causes B. Or, if we are content to infer

that A is probable, it will suffice if we can know that in most cases it is A that

causes B. If you hear thunder without having seen lightning, you confidently

infer that there was lightning, because you are convinced that the sort of noise

you heard is seldom caused by anything except lightning. As this example

shows, our principle is not only employed to establish the existence of other

minds, but is habitually assumed, though in a less concrete form, in physics. I

say “a less concrete form” because unseen lightning is only abstractly similar

to seen lightning, whereas we suppose the similarity of other minds to our

own to be by no means purely abstract.

    Complexity in the observed behaviour of another person, when this can all

be accounted for by a simple cause such as thirst, increases the probability of

the inference by diminishing the probability of some other cause. I think that

in  ideally  favourable  circumstances  the  argument  would  be  formally  as

follows:

  From subjective observation I know that A, which is a thought or feeling,

causes B, which is a bodily act, e.g. a statement. I know also that, whenever B

is an act of my own body, A is its cause. I now observe an act of the kind B in

a body not my own, and I am having no thought or feeling of the kind A. But

I still believe, on the basis of self-observation, that only A can cause B; I

therefore infer that there was an A which caused B, though it was not an A

that I could observe. On this ground I infer that other people’s bodies are

associated with minds, which resemble mine in proportion as their bodily

behaviour resembles my own.

  In practice, the exactness and certainty of the above statement must besoftened. We cannot be sure that, in our subjective experience, A is the only

cause of B. And even if A is the only cause of B in our experience, how can

we know that this holds outside our experience? It is not necessary that we

should know this with any certainty; it is enough if it is highly probable. It is

the assumption of probability in such cases that is our postulate. The postulate

may therefore be stated as follows:


  If, whenever we can observe whether A and B are present or absent,

  we find that every case of B has an A as a causal antecedent, then it is

  probable that most B’s have A’s as causal antecedents, even in cases

  where observation does not enable us to know whether A is present or

  not.

  This postulate, if accepted, justifies the inference to other minds, as well as

many other inferences that are made unreflectingly by common sense.                                          IX 


                      Summary of Postulates


AS the outcome of the discussions in previous Chapters of this Part, I suggest

that the postulates required to validate scientific method may be reduced to

five. It is highly probable that they can be further reduced, but I have not

myself succeeded in doing so. The five postulates to which previous analyses

have led us may be called:


I.  The postulate of quasi-permanence

II.  The postulate of separable causal lines.

III.  The postulate of spatio-temporal continuity in causal lines.

IV.  The postulate of the common causal origin of similar structures ranged

  about a centre, or, more simply, the structural postulate.

V.  The postulate of analogy.


  Each of these postulates asserts that something happens often, but not

necessarily  always;  each  therefore  justifies,  in  a  particular  case,  a  rational

expectation  which  falls  short  of  certainty.  Each  has  an  objective  and  a

subjective aspect: objectively, it asserts that something happens in most cases

of  a  certain  sort;  subjectively,  it  asserts  that,  in  certain  circumstances,  an

expectation falling short of certainty in a greater or less degree has rational

credibility. The postulates collectively are intended to provide the antecedent

probabilities required to justify inductions.I. The postulate of quasi-permanence.


The  chief  use  of  this  postulate  is  to  replace  the  common-sense  notions  of

“thing” and “person”, in a manner not involving the concept “substance”. The

postulate may be enunciated as follows:


  Given  any  event  A,  it  happens  very  frequently  that,  at  any

    neighbouring time, there is at some neighbouring place an event very

  similar to A.

  A “thing” is a series of such events. It is because such series of events are

common that “thing” is a practically convenient concept. It is to be observed

that, in a series of events which common sense would regard as belonging to

one “thing”, the similarity need only be between events not widely separated

in space-time. There is not very much similarity between a three-months’

embryo  and  an  adult  human  being,  but  they  are  connected  by  gradual

transitions  from  next  to  next,  and  are  therefore  accepted  as  stages  in  the

development of one “thing”.

  It will frequently happen—for example, in the case of a drop of water in

the  sea—that  there  are,  at  a  given  neighbouring  time,  many  neighbouring

events similar to A. We can pass by gradual transitions from any one drop in

the sea to any other. Our postulate neither affirms nor denies the multiplicity

of such events similar to A at a given time; it contents itself with asserting

that there is probably at least one such event. Our next postulate, that of

causal lines, will enable us to say that, when there are many such events at a

given time, there is usually one which has a special connection with A, of the

sort which makes us regard it alone as part of the history of the “thing” to

which A belongs. This is essential if we are to be able to say that a drop of

water in the sea at one time, rather than any other drop, is the “same” as a

certain drop at another time. Our present postulate does not suffice to enable

us to say this, but gives us a part of what we require.

  Our postulate has a subjective and an objective aspect. Suppose you have

been  looking  at  the  sun,  and  you  then  close  your  eyes.  Your  subjectivecondition  changes  rapidly,  but  not  discontinuously;  it  passes  through  the

stages of akoluthic sensation, immediate memory, and gradually fading true

memory. The sun, we believe, goes through no analogous changes; its changes

also,  we  believe,  are  gradual,  but  of  quite  a  different  sort.  Physical  and

psychological  continuity—for  example,  that  of  motion  and  that  of  fading

memory—have different laws, but both exemplify our postulate.II. The postulate of separable causal lines


This postulate has many uses, but perhaps the most important is in connection

with  perception,  for  example  in  attributing  the  multiplicity  of  our  visual

sensations in looking at the night sky to a multitude of stars as their causes.

The postulate may be enunciated as follows:


  It is frequently possible to form a series of events such that, from one

  or two members of the series, something can be inferred as to all the

  other members.

  The most obvious example is motion, particularly unimpeded motion such

as that of a photon in interstellar space. But even in the case of impeded

motion, so long as the phenomena can be interpreted as a “thing” changing its

position, there is an intrinsic causal law, though it tells us less than when the

motion  is  unimpeded.  For  instance,  we  can  recognize  a  billiard  ball

throughout a game of billiards; its motion is continuous, and its changes of

appearance  are  slight.  We  recognize  the  billiard  ball  by  means  of  laws  of

change which are intrinsic, in the sense that they do not  require that we

should take account of the effects of other things upon it.

  A series of events connected with each other in the manner suggested in

the postulate is what I call a “causal line”. What makes the inference possible

is a “causal law”. The first law of motion is an example, provided we give it

empirical content by adding that there are many motions in nature which, to

a first approximation, are unaffected by outside forces. The motion of light-

rays is the most obvious illustration.

  Our postulate is involved, however, in the very concept of “motion”. This

concept requires that something should preserve its identity while changing

its position. When we dispense with substance, the “something” will have to

be  a  series  of  events,  and  the  series  must  have  some  characteristic  which

facilitates the common-sense interpretation as a “thing” with changing states.

I suggest that the required characteristic is an intrinsic causal law, i.e. a law

which enables us to say something about unobserved members of the serieswithout having to take account of anything else in the world.

  As  we  have  seen,  when  two  causal  lines  interact,  for  example  in  the

collision of two billiard balls, we need no fresh postulate, but can content

ourselves with observation and induction.

  Our  postulates,  with  the  partial  exception  of  the  first,  all  involve  the

concept  of  “cause”.  I  cannot  accept  the  view  that  causation  is  merely

invariable  sequence.  This  opinion  cannot be  maintained  except  with  an

addendum (which is never made) to the effect that a “cause” must not be too

narrowly defined. A statement of the form “A is invariably followed by B”

requires that “A” and “B” should be general terms, such as “lightning” and

“thunder”. But it is possible to multiply the general terms applicable to a given

event, or to define them with quantitative precision, until “A” and “B” are

descriptions each only applicable to one event in the history of the world. In

that case, if A is the earlier, A is invariably followed by B, but in general we

should not regard A as the “cause” of B. We only think that A is the cause of

B if there are many instances of its being followed by B. In fact, I think, these

instances are regarded as evidence of something more than sequence, though

not, in general, as conclusive evidence.

  Between any two events belonging to one causal line, I should say, there is

a relation which may be called one of cause-and-effect. But if we call it so, we

must add that the cause does not completely determine the effect, even in the

most favourable cases. There is always some influence, which is also causal,

though in a slightly different sense, of the environment on the causal line. A

photon  in  interstellar  space  is  slightly  deflected  by  gravitation  from  its

rectilinear path, and in general the disturbing effect of the environment is

much greater than in this case. What our postulate asserts may be re-stated as

follows: A given event is very frequently one of a series of events (which may

last  a  fraction  of  a  second  or  a  million  years)  which  has  throughout  an

approximate law of persistence or change. The photon preserves direction and

velocity  of  motion,  the  billiard  ball  preserves  shape  and  colour,  a  foetus

develops into an animal of the appropriate species, and so on. In all these cases

there is spatio-temporal continuity in the series of events composing a causal

line; but this brings us to our third postulate.III. The postulate of spatio-temporal continuity


This postulate is concerned to deny “action at a distance”, and to assert that,

when there is a causal connection between two events that are not contiguous,

there  must  be  intermediate  links  in  the  causal  chain  such  that  each  is

contiguous to the next, or (alternatively) such that there is a process which is

continuous in the mathematical sense. When a number of people all hear a

speaker, it seems obvious that there is a causal connection between what the

different  auditors  hear,  and  it  also  seems  obvious  that,  since  they  are

separated in space, there must be a causal process in the intervening regions,

such as sound waves are considered to be. Or when you see a given person on

a  variety  of  occasions,  you  do  not  doubt  that  he  has  had  a  continuous

existence during the times when you were not seeing him.

  This postulate presupposes causal lines, and is only applicable to them. If

you know two twins, A and B, whom you cannot tell apart, and you see one

on one occasion and one on another, you cannot assume that a continuous

chain connects the two appearances until you have satisfied yourself that it

was the same twin on both occasions.

  This postulate is not concerned with the evidence for a causal connection

but with an inference in cases in which a causal connection is considered to be

already established. It allows us to believe that physical objects exist when

unperceived, and that it is in virtue of continuous processes in intervening

space  that  percipients  in  the  same  neighbourhood  have  perceptions  which

appear to be causally interconnected, though not directly caused the one by

the  other.  It  also  has  applications  in  psychology.  For  example,  we  may

recollect  a  given  occurrence  on  various  occasions,  and  in  the  intervening

times there is nothing observable that belongs to the same causal line as the

recollections, but we assume that there is something (in the brain?) which

exists at these intervening times, and makes the causal line continuous.

  A great many of our inferences to unobserved occurrences, both in science

and in common sense, depend upon this postulate.IV. The Structural Postulate


This postulate is concerned with certain circumstances in which inference to a

probable causal connection is warranted. The cases concerned are those in

which  a  number  of  structurally  similar  occurrences  are  grouped  about  a

centre. The phrase “grouped about a centre” is intentionally vague, but in

certain cases it is capable of a precise meaning. Suppose a given object to be

simultaneously seen by a number of people and photographed by a number of

cameras. The visual percepts and the photographs can be arranged by the laws

of  perspective,  and  by  the  same  laws  the  position  of  the  object  seen  and

photographed can  be  determined.  In  this  instance  the  sense  in  which  the

percepts and photographs are “.grouped about a centre” is precisely definable.

When a number of people hear the same sound, there is an equally precise

definition if there is an accurate method of determining when they hear it, for

it  is  found  that  the  times  when  they  hear  it  differ  from  a  given  time  by

amounts proportional to their distance from a certain point; in that case, the

point at the given time is the space-time centre or origin of the sound. But I

wish  to  employ  the  phrase  also  in  cases  (such  as  smells)  where  no  such

precision is possible.

  Of  the  three-fold  postulate  enunciated  in  Chapter  VI,  part  has  been

absorbed into our third postulate, and part is not at present relevant. What

remains is as follows:


  When  a  number  of  structurally  similar  complex  events  are  ranged

  about a centre in regions not widely separated, it is usually the case

  that all belong to causal lines having their origin in an event of the

  same structure at the centre.

  We say that this is “usually” the case, and the inference in a given instance

is therefore only probable. But the probability can be increased in various

ways. It is increased if the structure is very complex (e.g. a long printed book).

It is increased if there are many examples of the complex structure, e.g. when

six million people listen to the Prime Minister’s broadcast. It is increased byregularity  in  the  grouping  about  a  centre,  as  in  the  case  of  a  very  loud

explosion heard by many observers, who note the time when they hear it.

  It seems likely that the above postulate could be analysed into several

simpler postulates, and that the above ways of increasing probabilities would

then become demonstrable. But though I believe this to be possible, I have not

succeeded in doing it.

  The uses of this postulate have been sufficiently set forth in Chapter VI.V. The Postulate of Analogy


The postulate of analogy may be enunciated as follows:


  Given two classes of events A and B, and given that, whenever both A

  and B can be observed, there is reason to believe that A causes B, then

  if, in a given case, A is observed, but there is no way of observing

  whether B occurs or not, it is probable that B occurs; and similarly if B

  is observed, but the presence or absence of A cannot be observed.

  In connection with this postulate, it is necessary to recall what was said on

the subject of observed negative facts in Part II, Chapter IX. By looking out of

the window you can observe that it is not raining; this is different from not

observing that it is raining, which can be achieved by shutting the eyes. The

postulate is concerned with the second kind of non-observation, not with the

first, and there must be some reason for supposing that the unobserved fact, if

it occurs, will be unobservable. Suppose, for example that a barking dog is

running  after  a  rabbit,  and  for  a  moment  is  hidden  by  a  bush.  The  bush

accounts for your not seeing the dog, and allows you to infer that the bark,

which you still hear, is still associated with what you saw a moment ago.

When the dog emerges from the bush, you think your belief is confirmed.

  The non-perception of other minds is more analogous to that of the dog in

the bush than is generally thought. We do not see an object if an opaque body

is. between it and us, i.e. if no causal line leads from it to our eyes. We feel a

touch on any part of the body because causal lines travel along the nerves to

the brain from the part touched. If the nerves are cut, we feel nothing; the

effect is exactly analogous to that of an opaque body in the case of sight.

When some one else’s body is touched we feel nothing, because no nerves

travel from his body to our brain. Probably in time physiologists will be able

to make nerves connecting the bodies of different people; this will have the

advantage that we shall be able to feel another man’s tooth aching. In the

meantime, there are understandable reasons for the impossibility of observing

the bodily sensations of others, and therefore the fact that we do not observethem is no reason for supposing that they do not occur. It is only in cases

where some such reason for non-observability exists that our postulate can

legitimately be applied.

  Let us take as an illustration of our postulate the connection of certain

kinds of visual appearance with the expectation of hardness. There is a certain

kind of tactile sensation which leads us to call the body touched “hard”. The

word “hard” is a causal word: it denotes that property of an object in virtue of

which it causes a certain kind of tactile sensation. Our previous postulates

enable us to infer that there is such a property, which bodies possess while

they are causing the appropriate sensations. But our previous postulates do

not enable us to infer that bodies sometimes have this property when they are

not  being  touched.  But  now  we  find  that,  when  a  body  is  both  seen  and

touched, hardness is associated with a certain kind of visual appearance, and

our postulate allows us to infer that hardness is probably associated with this

visual appearance even when the body concerned is not being touched.

  As appears from the above discussion, this postulate has many uses in

addition to that of allowing us to infer mental occurrences connected with

bodies other than our own.

  The above postulates are probably not stated in their logically simplest

form, and it is likely that further investigation would show that they are not

all necessary for scientific inference. I hope and believe, however, that they

are  sufficient.  There  are  certain  epistemological  problems  connected  with

them which I shall consider in the next chapter; these problems do not depend

upon the exact form of the postulates, and would remain the same even if the

postulates were much modified.

  The postulates, in the form in which I have enunciated them, are intended

to justify the first steps towards science, and as much of common sense as can

be justified. My main problem in this Part has been epistemological: what

must we be supposed to know, in addition to particular observed facts, if

scientific inferences are to be valid? In dealing with this problem, it is not

science in its most advanced and technical form that we have to examine, for

advanced science is built on elementary science, and elementary science is

built on common sense. The progress of science is from generalizations thatare vague and liable to exceptions to others that are more nearly precise and

have  fewer  exceptions.  “Unsupported  bodies  in  air  fall”  is  a  primitive

generalization; the Psalmist noted that sparks are an exception, and nowadays

he might have added balloons and aeroplanes. But without this crude and

partly untrue law, we should never have arrived at the law of gravitation.

Premisses for theory of knowledge are always different from premisses for

logic, and it is premisses for theory of knowledge that I have been trying to

discover.

  In what sense can we be said to “know” the above postulates, or whatever

substitutes may hereafter be found preferable? Only, I think, in a sense which

takes account of the discussion of kinds of knowledge in Chapter I of this Part.

Knowledge  of  general  connections  between  facts  is  more  different  than  is

usually  supposed  from  knowledge  of  particular  facts.  Knowledge  of

connections between facts has its biological origin in animal expectations. An

animal which experiences an A expects a B; when it evolves into a primitive

man  of  science  it  sums  up  a  number  of  particular  expectations  in  the

statement  “A  causes  B”.  It  is  biologically  advantageous  to  have  such

expectations as will usually be verified; it is therefore not surprising if the

psychological laws governing expectations are, in the main, in conformity

with the objective laws governing expected occurrences.

  We may state the matter as follows. The physical world has what may be

called “habits”, i.e. causal laws; the behaviour of animals has habits, partly

innate,  partly  acquired.  The  acquired  habits  are  generated  by  what  I  call

“animal inference”, which occurs where there are the data for an induction,

but not in all cases where there are such data. Owing to the world being such

as it is, certain kinds of inductions are justified and others are not. If our

inductive propensities were perfectly adapted to our environment, we should

only be prone to an induction if the case were of the sort which would make

the induction legitimate. In fact, all except men of science are too prone to

induction when one of the characters concerned is interesting, and too little

prone to it when both characters are not easy to notice. When both characters

are interesting, the popular mind finds the impulse to induction irresistible:

comets foretell the death of princes, because both are felt to be noteworthy.But even in animal induction there are elements of validity. The inference

from smell to edibility is usually reliable, and no animal makes any of the

absurd inductions which the logician can invent to show that induction is not

always valid.

  Owing to the world being such as it is, certain occurrences are sometimes,

in fact, evidence for certain others; and owing to animals being adapted to

their environment, occurrences which are, in fact, evidence of others tend to

arouse expectation of those others. By reflecting on this process and refining

it, we arrive at the canons of inductive inference. These canons are valid if the

world  has  certain  characteristics  which  we  all  believe  it  to  have.  The

inferences made in accordance with these canons are self-confirmatory and

are not found to contradict experience. Moreover, they lead us to think it

probable that we shall have mental habits such as these canons will on the

whole justify, since such mental habits will be biologically advantageous.

  I think, therefore, that we may be said to “know” what is necessary for

scientific inference, given that it fulfils the following conditions: (1) it is true,

(2) we believe it, (3) it leads to no conclusions which experience confutes, (4) it

is logically necessary if any occurrence or set of occurrences is ever to afford

evidence in favour of any other occurrence. I maintain that these conditions

are  satisfied.  If,  however,  any  one  chooses  to  maintain  solipsism  of  the

moment,  I  shall  admit  that  he  cannot  be  refuted,  but  shall  be  profoundly

sceptical of his sincerity.                                          X 


                  The Limits of Empiricism


EMPIRICISM may be defined as the assertion “all synthetic knowledge is based

on experience”. I wish to consider what, exactly, this statement can signify,

and whether it is wholly true, or only true with certain limitations.

  Before  the  assertion  acquires  definiteness,  we  must  define  “synthetic”,

“knowledge”, “based on”, and “experience”. With the exception of the word

“synthetic”, these terms have been more or less defined in previous chapters,

but I will recapitulate, briefly and dogmatically, the conclusions of our earlier

discussions.  With  regard  to  the  word  “synthetic”,  the  precise  definition  is

difficult, but for our purposes we may define it negatively as any proposition

which is not part of mathematics or deductive logic, and is not deducible from

any proposition of mathematics or deductive logic. Thus it excludes not only

“2 and 2 are 4” but also “two apples and two apples are four apples”. But it

includes not only all statements of particular facts, but also all generalizations

which are not logically necessary, such as “all men are mortal” or “all copper

conducts electricity”.

    “Knowledge”,  as  we  have  seen,  is  a  term  incapable  of  precision.  All

knowledge is in some degree doubtful, and we cannot say what degree of

doubtfulness makes it cease to be knowledge, any more than we can say how

much loss of hair makes a man bald. When a belief is expressed in words, we

have to realize that all words outside logic and mathematics are vague: there

are objects to which they are definitely applicable, and objects to which they

are definitely inapplicable, but there are (or at least may be) intermediate

objects concerning which we are uncertain whether they are applicable or not.

When a belief is not expressed in words, but only displayed in non-verbal

behaviour, there is a great deal more vagueness than is usually the case when

it is expressed in language. It is even doubtful what behaviour can be regarded

as expressing a belief: going to the station to catch a train clearly does express

a belief; sneezing clearly does not; but putting up your arm to ward off a blowis  an  intermediate  case  which  inclines  towards  “yes”,  and  blinking  when

something approaches the eye is an intermediate case which inclines towards

“no”.

  But let us leave these difficulties in the definition of “knowledge”, as there

are others that are perhaps more important in the present context.

    “Knowledge” is a sub-class of true beliefs. We have just seen that “belief”

is not easy to define, and “true” is a very difficult term. I shall not, however,

repeat  what  was  said  about  this  term  in Part  II,  as  the  really  important

question for us is what must be added to truth in order to make a belief an

instance of “knowledge”.

  It  is  agreed  that  everything  inferred  from  a  piece  of  knowledge  by  a

demonstrative  argument  is  knowledge.  But  since  inferences  start  from

premisses, there must be knowledge which is un-inferred if there is to be any

knowledge.  And  since  most  inferences  are  non-demonstrative,  we  have  to

consider when such an inference makes its conclusion a piece of “knowledge”,

granted that we know the premisses.

  This second question has sometimes a precise answer. Given an argument

which, from known premisses, confers a probability p on a certain conclusion,

then, if the premisses embrace all the known relevant evidence, the conclusion

has a degree of credibility measured by p, and we may say that we have

“uncertain knowledge” of the conclusion, the uncertainty being measured by I

—p. Since all knowledge (or almost all) is doubtful, the concept “uncertain

knowledge” must be admitted.

  But  such  precision  is  seldom  possible.  We  do  not  usually  know  any

mathematical measure of the probability conferred by a non-demonstrative

inference,  and  we  hardly  ever  know  the  degree  of  doubtfulness  of  our

premisses. Nevertheless, the above gives a kind of ideal towards which we can

approximate  in  estimating  the  doubtfulness  of  a  conclusion  of  a  non-

demonstrative argument. The supposed absolute concept “knowledge” should

be replaced by the concept “knowledge with degree of certainty p”, where p

will be measured by mathematical probability when this can be ascertained.

  We have next to consider knowledge of premisses. These are prima facie

of three kinds: (1) knowledge of particular facts, (2) premisses of deductiveinference, (3) premisses of non-deductive inference. I shall ignore (2), which

has  little  relevance to  our  problems,  and  does  not  involve  any  of  the

difficulties in which we are interested in this inquiry. But both (1) and (3)

involve the fundamental issues with which we have been concerned.

  That knowledge of particular facts must depend upon perception is one of

the  most  essential  tenets  of  empiricism,  and  it  is  one  which  I  have  no

inclination to dispute. It was not admitted by those philosophers who accepted

the ontological argument, or by those who thought the characteristics of the

created world deducible from God’s goodness. Such views, however, are now

rare. Most philosophers now admit that knowledge of particular facts is only

possible  if  the  facts  are  perceived  or  remembered,  or  inferred  by  a  valid

argument  from  such  as  are  perceived  or  remembered.  But  when  this  is

admitted many difficulties remain. “Perception”, as we saw in Part III, is a

vague and slippery concept. The relation of perception to memory is not easy

to define. And the question what is a valid argument, when the argument is

non-demonstrative,  involves  all  the  problems  of Part  VI.  But  before

considering argument, let us concentrate on the part played by perception and

memory in generating knowledge.

    Confining ourselves for the present to verbal knowledge, we may consider

perception  and  memory  in  relation  to  (a)  understanding  of  words,  (b)

understanding of sentences, (c) knowledge of particular facts. We are here in

the region of Locke’s polemic against innate ideas and Hume’s principle “no

idea without an antecedent impression”.

  As regards the understanding of words, we may confine ourselves to such

as are defined ostensively. Ostensive definition consists in the repeated use of

a  certain  word  by  a  person  A  at  times  when  what  the  word  means  is

occupying the attention of another person B. (We may take it that A is a

parent and B a child.) It must be possible for A to surmise with a high degree

of probability what B is attending to. This is easiest in the case of objects

perceived by the public senses, especially sight and hearing. It is slightly more

difficult in such matters as toothache, earache, stomachache, etc. It is still

more difficult in regard to “thoughts”, such as recollections, the multiplication

table, etc. Consequently children do not learn to talk about these as early asthey learn to talk about cats and dogs. But in all such cases perception of the

object  which  is  what  the  word  means  is  still,  in  some  sense,  essential  to

understanding the word.

  At this point it is desirable to recapitulate certain theories that have been

set forth in Part II.

  There is a distinction between “object words” and “syntax words”. “Cat”,

“dog”, “Stalin”, “France” are object words; “or”, “not”, “than”, “but” are syntax

words. An object word can be used in an exclamatory manner, to indicate the

presence of what it means; this is, indeed, its most primitive use. A syntax

word cannot be so used. During a Channel crossing, when first Cap Grisnez

comes in sight, one may exclaim “France!”, but there are no circumstances in

which it would be appropriate to exclaim “than!”

  Syntax words can only be verbally defined in terms of other syntax words;

therefore in any language which has a syntax there must be undefined syntax

words. The question arises: what is the process of ostensive definition in the

case of a syntax word? Is there any way of pointing out what it means, in the

way in which one can point out a cat or a dog?

  Let us take the word “not” as it enters into the life of a child learning to

talk. It is, I think, derivative from the word “no”, which most children learn

very soon. The word “no” is intended to be associated with the expectation of

unpleasant  feelings,  so  that  an  act  otherwise  attractive  can  be  rendered

unattractive by the utterance of this word. I think “not” is only “no” confined

to the sphere of belief. “Is that sugar?” “No, it is salt, so if you sprinkle it on

your  plum  tart  you  will  experience  a  disagreeable  taste.”  There  are  ideas

which it is advantageous to act upon, and others which it is disadvantageous

to act upon. The word “not” means, initially, “disadvantageous to act upon”.

More simply: “Yes” means “pleasure this way”, and “no” means “pain that

way”. (The pleasure and pain may be due to social sanctions established by

the parents.) And so “not” will be initially only a negative imperative applied

to beliefs.

  But this seems still somewhat remote from what the logician means by

“not”.  Can  we  fill  in  the  intervening  stages  in  the  child’s  linguistic

development?  I think we may say that “not” means something like: “You do right to

reject the belief that …” And “rejection” means, primarily, a movement of

aversion. A belief is an impulse towards some action, and the word “not”

inhibits this impulse.

  Why this curious theory? Because the world can be described without the

use of the word “not”. If the sun is shining, the statement “the sun is shining”

describes a fact which takes place independently of the statement. But if the

sun is not shining, there is not a fact sun-not-shining which is affirmed by the

true statement “the sun is not shining”. Now clearly I can believe, and believe

truly, that the sun is not shining. But if “not” is unnecessary for a complete

description of the world, it must be possible to describe what is happening

when I believe that the sun is not shining without using the word “not”. I

suggest that what is happening is that I am inhibiting the impulses generated

or constituted by the belief that the sun is shining. This state of affairs is also

called a belief, and is said to be “true” when the belief that the sun is shining

is false. A perceptive belief is true when it has certain causal antecedents, and

false when it has others; “true” and “false” are both positive predicates. Thus

the word “not” is eliminated from our fundamental apparatus.

  A similar treatment may be applied to the word “or”.

  There is more difficulty about the words “all” and “some”. Either of these

may be defined in terms of the other and negation, since “f(x) always” is the

negation of “not-f(x) sometimes” and “f(x) sometimes” is the negation of “not-

f(x) always”. It is easy to prove the falsehood of “f(x) always” or the truth of

“f(x) sometimes”, but it is not easy to see how we can prove the truth of “f(x)

always” or the falsehood of “f(x) sometimes”. But at the moment I am not

concerned with the truth or falsehood of such propositions, but with how we

come to understand the words “all” and “some”.

  Take, say, the proposition “some dogs bite”. You have observed that this,

that, and the other dog bite; you have observed other dogs which, so far as

your experience went, did not bite. If, now, in the presence of a certain dog,

some  one  says  to  you  “that  dog  bites”,  and  you  believe  him,  you  will  be

prompted to certain actions. Some of these actions depend on the particular

dog, others do not. Those acts which will occur whatever dog it is may be saidto constitute the belief “some dogs bite”. The belief “no dogs bite” will be the

rejection of this. Thus beliefs expressed by using the words “all” and “some”

contain no constituents not contained in beliefs in whose verbal expression

these words do not occur.

  This disposes of the understanding of logical words.

  We may sum up this discussion of vocabulary as follows.

  Some words denote objects, others express characteristics of our belief-

attitude; the former are object-words, the latter syntax-words. An object-word

is  understood  either  through  a  verbal  definition  or  through  an  ostensive

definition.  Verbal  definitions  must,  in  the  last  resort,  employ  only  words

having  an  ostensive  definition.  An  ostensive  definition  consists  in  the

establishment of an association through the hearing of closely similar sounds

whenever  the  object  to  be  defined  is  present.  It  follows  that  an  ostensive

definition must apply to a class of similar sensible occurrences; to nothing else

is the process applicable. An ostensive definition can never apply to anything

not experienced.

  Passing  now  to  the  understanding  of  sentences,  it  is  clear  that  every

statement  that  we  can  understand  must  be  capable  of  being  expressed  in

words having ostensive definitions, or derived from a statement so expressed

by means of syntax words.

  The consequences of this principle are, however, not quite so far-reaching

as  is  sometimes  thought.  I  have  never  seen  a  winged  horse,  but  I  can

understand the statement “there is a winged horse”. For, if A is an object

which I have named, I can understand “A is a horse” and “A has wings”;

therefore I can understand “A is a winged horse”; therefore I can understand

“something  is  a  winged  horse”.  The  same  principle  shows  that  I  can

understand “the world existed before I was born”. For I can understand “A is

earlier than B” and “B is an event in my own life”; therefore I can understand

“if B is an event in my own life, A is earlier than B”, and I can understand the

statement that this is true of every B, for some A; and this is the statement

“the world existed before I was born”.

  The  only  disputable  point  in  the  above  is  the  assertion  that  I  can

understand the statement “A is an event in my own life”. There are variousways  of  defining  my  own  life,  all  equally  suitable  to  our  purpose.  The

following will do. “My life” consists of all events that are connected with this

by a finite number of memory-links backward or forward, i.e. remembering or

being remembered. Various other possible definitions will make the statement

in question equally understandable.

  Similarly,  given  a  definition  of  “experience”,  we  can  understand  the

statement “there are events that I do not experience” and even “there are

events which no one experiences”. Nothing in the principle connecting our

vocabulary with experience excludes such a statement from intelligibility. But

whether any reason can be found for supposing such a statement true, or for

supposing it false, is another question.

  To illustrate, take the proposition “there is matter perceived by no one”.

The word “matter” may be defined in various ways, in all of which the terms

used  in  the  definition  have  ostensive  definitions.  We  shall  simplify  our

problems if we consider the proposition “there are events perceived by no

one”. Clearly this is intelligible if the word “perceive” is intelligible. A piece of

matter, in my opinion, is a set of events; therefore we can understand the

hypothesis that there is matter perceived by no one. (A piece of matter may be

said to be perceived when one of its constituent events is connected with a

percept by a causal line.)

  The  reason  that  we  can  understand  sentences  that,  if  true,  deal  with

matters  lying  outside  experience,  is  that  such  sentences,  when  we  can

understand them, contain variables (i.e. “all” or “some” or an equivalent), and

that  variables  are  not  constituents  of  the  propositions  in  whose  linguistic

expression they occur. Take (say) “there are men whom I have never heard

of”. This says: “the propositional function ‘x is human and I have not heard of

x’ is sometimes true”. Here ‘x’ is not a constituent; no more are the names of

the  men  I  have  not  met.  But  to  the  principle  that  words  which  I  can

understand  derive  their  meaning  from  my  experience  there  is  no  need  to

admit any exceptions whatever. This part of empiricist theory appears to be

true without any qualification.

  It  is  otherwise  with  knowledge  of  truth  and  falsehood,  as  opposed  to

knowledge of the meanings of words. We must now turn our attention to thiskind  of  knowledge,  which,  in  fact,  alone  strictly  deserves  the  name  of

“knowledge”.

  Taking the question first as one of logic, we have to ask ourselves: “Do we

ever  know,  and  if  so  how,  (1)  propositions  of  the  form  ‘f(x)  always’,  (2)

propositions  of  the  form  ‘f(x)  sometimes’  in  cases  where  we  know  no

proposition  of  the  form  ‘f(a)’?”  We  will  call  the  former  “universal”

propositions and the latter “existence” propositions. A proposition of the form

“f(a)”, in which there are no variables, we will call a “particular” proposition.

  As  a  matter  of  logic,  universal  propositions,  if  inferred,  can  only  be

inferred  from  universal  propositions,  while  existence-propositions  can  be

inferred  either  from  other  existence  propositions  or  from  particular

propositions,  since  “f(a)”  implies  “f(x)  sometimes”.  If  we  know  “f(x)

sometimes” without knowing any proposition of the form “f(a)”, I shall call

“f(x) sometimes” an “unexemplified” existence-proposition.

  I  shall  assume,  on  the  basis  of  previous  discussions,  that  we  have

knowledge of some universal propositions and also of some unexemplified

existence propositions. We have to inquire whether such knowledge can be

wholly based on experience.


(1)  Universal propositions.—It would seem natural to say that what we learn

  by  perception  is  always  particular,  and  that  therefore,  if  we  have  any

  universal knowledge, this must be, at least in part, derived from some

  other source. But the reader may remember that doubt was thrown on this

  view by the discussions in Part II, Chapter X. We there decided that there

  are  negative  perceptive  judgments,  and  that  these  sometimes  imply

    negative universals. E.g. if I am listening to the B.B.C., I can make the

    negative perceptive judgment “I did not hear pippiness”, and infer “I heard

  no pips”. We saw that every empirical enumerative judgment, such as “I

  have just three children”, involves a process of the above kind. This is

    connected with the doctrine developed in Part IV, Chapter VIII, on the

    principle of individuation. The rule is simple: if the absence of a certain

  quality can be perceived, we can infer the absence of all complexes of

  which this quality is a constituent. There are therefore some universal    propositions which empiricism will allow us to know. Unfortunately they

  are  all  negative,  and  do  not  nearly  coincide  with  all  the  general

    propositions that we believe ourselves to know.

•  Universal propositions based on perception alone apply only to a definite

period of time, during which there has been continuous observation; they

cannot tell us anything about what happens at other times. In particular,

they can tell us nothing about the future. The whole practical utility of

  knowledge depends upon its power of foretelling the future, and if this is to

be possible we must have universal knowledge not of the above sort.

•  But universal knowledge of a different kind is only possible if some such

  knowledge is known without inference; this is obvious as a matter of logic.

Consider, for example, induction in its crude form. It is supposed by those

who believe in it that, given n observed facts f(a)1, f(a) … f(a), and no
                                                                    2n

observed  fact  not-f(b),  the  universal  proposition  “f(x)  always”  has  a

probability which approaches certainty as n increases. But in the statement

of this principle “a”, “a” … “a” and “f” are variables, and the principle is a
                      12n

universal proposition. It is only by means of this universal proposition that

the champions of induction believe themselves able to infer “f(x) always” in

the case of a particular “f”.

•  Induction, we have seen, is not quite the universal proposition that we need

to justify scientific inference. But we most certainly do need some universal

  proposition or propositions, whether the five canons suggested in an earlier

chapter or something different. And whatever these principles of inference

may be, they certainly cannot be logically deduced from facts of experience.

Either,  therefore,  we  know  something  independently  of  experience,  or

science is moonshine.

  It  is  nonsense  to  pretend  that  science  can  be  valid  practically  but  not

theoretically, for it is only valid practically if what it predicts happens, and if

our canons (or some substitute) are not valid, there is no reason to believe in

scientific predictions.

  There are some things to be said to soften the harshness of the above

conclusion. We need only more or less know our postulates; subjectively they

may be only certain habits according to which we infer; we need only knowtheir instances, not their general form; they all state only that something is

usually the case. But although this softens the sense in which we must know

them, there is only a limited possibility of softening the sense in which they

must be true; for if they are not in fact true, the things that we expect will not

happen. They may be approximate, and usual rather than invariable; but with

these limitations they must represent what actually occurs.

                                              1
(2)  Unexemplified existence-propositions.—There are here two different cases:

  (a)  when  there  is  no  example  in  my  experience,  (b)  when  there  is  no

  example in all human experience.

(a)  If you say “I saw a kingfisher to-day”, and I believe you, I am believing an

      existence-proposition of which I know no example. So I am if I believe

  “there was a king of Persia called Xerxes”, or any other fact of history

  before  my  time.  The  same  applies  to  geography:  I  believe  in  Cape  St.

  Vincent because I have seen it, but in Cape Horn only from testimony.

•  Inference to unexemplified existence-propositions of this sort is, I think,

always dependent upon causal laws. We have seen that where testimony is

involved, we depend upon our fifth postulate, which involves “cause”. Other

postulates as well are involved in any attempt to test the truthfulness of

witnesses.  All  verification  of  testimony  is  only  possible  within  the

framework  of  a  common  public  world,  for  the  knowledge  of  which  our

postulates (or equivalents) are necessary. We cannot therefore know such

  existence-propositions as the above unless adequate postulates are assumed.

(b)  Per contra, no more in the way of postulates is required to justify belief in

    existence-propositions not exemplified in any human experience than to

  justify belief when they are only not exemplified in my own experience.

  In principle, my grounds for believing that the earth existed before there

  was life on it are of just the same sort as my grounds for believing that

  you saw a kingfisher when you say you did. My grounds for believing that

  rain sometimes falls where there is no one to see it are better than my

    grounds for believing you when you say you saw a kingfisher; so are my

    grounds for believing that the summit of Mount Everest exists at times

  when it is invisible.We  must  therefore  conclude  that  both  kinds  of  unexemplified  existence-

propositions are necessary for ordinary knowledge, that there is no reason to

regard one kind as easier to know than the other, and that both require, if

they are to be known, the very same postulates, namely those that allow us to

infer causal laws from the observed course of nature.

  We can now sum up our conclusions as to the degree of truth in the

doctrine that all our synthetic knowledge is based on experience.

  In the first place, this doctrine, if true, cannot be known, since it is a

universal proposition of just the sort that experience alone cannot prove. This

does not prove that the doctrine is not true; it proves only that it is either false

or unknowable. This argument, however, may be regarded as logic-chopping;

it is more interesting to inquire positively into the sources of our knowledge.

  All  particular  facts  that  are  known  without  inference  are  known  by

perception or memory, that is to say, through experience. In this respect, the

empiricist principle calls for no limitation.

  Inferred  particular  facts,  such  as  those  of  history,  always  demand

experienced particular facts among their premisses. But since, in deductive

logic, one fact or collection of facts cannot imply any other fact, the inferences

from  facts  to  other  facts  can  only  be  valid  if  the  world  has  certain

characteristics  which  are  not  logically  necessary.  Are  these  characteristics

known to us by experience? It would seem not.

  In practice, experience leads us to generalizations, such as “dogs bark”. As

a starting-point for science, it suffices if such generalizations are true in a

large majority of cases. But although experience of barking dogs suffices to

cause belief in the generalization “dogs bark”, it does not, by itself, give any

ground for believing that this is true in untested cases. If experience is to give

such a ground, it must be supplemented by causal principles such as will make

certain  kinds  of  generalization  antecedently  plausible.  These  principles,  if

assumed, lead to results which are in conformity with experience, but this fact

does not logically suffice to make the principles even probable.

  Our knowledge of these principles—if it can be called “knowledge”—exists

at first solely in the form of a propensity to inferences of the kind that they

justify. It is by reflecting upon such inferences that we make the principlesexplicit.  And  when  they  have  been  made  explicit,  we  can  use  logical

technique  to  improve  the  form  in  which  they  are  stated,  and  to  remove

unnecessary accretions.

  The  principles  are  “known”  in  a  different  sense  from  that  in  which

particular facts are known. They are known in the sense that we generalize in

accordance with them when we use experience to persuade us of a universal

proposition such as “dogs bark”. As mankind have advanced in intelligence,

their inferential habits have come gradually nearer to agreement with the

laws of nature which have made these habits, throughout, more often a source

of true expectations than of false ones. The forming of inferential habits which

lead to true expectations is part of the adaptation to the environment upon

which biological survival depends.

  But although our postulates can, in this way, be fitted into a framework

which has what we may call an empiricist “flavour”, it remains undeniable

that our knowledge of them, in so far as we do know them, cannot be based

upon  experience,  though  all  their  verifiable  consequences  are  such  as

experience will confirm. In this sense, it must be admitted, empiricism as a

theory of knowledge has proved inadequate, though less so than any other

previous theory of knowledge. Indeed, such inadequacies as we have seemed

to find in empiricism have been discovered by strict adherence to a doctrine

by which empiricist philosophy has been inspired: that all human knowledge

is uncertain, inexact, and partial. To this doctrine we have not found any

limitation whatever.1 I am here summarizing the argument of Chapter III of this Part.                                      Index










Abstraction, principle of, 303

Abstractness, and common perception, 108

Acceleration, 31, 328, 497

Accuracy, and convenience, 302–3

Acquaintance, and description, 103 ff.

Action(s)

at a distance, 509

  human, and determinism, 54–5

  rules of, 417

Adaptation to facts, 160

Adrian, E. D., 52

After-images, 64

“All”, 145, 146 ff., 520

Alpha Centauri, 26

Alternatives, divisible, 392, 396

Ambiguity, verbal, and structural identity, 492–3

Analogy, 501 ff.

as premiss of knowledge, 208–9

  postulate of, 511–12

Analysis,

and minimum vocabularies, 274

  spatio-temporal, 275

Analytic statements, 94–5

Anaxagoras, 23Anaximander, 47

Ancestors, causal, see Casual “And” and “or”, relation, 151

Animals,

as automata, 29

  propensities and habits, 447

Antecedent, invariable, and cause, 333

Appropriate reactions, 200–1

Argument(s),

and credibility of beliefs, 409

and degrees of credibility, 399–400

long and short, 400

steps in, and data, 412

Aristarchus of Samos, 23, 25

Aristotle, 23, 29, 30, 46, 48

Arithmetic, induction in, 420

Arnauld, 317

Assertion and intention, 103

Assimilation, 46

Association,

and ostensive definition, 79

of ideas, 63

Astronomy, 23–8

  Greek conceptions, 23–5

  minimum vocabulary of, 261–3

Atomic number, 35

Atoms,

and quantum theory, 34.

  structure of, 35 ff.

Attitudes to ideas, expression of, 121

Augustine, St., 228, 284

Automata, animals as, 29

Awareness of logical connection, dim, 411–12

Axiom(s), 257  conjunctive, 363, 364, 369

  disjunctive, 363, 364, 369


Bayes’s theorem, 365

Behaviour,

and animal inferences, 199

  inference to minds, 504–5

  non-verbal, reference of, 130

  observable, cause of, 503

Belief(s)

and animal habits, 448–9

as state of mind and body, 161–2

as to the unexperienced, 170

  bodily state as a, 161

  definition, 129

and delayed reactions, 109

  external reference of, 123 ff.

  general, pre-verbal forms, 450

  habitual and active, 114

  how acquired, 114

“in”, 123

and knowledge, 113 ff.

  meaning, 160

and mental habit, 449

  non-verbal, 114

  perceptive and inferential, 180

  pre-linguistic, 109

  rational, and probability, 390

  reasons for, 66

  spontaneous and other, 181

  static, 164

and tendency to action, 116

true and false, 129, 165and understanding, 115–16

  unverbalized, 160–1

  varieties, 162

and verbal precision, 163

see also Truth

Bentham, J., 63

Berkeley, 9, 57, 65, 196

Bernouilli’s law, 365–6, 373–4, 386

Beta-particles,

time and, 291

  velocity, 305

Bible, biology and, 43–4

Biography, 292, 308

Body-mind relation, 212

Bohr, N., 35, 36–7

Booth (actor), 116

Brain,

and mind, 54–5, 244–5

  operation of, 53–4

and physical laws, 55

  unstable equilibrium possibility, 55–6

Broad, C. D., 363

Brouwer, 158, 464

Buffon, 44

Butler, Bishop, 357, 379, 415, 417


Calculus,

  infinitesimal, 31

of probability, interpretation of, 362

of propositions, minimum vocabulary of, 259

Calderon, 186

Carnap, 89, 90–1, 95, 262

Causal ancestors, common, 483Causal chains, never independent, 490–1

Causal continuity, spatio-temporal, 209–10

Causal laws, 326 ff., 474 ff., 508

and expectations, 444

  meaning of, 344

  mental, 63–5

and structure, 479 ff.

Causal lines, 333–4, 476–7, 500, 510

  separable, postulate of, 507–9

Causality, and scientific method, 329

Causation, 329

belief in, and language, 473

  intrinsic, 494 ff.

law of universal, 471

  physical, and perception, 212–13

Cause(s)

  -and-effect relation, 472, 509

and effect, distinction of, and time-order, 349

  independence of, 215–16

  meaning of, 332

  plurality of, 504

  primitive concept, 471 ff.

and truth, 134–5

see also Antecedent

Centrality, 280–2

Certainty,

  epistemological, 414

kinds of, 413–14

  logical, 414

  psychological, 414

  subjective, and credibility, 414–15

  subjective, degrees of, 413–15

Chains, causal, 244  never independent, 490–1

Change,

  causai laws of, 328

  discontinuity of, 216

  perception of, 226

  sudden, in atomic physics, 495

Characters, specific, 460

Chemistry, relation to physics, 214–15

Clarke, Dr., 32

Class, fundamental, 407–8

Class names, see Names

Classes, “manufactured”, 432

Classification, words and, 441

Cleanthes, 25

Coherence, and knowledge, 172, 173

Coincidences, improbability of complex, 484

Colours,

  definition of, 277 ff.

  incompatibility of, 138–42

Common-sense world, and physical world, 339 ff.

Compresence, 312, 314–15, 321–2

  complex of, 312, ff., 322 ff. when determinate, 324

as ordering relation of space-time, 347

  private, 323

Conjunctions, 136

Connections, between facts, knowledge and, 439, 448–50

Constant(s)

  cosmical, 33

  electronic, 42

  physical, 41–2

  quantum, see Quantum

and variables, 88–9

Contiguity,as ordering relation of space-time, 347

  spatio-temporal, 488

Continuity, spatio-temporal, 209–10, 224, 324

  postulate of, 509

Contradiction, law of, 141

Convention and belief, 167

Conviction, subjective, and credibility, 360

Co-ordinates, 33, 346

as substitutes for names, 89, 90–2

of astronomical space, 338

Copernican system, Aristarchus and, 23

Copernicus, 25

Copunctual areas and volumes, 297–8

Correlation, functional laws of, 461

Cosmical constant, 33

Cosmology,

  Dante’s, 24–5

  Greek, 23–4

  modern, 25 ff.

Credibility,

  degree(s) of, 11, 359–60, 398 ff.

and frequency, 402 ff.

and mathematical probability, 399

of data, 409 ff.

  rational, and probability, 393–4

and subjective conviction, 360

and truth, 360


Dante, 24

Darwin, 45, 48

Data, 181, 185–6

  credibility of, 409 ff.

and degrees of credibility, 399and inference to “things”, 242

and inferences, relation, 401

  physical and psychological, 59

  privateness of, 10

  public and private, 59–62

  remembered facts as, 205

and solipsism, 191

  uncertainty of, 412–13

visual, qualities of, 316

why private, 241–2

Dates, 287–8, 323

  knowledge of, 104

De Broglie, 37

Decalogue, the, 415–16

Deduction, 171–2

Definition(s),

  denotational, and names, 296

  denotative and structural, 293–4

  ostensive, 18, 78 ff., 518, 521

      and association, 79

        conditions of, 79

      and interpretation, 258

      kinds of words learnt by, 83–4

        limits of, 84

        vagueness of, 442

        what words can have, 89

  verbal and ostensive, 18

Democritus, 32

Descartes, 26, 29, 32, 47, 57, 172, 188

Descriptions, as substitutes for names, 92–4

Desire,

  expression and assertion of, 120

and ideas, 110Determinism,

  atoms and, 38

and human actions, 54–5

  physical, 29–30

Dewey, John, 172, 186, 409, 414

Difference, 139

Disbelief, character of, 141

Discreteness, and definition of qualities, 277

Disjunctive facts, 142 ff.

Dissolution, in natural processes, 491

Distance,

  ambiguity of, 306

  estimation of, 339

  measurement of, 303

  spatial and temporal, distinction, 349

Divisibility, in Keynes, 392, 396, 406

Doubt, Cartesian, 188, 196

Doubtfulness, intrinsic, 359

Dreams, 181–2, 184, 186

as scientific data, 60

and false memories, 230

  memory and perception in, 230

Duration, 284, 290


Earth,

age of, 50

  rotation of, 33

Eddington, Sir A., 27, 28, 41, 178, 349, 427

Effect, see Cause

Egocentric particulars, 84, 100 ff.

Einstein, 33–4, 213–14, 305, 328, 497

Electronic constants, 42

Electrons, 35 ff.Elements, the four, 29

Emotion and information, 73

Emotional consequences of beliefs, and signification, 467

Emotional interest and habit, 202

Empiricism,

  defined, 516

  inadequacy of, 527

its limits, 516 ff.

and solipsism, 195

  whether true, 525–6

Energy, 309

  conservation of, 40

  definition of, 264–5

  kinetic and potential, 39–40

and mass, 39–40, 309

  physical laws and, 345–6

  role in quantum physics, 41, 256

Enumeration, complete, 148

Enumeration, simple, 418–24

Environment, unusual, and deception, 185

Equality, 302, 395

Equations, differential, and causal laws, 475

Equilibrium, unstable, possible explanation of mental processes, 55–6

Equiprobability, 374

Eratosthenes, 23

Error, 201

  absolute and quantitative, 414

of memory, 229–30

Ethics,

and probability, 416

  theories of, 415–16

Euclid, 254

Event(s),and instants, 287, 297

  meaning, 97–8, 292, 298

  mental, as data, 186

        common-sense and, 240–1

        connections between, 63

        definition, 245

as particulars, 311

  physical, definition, 245

      and mental, differences, 246–7

  relation of, in relativity theory, 308

  unique, spatio-temporal position, 313 ff., 322

Evidence, 171

Evolution, 45 ff.

  discontinuous, 335

and knowledge, 439

Existence, struggle for, 48

Existence-propositions, 157, 463, 468–470

  how known, 468

  unexemplified, 524–5

Expansion, with heat, 300

Expectation(s)

and causal laws, 443

  development of, 444

  immediate, 113–14 and time, 226

  importance of, 443–4

and knowledge, 445 ff.

part played in judgment, 443

and scientific laws, 331–2

and truth, 167–8

  when not knowledge, 446–7

Experience, 348

and inference, 470

and scientific laws, 331–2  relation to knowledge, 441, 463

total momentary, 315, 316

Extensionality, mathematical logic and, 432

Extensions, see Intensions

Extrapolation, and empirical laws, 334–5

Eye, working of, 49


Fact(s),

  adaptation to, 160

  general, 149–50

  meaning, 159–60

  particular,

        knowledge of, 518

      and probability, 354

  unimaginable, 168

“Faith” of science, 332

Falling bodies, law of, 497

Falsehood, 127 ff.

  definition, 164–70

see also Truth

Fichte, 57

Fictiveness, degrees of, 255

Field, structural, 271

Finite frequency interpretation, 368.

and induction, 377–9

Fittest, survival of, 48

Force, 30

Four-dimensional manifold, 216

Franklin, Mrs. C. L., 196

Frequency, 346

Freud, S., 60

Functions, propositional, 468

and probability, 358Galaxy, 26

Galileo, 29–31, 328, 497

Gauss, 421

General propositions, how arrived at, 47–8

Generalizations,

  animal inferences and, 183

belief in, 183, 526

  inductive, 155 ff.

  Keynes and probability of, 453

Generator properties, 458

Genes, 47, 48

Geodesies, 304, 328

Geography, minimum vocabulary of, 259–60

Geology, and traditional beliefs, 44–5

Geometries, varying, 34

Geometry,

  interpretation of, 254

  theoretical and practical, 253–4

Gestalt psychology, 320

Geulincx, 332, 473

Gnostics, 24

Gravitation, 31, 34, 328

  relativity and, 307, 328


h, see Quantum constant

Habit(s), 54

  acquired, 114

  animal, and beliefs, 448–9

and belief, 114

and emotional interest, 202

        formation, 66

its part in perception, 183

  mental, and beliefs, 449Harmony, pre-established, 211–12

see also Leibniz

Hartley, 63

Heat, meaning of, 265

Hegel, 48, 57, 172, 173, 409, 414

Heisenberg, 37, 38

Helium, 35, 40–1

Heraclides, 23

Heraclitus, 499

“Here” and “now”, 19–20, 105–7

Heredity, 47

Hubble, 27

Hume, 57, 82, 125, 189, 202, 377, 400, 419, 451, 472, 473, 518

Huxley, Julian, 48 n

Huygens, 37

Hydrogen atom, 35, 40–1

Hypotheses, 116–17

  choice between, and observation, 500

  physical, 214

“I-now”, 319, 322

Idea(s),

  association of, 63

  “believed” and “entertained”, 201

  definition, 110, 200

  expression of attitude to, 121

  external reference of, 123 ff.

and impressions, 82, 166, 199, 473

  indicative, 121

  pre-linguistic, 109–10

Identity, 476

Ignorance, partial, and probability, 371

Illusion(s), 64

of sense, 181–2Images, 124–5

Imagination, pure, 125

Impact, theory of, 496

Imperative words, 85–6

Impression, in Hume, 473

see also Idea

Incompatibility, 139–40, 149

Indicative words, 85

Indifference, principle of, 391–2, 395–7, 402 ff.

Individuation, principle of, 310 ff.

Indivisibility, see Divisibility

Induction, 172, 179

  animal, 451–2

by simple enumeration, 418–24

and finite frequency theory, 377–9

  general and particular, 419

  hypothetical, 435

in advanced sciences, 427–9

  invalid, as logical principle, 422

its role, 451 ff.

  justification of, 330

  mathematical, 252

  mathematical treatment, 425 ff.

and natural numbers, 419

not a law of nature, 354

and probability, 418 ff.

and probability of generalizations, 454–5

  Reichenbach’s theory, 430 ff.

and scientific laws, 330–1

  scientific use, Keynes’ results, 452–453

and serial arrangement, 422–3

  summary of probability arguments, 435–6

Inductive generalizations, 155 ff.Inertia, law of, 30–1

Inference(s), 10 ff.

  analogical, 209

  animal, 126, 134, 182 ff., 198, 210, 514

  animal, and generalizations, 183

  animal, and human behaviour, 199

  animal, mistaken, 343

  animal and scientific, 202

and data, relation, 401

  deductive and inductive, 178–9

  forbidden, and solipsism, 193

  from perceptions to physical objects, 341 ff.

and future experience, 469

  mathematical and substantial, 198

  principles of, need for, 524

  probable, 178, 198 ff.

  scientific, only probable, 353

  scientific, postulates of, 439

  suggested principles of, 489–90

Infinite collections,

and finite frequency theory, 375

and probability, 385

Infinity, in Reichenbach theory, 382

Instance, meaning of, 311, 316

Instant(s), 255, 286–7, 292

  definition of, 287–9

and total momentary experience, 317

Insurance, life, and probability, 358–359

Intensions and extensions, 147, 155, 156, 423

and induction, 423

Intention, and assertion, 103

Interactions, 494., 508

Interconnection of events, 10–11Interjections, 72

Interpolation, and empirical laws, 334

Interpretation, 251 ff.

of calculus of probability, 362

in logic, 257

  logical and empirical, 258

  role in perception, 187

Interrogative words, 85

Interval, 306–7

in relativity theory, 33, 307

and space-time order, 348

Introspection, as scientific method, 59, 64–5

Intuition, mathematical, 252, 421–2

Irrelevance, in Keynes’s theory, 392


James, William, 220

Jevons, 420

Johnson, 362

Johnson, Dr., 187

Jones, H. Spencer, 27, 50

Joule, 40, 265

Judgments,

  negative, 138 ff., 146

of perception, 321

Kant,

ethical theory of, 415

and human knowledge, 9

and nebular hypothesis, 26, 47

and unitary space, 237

Kepler, 499

Keynes, 12, 332, 336, 355, 362, 410, 426–8, 451, 452 ff., 456, 458 ff.

his probability theory, 390 ff.

Kinds, natural, 335–6, 456 ff.and physics, 461

Knowledge,

based on individual percepts, 22

and coherence, 172, 173

  cosmically unimportant, 9

  definition of, 170–4, 516 ff.

and delayed reactions, 109

  doubtfulness of, 359, 398, 516

  evolution and, 439

  expectations and, 445 ff.

  general, 146.

  generalized, 117

  individual and collective, 17

kinds of, 439

a matter of degree, 174, 444

  meaning of, 111 ff.

of facts, sources of, 440

of particular facts, 518

of premisses, 517 ff.

of truth and falsehood, 522 ff.

order for, and causal order, 22

  psychology and, 66–7

  relation to experience, 463

  socialized potential, 117

  synthetic, whether all based on experience, 525–6

  theories of, 172–4

and true belief, 113, 170–1

  uncertain, 517

an unprecise concept, 13, 113

  verbal, 111

  verbal expression, and experience, 441


Language,  autonomy of, 74, 76

and belief in causation, 473

  dangers of, 76–7

  dependence on physics, 74

  exclamatory use, 109

  how learnt, 78–9

  learning to use, 18

merits of, 73

  personal element in, 18–19

  purposes of, 71, 72

social character, 17–18

  subjective, see Egocentric Particulars

and thought, 74

  understanding of, and animal inference, 199

Laplace, 26, 29, 47, 365, 372–3, 404, 424, 425

Laws,

  causal, see Causal Laws

  limit of complexity, 329–30

  natural, postulate of existence of, 496–7

  necessity of, for inference, 354

  scientific,

        approximative character, 335

      and memory, 204

Least action, principle of, 328

Leibniz, 31, 32, 57, 105, 140, 173, 211, 217, 295, 310, 317, 325, 362, 439, 480, 482

Life,

age of, 50

  distinguishing marks of, 46

  physico-chemical basis of, 50

rarity of, 49–50

Light,

  nature of, 214

  velocity of, 26, 305  waves v. particles, 37

Limited variety, principle of, 336

see also Kinds, Natural

Lines, causal, see Causal lines

Littlewood, 421

Locke, 187, 518

Logic and psychology, difference, 143

Logical concepts, impersonality of, 19


Magnitude, 302–3

Manicheans, 24

Marx, Karl, 439–40

Mass, and energy, 39–40, 309

Materialism, probably irrefutable by external observation, 503

Mathematics, and language, 74–5

Matter,

  homogeneity of, 29, 215

  impenetrability of, 297

and motion, 214

and “things”, 340

Meaning, 129

  derivative, 131–2

  indicative and imperative, 85–6

in mathematics, 75

Measurement, 299 ff.

  presuppositions of, 299

Memory,

as premiss of knowledge, 203–5, 210

as source of knowledge, 440–1

  communication of, 73

  errors of, 229–30

false, and dreams, 230

and imaginative accretions, 124  immediate, 111

lapses of, 193

  non-verbal, 124

part in generating knowledge, 518 ff.

and recognition, 82

and solipsism, 193–4

test of accuracy, 441

and time, 226–7

true, 111

  trustworthiness of, 228–9

and truth, 131, 167–8

  uncertain, 411

Memory-images, truth of, 442

Mendelism, 47, 48

Michelson–Morley experiment, 302, 305

Milky Way, see Galaxy

Mill, J. S., 471

Miller, Hugh, 44

Milne, E. A., 34, 481

Mind, and brain, 54–5, 244–5

Mind-body relation, 212

Mind–matter relation, 51, 55–6, 57 ff., 216 ff., 240 ff.

Minds, inference to, from behaviour, 504–5

Minds, other, belief in, 501

Minimum vocabularies, see Vocabularies

Miracles, 30

Mirage, 342

Mises, von, 380

Mises–Reichenbach theory, 380 ff.

Monadology, 57, 105, 211, 439, 480; see also Leibniz

Motion, first law, 30, 475, 491

Movement, sensations of, 235

Mutants, 47Names,

  apply to occurrences experienced, 96

as permanent symbols, 75–6

  knowledge of, 103–4

of qualities, 84, 95–8

of relations, 84, 95–8

of species, 83

of substances, 83

power attributed to, 71

  proper, 87 ff.

      and animal inference, 203

      and class, 87

        definition of, 87–9

      and individuation, 318 ff.

        misleading, 98–9

        need for, and method of acquiring knowledge, 325

        substitutes for, 90–4

and structural definitions, 295–6

Natural kinds, see Kinds

Nature, uniformity of, 335

Nebulae,

  extra-galactic, 26–7

  recession of, 27, 34

Nebular hypothesis, 26, 45

Negative facts, 137–8, 512

Negative judgments, 138 ff., 146

Negatives,

  attempted elimination of, 137 ff.

  universal, 149, 523

Neptune, argument re probability of discovery, 428–30

Nerve fibres, afferent and efferent, 52

Nervous stimulus, see Stimulus

Neutron, 35–6Newton, 25–6, 31, 32, 37, 43, 255, 284–8, 328, 497

and space, 295, 296

his first law of motion, 30

Nicod, 459

Noises, animal, function of, 72

Non-observation, 512

Nonsense, 127

Non-sufficient reason, principle of, 391 see also Indifference, principle of

“Not”, 137–45, 519–20

“Now”, see “here”

Number, meaning of, 253

Numbers, natural, and induction, 419


Observation,

appeal to, and scientific truth, 263

  development from, to law, 498

and theory, relation, 339

Observer, 481

  normal, 223

Occurrence, representational, 131

“Of”, 123, 125

Omniscience, first-order, 150

“Or”, 142–5, 151, 520

Order, causal, and order for knowledge, 22

Order, space-time, 311 ff.

Organism, concept of, 48–9

Origen, 71

Origin, definition of, in relation to co-ordinates, 92, 95

Ostensive definition, see Definition

Overlapping,

spatial, 297

  temporal, 288–9Parallelogram law, 30, 31–2, 215

Particle(s), 256, 317

  definition of, 308

and physics, 214

and waves, 37, 273

Particular, meaning of, 310 ff.

Particularity, spatio-temporal, 282

Particulars, egocentric, see Egocentric particulars

Pavlov, 66, 203

Peano, 94, 252–3, 357, 362

Percept(s),

and inferences to objects, 222

  inference to physical occurrences, 213, 341–2, 348

and laws, harmony of, 342

  meaning of, 218 ff.

  spatial relations of, 217

and structural causal lines, 492

  subjective variations in, 223

and verification of physical laws, 219

see also Perception(s)

Perception(s),

and belief in causation, 474

and causal lines, 476

  common sense and, 241

  dualistic view, 220

  empirical theory, 211 ff.

  faint, 410–11

and “here-now”, 107

  idealist theory, 211

  inescapable basis of knowledge, 22

  judgments of, 321

and knowledge, 440

part in generating knowledge, 518 ff.